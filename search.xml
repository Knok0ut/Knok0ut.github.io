<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2025调研</title>
      <link href="/2025/08/28/2025-diao-yan/"/>
      <url>/2025/08/28/2025-diao-yan/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="USENIX-2024"><a href="#USENIX-2024" class="headerlink" title="USENIX 2024"></a>USENIX 2024</h2><h3 id="错误注入和鲁棒性"><a href="#错误注入和鲁棒性" class="headerlink" title="错误注入和鲁棒性"></a>错误注入和鲁棒性</h3><ul><li>DNN-GP: Diagnosing and Mitigating Model’s Faults Using Latent Concepts.</li><li>Yes, One-Bit-Flip Matters! Universal DNN Model Inference Depletion with Runtime Code Fault Injection.</li><li>Tossing in the Dark: Practical Bit-Flipping on Gray-box Deep Neural Networks for Runtime Trojan Injection.</li><li>Forget and Rewire: Enhancing the Resilience of Transformer-based Models against Bit-Flip Attacks.<h3 id="大模型攻击与防御"><a href="#大模型攻击与防御" class="headerlink" title="大模型攻击与防御"></a>大模型攻击与防御</h3></li><li>An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection. </li><li>REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models. 水印</li><li>Formalizing and Benchmarking Prompt Injection Attacks and Defenses.</li><li>Instruction Backdoor Attacks Against Customized LLMs.<h3 id="安全ML"><a href="#安全ML" class="headerlink" title="安全ML"></a>安全ML</h3></li><li>AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE.</li><li>Fast and Private Inference of Deep Neural Networks by Co-designing Activation Functions.</li><li>OblivGNN: Oblivious Inference on Transductive and Inductive Graph Neural Network.</li><li>MD-ML: Super Fast Privacy-Preserving Machine Learning for Malicious Security with a Dishonest Majority.</li><li>Accelerating Secure Collaborative Machine Learning with Protocol-Aware RDMA.<h3 id="隐私推理"><a href="#隐私推理" class="headerlink" title="隐私推理"></a>隐私推理</h3></li><li>A Linear Reconstruction Approach for Attribute Inference Attacks against Synthetic Data.</li><li>Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models.</li><li>MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training.</li><li>Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks.</li><li>Property Existence Inference against Generative Models.</li><li>How Does a Deep Learning Model Architecture Impact Its Privacy? A Comprehensive Study of Privacy Attacks on CNNs and Transformers.</li><li>Reconstructing training data from document understanding models.</li><li>Privacy Side Channels in Machine Learning Systems.</li><li>FaceObfuscator: Defending Deep Learning-based Privacy Attacks with Gradient Descent-resistant Features in Face Recognition.<h3 id="后门"><a href="#后门" class="headerlink" title="后门"></a>后门</h3></li><li>Neural Network Semantic Backdoor Detection and Mitigation: A Causality-Based Approach.</li><li>On the Difficulty of Defending Contrastive Learning against Backdoor Attacks.</li><li>Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models.</li><li>Xplain: Analyzing Invisible Correlations in Model Explanation.</li><li>Verify your Labels! Trustworthy Predictions and Datasets via Confidence Scores.<h3 id="Digital-Adversarial-Attacks"><a href="#Digital-Adversarial-Attacks" class="headerlink" title="Digital Adversarial Attacks"></a>Digital Adversarial Attacks</h3></li><li>More Simplicity for Trainers, More Opportunity for Attackers: Black-Box Attacks on Speaker Recognition Systems by Inferring Feature Extractor.</li><li>Transferability of White-box Perturbations: Query-Efficient Adversarial Attacks against Commercial DNN Services.</li><li>Adversarial Illusions in Multi-Modal Embeddings.</li><li>It Doesn’t Look Like Anything to Me: Using Diffusion Model to Subvert Visual Phishing Detectors.</li><li>Invisibility Cloak: Proactive Defense Against Visual Game Cheating.<h3 id="对抗攻防"><a href="#对抗攻防" class="headerlink" title="对抗攻防"></a>对抗攻防</h3></li><li>Correction-based Defense Against Adversarial Video Attacks via Discretization-Enhanced Video Compressive Sensing.</li><li>Rethinking the Invisible Protection against Unauthorized Image Usage in Stable Diffusion.</li><li>Splitting the Difference on Adversarial Training.</li><li>Machine Learning needs Better Randomness Standards: Randomised Smoothing and PRNG-based attacks.</li><li>PatchCURE: Improving Certifiable Robustness, Model Utility, and Computation Efficiency of Adversarial Patch Defenses.<h3 id="评估和最好的实践"><a href="#评估和最好的实践" class="headerlink" title="评估和最好的实践"></a>评估和最好的实践</h3></li><li>SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models<h3 id="后门-1"><a href="#后门-1" class="headerlink" title="后门"></a>后门</h3></li><li>UBA-Inf: Unlearning Activated Backdoor Attack with Influence-Driven Camouflage</li></ul><h3 id="越狱"><a href="#越狱" class="headerlink" title="越狱"></a>越狱</h3><ul><li>LLM-Fuzzer: Scaling Assessment of Large Language Model Jailbreaks.</li><li>Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models.</li><li>Malla: Demystifying Real-world Large Language Model Integrated Malicious Services.</li><li>Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction.<h3 id="模型萃取和水印"><a href="#模型萃取和水印" class="headerlink" title="模型萃取和水印"></a>模型萃取和水印</h3></li><li>SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice.</li><li>Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through Data-Free Model Extraction Attacks?</li><li>ClearStamp: A Human-Visible and Robust Model-Ownership Proof based on Transposed Model Training.</li><li>DeepEclipse: How to Break White-Box DNN-Watermarking Schemes.</li><li>ModelGuard: Information-Theoretic Defense Against Model Extraction Attacks.</li></ul><h3 id="大模型滥用"><a href="#大模型滥用" class="headerlink" title="大模型滥用"></a>大模型滥用</h3><ul><li>Moderating Illicit Online Image Promotion for Unsafe User Generated Content Games Using Large Vision-Language Models.</li><li>Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text.</li><li>Prompt Stealing Attacks Against Text-to-Image Generation Models.</li><li>Quantifying Privacy Risks of Prompts in Visual Prompt Learning.<h3 id="安全分析"><a href="#安全分析" class="headerlink" title="安全分析"></a>安全分析</h3></li><li>Hijacking Attacks against Neural Network by Analyzing Training Data.</li><li>False Claims against Model Ownership Resolution.</li><li>Landscape More Secure Than Portrait? Zooming Into the Directionality of Digital Images With Security Implications.</li><li>Information Flow Control in Machine Learning through Modular Model Architecture.<h3 id="物理对抗攻击"><a href="#物理对抗攻击" class="headerlink" title="物理对抗攻击"></a>物理对抗攻击</h3></li><li>Devil in the Room: Triggering Audio Backdoors in the Physical World.</li><li>FraudWhistler: A Resilient, Robust and Plug-and-play Adversarial Example Detection Method for Speaker Recognition.</li><li>pi-Jack: Physical-World Adversarial Attack on Monocular Depth Estimation with Perspective Hijacking.</li><li>AE-Morpher: Improve Physical Robustness of Adversarial Objects against LiDAR-based Detectors via Object Reconstruction.</li></ul><h3 id="用户研究"><a href="#用户研究" class="headerlink" title="用户研究"></a>用户研究</h3><ul><li>“I Don’t Know If We’re Doing Good. I Don’t Know If We’re Doing Bad”: Investigating How Practitioners Scope, Motivate, and Conduct Privacy Work When Developing AI Products.</li><li>Towards More Practical Threat Models in Artificial Intelligence Security.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 论文调研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations 阅读笔记</title>
      <link href="/2025/04/03/a-survey-of-safety-on-large-vision-language-models-attacks-defenses-and-evaluations-yue-du-bi-ji/"/>
      <url>/2025/04/03/a-survey-of-safety-on-large-vision-language-models-attacks-defenses-and-evaluations-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文题目：A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations<br>发表时间: 2025<br>发表期刊/会议：<br>论文作者：Ye, Mang; Rong, Xuankun; Huang, Wenke; Du, Bo; Yu, Nenghai; Tao, Dacheng</p><h2 id="1-内容简介"><a href="#1-内容简介" class="headerlink" title="1. 内容简介"></a>1. 内容简介</h2><p>这篇文章是一篇关于视觉-语言多模态模型的安全综述。主要对现有攻击、防御和评估方法进行分类。这篇笔记主要记录攻击和防御部分。</p><h2 id="2-主要内容"><a href="#2-主要内容" class="headerlink" title="2. 主要内容"></a>2. 主要内容</h2><h3 id="2-1-文章贡献"><a href="#2-1-文章贡献" class="headerlink" title="2.1 文章贡献"></a>2.1 文章贡献</h3><ul><li><strong>系统化分析LVLM安全性</strong>：论文整合了攻击、防御和评估这三个相互关联的方面，对LVLM安全性进行了全面、系统的分析。单独研究攻击或防御无法全面刻画LVLM的安全态势，而论文的研究结合了这些关键要素，提供了对LVLM漏洞及其缓解策略的整体理解。</li><li><strong>构建通用安全分类框架</strong>：基于LVLM生命周期分析，论文们提出了一个通用分类框架，按照推理阶段（Inference Phase）和训练阶段（Training Phase）对安全相关研究进行分类，并进一步细分子类别，以提供更细粒度的理解。对于每项研究，论文深入探讨其方法论与贡献，全面剖析当前LVLM安全领域的研究格局。</li><li><strong>最新LVLM安全性评估与未来研究方向</strong>：论文对最新的LVLM——Deepseek Janus-Pro进行了安全评估，并探讨未来研究路径，提供深刻见解和战略性建议。这些指导意见将有助于研究社区进一步提升LVLM的安全性与稳健性，确保其在关键任务应用中的安全可靠部署。</li></ul><h3 id="2-2-背景"><a href="#2-2-背景" class="headerlink" title="2.2 背景"></a>2.2 背景</h3><p><strong>大视觉语言模型</strong>：大型语言模型（LLMs）的发展已成为人工智能领域的基石，彻底改变了机器对人类语言的理解与生成方式。代表性的 LLM 包括 OpenAI 的 GPT-4、Google 的 PaLM、Meta 的 LLaMA 以及 Vicuna，这些模型在自然语言理解与生成方面展现出了卓越的能力。</p><p>为了拓展 LLM 的应用范围，研究者通常将其与视觉组件集成，从而发展出大型视觉-语言模型（LVLMs）。LVLMs 通过视觉特征提取器对图像进行编码，并利用连接模块将视觉标记投影到 LLM 的词嵌入空间，使得模型能够联合处理文本和视觉输入。这一多模态集成弥合了视觉与语言之间的鸿沟，为各个领域的高级应用铺平了道路。</p><p><strong>LVLMs 面临的挑战</strong>：尽管 LVLMs 展现出了卓越的能力，但它们仍面临多个关键挑战：</p><ul><li><strong>可扩展性（Scalability）</strong>：多模态数据的集成显著增加了训练和推理阶段的计算需求，带来计算成本和能耗问题。</li><li><strong>对抗性鲁棒性（Robustness to Adversarial Inputs）</strong>：多模态环境中的对抗攻击能够利用文本和视觉输入之间的交互，导致模型生成意外或不安全的输出。</li><li><strong>偏见与公平性（Bias and Fairness）</strong>：LVLMs 可能会继承训练数据中的偏见，在敏感场景下产生不公平或有害的结果。</li><li><strong>安全性与对齐（Safety and Alignment）</strong>：由于训练数据的不足或模型在多模态查询上的理解缺陷，LVLMs 仍然容易生成有害或误导性内容，安全性和对齐性问题仍需持续优化。</li></ul><p><strong>攻击者能力</strong>：可以按照按照攻击者的知识集合$\mathcal K$对攻击进行分类。该集合包括模型参数$\theta$,模型架构$\mathcal A_\theta$，梯度$\nabla_\theta\mathcal L$，输入$x$和输出$y$。按照这些信息可以大致把攻击者能力分为三类：</p><ul><li>白盒能力：可以知道所有知识。</li><li>灰盒能力：可以知道部分内部信息，如模型架构，但是缺少参数和梯度信息。</li><li>黑盒能力：只能知道输入和输出。</li></ul><p><strong>攻击目标</strong>：分为有目标、无目标、越狱攻击</p><ul><li>有目标攻击：将任意输入x的输出变为指定的y</li><li>无目标攻击：改变任意x的输出为其他值</li><li>越狱攻击：绕过模型安全机制，使其输出不安全的内容</li></ul><p><strong>攻击策略</strong>：按照攻击策略划分可以分为如下的五类：</p><ul><li>基于扰动的攻击：（类似对抗样本）在输入中添加难以察觉的扰动，使模型产生错误输出</li><li>基于迁移的攻击：利用对抗样本的迁移性进行攻击</li><li>基于提示的攻击：通过操纵输入提示来误导模型</li><li>基于投毒的攻击：训练数据中注入恶意数据，影响模型的学习过程</li><li>基于触发器的攻击：训练数据中嵌入特定后门，在推理过程中发现后门时会导致模型按照预设方式进行变化<h3 id="2-3-攻击方法"><a href="#2-3-攻击方法" class="headerlink" title="2.3 攻击方法"></a>2.3 攻击方法</h3>针对视觉语言模型的攻击可以分为推理时攻击和训练时攻击。<br><strong>推理时攻击</strong>：通过精心设计的恶意输入来完成攻击。分为黑盒、白盒、灰盒。总结如下：<br><img src="/2025/04/03/a-survey-of-safety-on-large-vision-language-models-attacks-defenses-and-evaluations-yue-du-bi-ji/Pasted image 20250403181150.png" alt="img"></li></ul><p><strong>训练时攻击</strong>：<br><img src="/2025/04/03/a-survey-of-safety-on-large-vision-language-models-attacks-defenses-and-evaluations-yue-du-bi-ji/Pasted image 20250403181240.png" alt="img"></p><h3 id="2-4-防御方法"><a href="#2-4-防御方法" class="headerlink" title="2.4 防御方法"></a>2.4 防御方法</h3><p><strong>推理时防御</strong>：<br><img src="/2025/04/03/a-survey-of-safety-on-large-vision-language-models-attacks-defenses-and-evaluations-yue-du-bi-ji/Pasted image 20250403181346.png" alt="img"></p><p><strong>训练时防御</strong>：<br><img src="/2025/04/03/a-survey-of-safety-on-large-vision-language-models-attacks-defenses-and-evaluations-yue-du-bi-ji/Pasted image 20250403181415.png" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 综述 </tag>
            
            <tag> 大语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>High-Dimensional Probability 第二章笔记-1</title>
      <link href="/2025/03/24/high-dimensional-probability-di-er-zhang-bi-ji-1/"/>
      <url>/2025/03/24/high-dimensional-probability-di-er-zhang-bi-ji-1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>本章介绍几种集中不等式并介绍几种分布。笔记中只简单记录结论，证明过程可以参阅原书”High-Dimensional Probability An Introduction with Applications in Data Science”</p></blockquote><h2 id="2-1-为什么需要集中不等式"><a href="#2-1-为什么需要集中不等式" class="headerlink" title="2.1 为什么需要集中不等式"></a>2.1 为什么需要集中不等式</h2><p>集中不等式量化了一个随机变量$X$相较于其均值$\mu$的偏移程度。这件事通常使用$X-\mu$分布两端的一个约束表示：</p><script type="math/tex; mode=display">\mathbb P\{\left |X-\mu \right |>t\} \le something \, small</script><p>切比雪夫不等式就是一个较弱的集中不等式。</p><p><strong>主张2.1.2：高斯分布的尾约束</strong>：令$g\sim N(0,1),$则对于所有的$t&gt;0$，有：</p><script type="math/tex; mode=display">(\frac{1}{t}-\frac{1}{t^3})\cdot\frac{1}{\sqrt{2\pi}}e^{-t^2/2}\le\mathbb P\{g\ge t\}\le\frac{1}{t}\cdot\frac{1}{\sqrt{2\pi}}e^{-t^2/2}</script><p>特别的，当$t&gt;1$时，尾部由概率密度函数本身约束：</p><script type="math/tex; mode=display">\mathbb{P}\{g\ge t\}\le\frac{1}{\sqrt{2\pi}}e^{-t^2/2}</script><p>2.1.2所展示的下界有可能是个负值，所以下界在t&gt;1的时候才有意义。</p><p>使用大数定理可知当$N\to\infty$时$Z_N$趋向于正态分布。故独立同分布随机变量的和的界可以用高斯分布的尾约束来计算。但即使该约束的上界是以指数速度逼近0的，但$Z_N$和正态分布之间的误差可能会减小的过于缓慢。</p><p><strong>定理2.1.3 Berry-Esseen中心极限定理</strong>： i.i.d.随机变量序列$X_i$及其和$S_N$，对于任意$N$和$t\in \mathbb R$，有：</p><script type="math/tex; mode=display">\left|\mathbb P\{Z_N\ge t\}-\mathbb P\{g \ge t\}\right| \le \frac{\rho}{\sqrt{N}}</script><p>其中$\rho=\mathbb E|X_1-\mu|^3/\sigma^3\,,g\sim N(0,1)$<br> 这个定理说明$Z_N$和正态分布的误差减小的过于缓慢，结合2.1.2使用会导致高斯分布尾约束的指数下降速率无法发挥作用</p><p><strong>练习2.1.4</strong>：令$g\sim N(0,1)$。证明对于任意$t\ge 1$，有：</p><script type="math/tex; mode=display">\mathbb{E} g^2 \mathbf{1}_{\{g>t\}}=t \cdot \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2}+\mathbb{P}\{g>t\} \leq\left(t+\frac{1}{t}\right) \frac{1}{\sqrt{2 \pi}} e^{-t^2 / 2}</script><h2 id="2-2-霍夫丁不等式"><a href="#2-2-霍夫丁不等式" class="headerlink" title="2.2 霍夫丁不等式"></a>2.2 霍夫丁不等式</h2><p><strong>定义 2.2.1 对称伯努利分布(Symmetric Bernoulli distribution)</strong>：一个随机变量$X$是对称伯努利分布（也叫拉德马赫分布）当且仅当它取值为-1或1的概率都为$1/2$，即：</p><script type="math/tex; mode=display">\mathbb P(X=-1)=\mathbb P(X=1)=\frac{1}{2}</script><p>显然，如果$X$为一般(usual)伯努利分布当且仅当$2X-1$为对称伯努利分布。</p><p><strong>定理2.2.2霍夫丁不等式</strong>：令$X_1,\dots,X_N$不相关的对称伯努利随机变量，$a=(a_1,\dots,a_N)\in \mathbb R^N$。对于任意$t\ge 0$，有：</p><script type="math/tex; mode=display">\mathbb P\left\{\sum_{i=1}^Na_iX_i\right\}\le \exp\left(-\frac{t^2}{2||a||^2_2}\right)</script><p>霍夫丁定理可以看作是中心极限定理的集中形式。从中我们可以看出$\sum a_iX_i$的尾部和高斯分布的尾部表现相似。当$Z_n$和高斯分布之间的误差并不是指数级小的时候，我们可以依据该定理获得和高斯分布一样指数小的尾部约束。</p><p><strong>评论2.2.4</strong>：值得注意的是，霍夫丁定理并不是渐进不等式，即它对所有的N都成立（其它经典的概率论定理可能只有在$n\to\infty$时才成立），而且N越大，霍夫丁不等式越强。这在实际应用中尤其是N和数据规模相关时尤其好用。</p><p><strong>定理2.2.5双边霍夫丁不等式</strong>：令$X_1,\dots,X_N$不相关的对称伯努利随机变量，$a=(a_1,\dots,a_N)\in \mathbb R^N$。对于任意$t&gt; 0$，有：</p><script type="math/tex; mode=display">\mathbb{P}\left\{\left|\sum_{i=1}^N a_i X_i\right| \geq t\right\} \leq 2 \exp \left(-\frac{t^2}{2\|a\|_2^2}\right)</script><p><strong>定理2.2.6只有一般约束的随机变量的霍夫丁不等式</strong>：令$X_1,\dots,X_N$不相关的随机变量，假设对于每个随机变量有$X_i\in[m_i,M_i]$，则对于任意的$t&gt;0$，有：</p><script type="math/tex; mode=display">\mathbb{P}\left\{\sum_{i=1}^N\left(X_i-\mathbb{E} X_i\right) \geq t\right\} \leq \exp \left(-\frac{2 t^2}{\sum_{i=1}^N\left(M_i-m_i\right)^2}\right)</script><p><strong>练习2.2.8加强随机算法</strong>：有一个随机决策算法，该算法有$1/2+\delta$的概率返回正确答案，其中$\delta&gt;0$。为了提升该算法，我们把该算法运行N次并进行投票（少数服从多数）。证明对于任意的$\epsilon\in(0,1)$，投票后算法正确率至少为$1-\epsilon$，只要：</p><script type="math/tex; mode=display">N\ge\frac{1}{2\delta^2}ln(\frac{1}{\epsilon})</script><p><strong>练习2.2.9均值的鲁棒估计</strong>：假设我们想估计随机变量$X$的均值$\mu$，从X独立采样出一批样本$X_1,\dots,X_N$。我们想要$\varepsilon$-准确度，即$\mu\in(\mu-\varepsilon,\mu+\varepsilon)$<br>(a) 证明采样数量为$N=O(\sigma^2/\epsilon^2)$的样本足以以3/4的概率达到$\varepsilon$-准确度，其中$\sigma^2$是X的方差<br>(b) 证明采样数量为$N=O(log(\delta^{-1})\sigma^2/\epsilon^2)$的样本足以至少以$1-\delta$的概率达到$\varepsilon$-准确度。</p><p><strong>练习2.2.10小球概率</strong>：令$X_1,\dots,X_N$是非负的且连续分布的独立随机变量。假设$X_i$一致有界于1<br>(a) 证明$X_i$的矩生成函数满足：</p><script type="math/tex; mode=display">\mathbb E(exp(-tX_i))\le\frac{1}{t}\quad for\,\,\,all\,\,\,t>0</script><p>(b) 推理对于任意的$\varepsilon &gt;0$,有：</p><script type="math/tex; mode=display">\mathbb P\left\{\sum_{i=1}^NX_i\le\varepsilon N\right\}\le(e\varepsilon)^N</script>]]></content>
      
      
      <categories>
          
          <category> High-Dimensional Probability </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高维概率论 </tag>
            
            <tag> 阅读笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Can Neural Network Memorization Be Localized 阅读笔记</title>
      <link href="/2025/03/22/can-neural-network-memorization-be-localized-yue-du-bi-ji/"/>
      <url>/2025/03/22/can-neural-network-memorization-be-localized-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文题目：Can Neural Network Memorization Be Localized<br>发表时间：2023<br>发表期刊/会议： ICML, CCF A</p><h2 id="1-内容简介"><a href="#1-内容简介" class="headerlink" title="1. 内容简介"></a>1. 内容简介</h2><p>这篇文章通过实验讨论模型的记忆性是否可以被定位到网络的特定部分。有之前的工作认为主要是模型的最后几层具有记忆性，但本篇论文反驳了这个观点，并提出模型的记忆性来自于所有层的个别神经元。</p><h2 id="2-主要内容"><a href="#2-主要内容" class="headerlink" title="2. 主要内容"></a>2. 主要内容</h2><h3 id="2-1-问题设定"><a href="#2-1-问题设定" class="headerlink" title="2.1 问题设定"></a>2.1 问题设定</h3><p>讨论模型的记忆性首先要明确什么是模型的记忆性。在论文intro部分写道 目前一种常见模型记忆性的定义是，如果把一个非典型（可能指离群点）样本从训练集中移除，模型对其预测标签产生了明显的变化（在训练集时预测正确，移除后预测错误），则认为模型对这个样本产生了记忆。但计算这一度量的开销较大，故许多研究使用错误标注样本来衡量记忆程度，因为模型必须记住这个样本才能正确预测。这篇论文同时探讨非典型的记忆化喝错误标注的记忆化。</p><p><strong>Preliminaries</strong>:考虑一个d层的前馈神经网络$\mathcal F_d$，其参数为$\theta = (\theta_1,\dots,\theta_d)$。使用$z_l$表示第$l$层的激活输出，可以使用下式进行表达：</p><script type="math/tex; mode=display">z_l = \mathcal F(z_{l-1};\theta_l)</script><p>使用$[z_l]_j$表示第j个神经元/通道。模型使用监督学习训练，数据集为$\mathcal S=\{x_i,y_i\}^n$，优化器为SGD。第t个epoch训练的结果为$\theta^t=\{\theta_1^t,\dots,\theta_d^t\}$</p><p>在带有标签噪声的实验中，论文随机改变了固定比例样本的标签。$\mathcal S=\mathcal S_c \cup \mathcal S_n$，其中c表示干净样本，n表示改变了标签的样本。</p><p><strong>数据集和模型</strong>：数据集有CIFAR10，MNIST，SVHN。模型有ResNet-9，ResNet-50，ViT</p><h3 id="2-2-梯度核算"><a href="#2-2-梯度核算" class="headerlink" title="2.2 梯度核算"></a>2.2 梯度核算</h3><p>在训练模型时，梯度更新公式为：</p><script type="math/tex; mode=display">\theta_l^{t+1}=\theta^t_l-\eta \frac{d\mathcal L(\mathcal S,\mathcal F_d)}{d\theta_l}</script><p>梯度决定了如何更新模型的参数。如果一个知识被编码进模型参数，则梯度的大小就可以作为模型对该知识记忆潜力的表征。</p><h4 id="2-2-1-分层范数贡献"><a href="#2-2-1-分层范数贡献" class="headerlink" title="2.2.1 分层范数贡献"></a>2.2.1 分层范数贡献</h4><p>为了获得干净样本和错误标注样本对梯度的贡献，论文使用$||d\mathcal L(\mathcal S_c,\mathcal F_d)||_2,||d\mathcal L(\mathcal S_n,\mathcal F_d)||_2$作为指标。使用每一层参数个数的算数平方根对其进行归一化。为了综合不同轮数或不同层的结果，使用平均值进行统计。</p><p>从下图可以看出不存在哪一层的错误标注样本（记忆样本，Noisy样本）比干净样本的贡献值更大。同时论文也观测到一个有趣的事实是即使错误标注样本只占10%，他们对梯度的贡献度和其它样本整体的贡献度几乎一样大。这表明即使不存在特定的哪一层记住了错误标注样本，他们也确实对模型的每一层产生了巨大影响，这也意味着错误标注样本的梯度范数比干净样本高一个数量级。<br><img src="/2025/03/22/can-neural-network-memorization-be-localized-yue-du-bi-ji/Pasted image 20250322162251.png" alt="img"></p><h4 id="2-2-2-干净样本和错误标注样本的梯度相似度"><a href="#2-2-2-干净样本和错误标注样本的梯度相似度" class="headerlink" title="2.2.2 干净样本和错误标注样本的梯度相似度"></a>2.2.2 干净样本和错误标注样本的梯度相似度</h4><p>上图展示的结果可以看到整体梯度值比单独的干净或错误标注样本小很多，这说明干净样本和噪声样本的梯度方向可能是相反的。论文对每一层每一个epoch里干净样本和错误标注样本的梯度平均值的余弦相似度进行计算。结果如下图。</p><p>可以看到干净样本和错误标注样本的余弦相似度非常小。但该实验暂时无法说明(1) 每个神经元是否存在梯度错位（2）这是一个只发生在宏观层级的现象。<br><img src="/2025/03/22/can-neural-network-memorization-be-localized-yue-du-bi-ji/Pasted image 20250322163740.png" alt="img"></p><p>和图三（下图）的曲线对比可以发现错误标注样本被模型学习的主要阶段在第10-30轮。对于所有轮所有层大多数时间两种样本余弦相似度的值都低于-0.75。这表明对错误标注的样本进行过拟合可能不是一种良性的现象。</p><h3 id="2-3-层的功能临界性"><a href="#2-3-层的功能临界性" class="headerlink" title="2.3 层的功能临界性"></a>2.3 层的功能临界性</h3><h4 id="2-3-1-层回溯"><a href="#2-3-1-层回溯" class="headerlink" title="2.3.1 层回溯"></a>2.3.1 层回溯</h4><p>对于一个已经训练T轮收敛的模型$\mathcal F_d$，参数是$(\theta_1^T,\dots.\theta_d^T)$，论文把模型某一层的参数回退到第t轮的checkpoint，对每一层都重复这个过程。接着利用模型训练集对模型性能进行测试。结果如下图（图三）所示<br><img src="/2025/03/22/can-neural-network-memorization-be-localized-yue-du-bi-ji/Pasted image 20250322164618.png" alt="img"><br>分析可知：</p><ol><li>对模型最后一层进行回溯造成的影响较少（绿线和红线末尾准确率都比较高）</li><li>（没看懂在说啥，感觉和图展示的不一样）If contrasted with learning dynamics of the memorized examples (Figure 3 (left)), we observe that the model accuracy on noisy examples stays below 20% until the 30th epoch. Despite this, rewinding individual model layers to a checkpoint before the 30th epoch does not reduce accuracy on memorized examples. This seemingly contradicts the hypothesis that memorization happens in the last (few) layer(s).</li><li>不同模型记忆错误标注样本的关键层不同。</li></ol><p>综合信息表明：</p><ol><li>模型的记忆性分布在不同的层里</li><li>模型记忆性分布和问题相关</li><li>记忆层与学习干净样本的关键层有一定重叠</li></ol><h4 id="2-3-2-层重训练"><a href="#2-3-2-层重训练" class="headerlink" title="2.3.2 层重训练"></a>2.3.2 层重训练</h4><p>从上面的实验中注意到错误标注样本的关键层对干净样本同样重要。不同层固有的的学习速率可能是一个额外因素。为了进一步研究，论文将某一层回溯到初始状态后使用干净数据进行重训练。</p><p>对$\mathcal F_d$的第l层进行重新训练时，初始化参数如下：</p><script type="math/tex; mode=display">\theta_{init}=(\theta_1^T,\dots,\theta_{l-1}^T,\theta_l^0,\theta_{l+1}^T,\dots,\theta^T_d)</script><p>使用单周期学习率调度对模型进行20轮训练，第10轮时学习率到达最大值0.1，其它层参数保持冻结。</p><p>下图结果表明，即使只使用干净样本训练单个层，它仍可以在没见过噪声样本的情况下对其进行准确预测。表明预测噪声样本的信息已经保存在其他层，该层对于噪声样本的记忆是冗余的。需要注意的是这个推导过程是单向的，即如果预测效果不好也不能说明该层对记忆至关重要，因为模型优化可能有多个最优解，可能由于优化路径不同导致结果的变化。<br><img src="/2025/03/22/can-neural-network-memorization-be-localized-yue-du-bi-ji/Pasted image 20250322182410.png" alt="img"></p><h3 id="2-4-单个神经元的记忆性"><a href="#2-4-单个神经元的记忆性" class="headerlink" title="2.4 单个神经元的记忆性"></a>2.4 单个神经元的记忆性</h3><h4 id="2-4-1-预测一个样本需要多少个神经元？"><a href="#2-4-1-预测一个样本需要多少个神经元？" class="headerlink" title="2.4.1 预测一个样本需要多少个神经元？"></a>2.4.1 预测一个样本需要多少个神经元？</h4><p>论文利用基于梯度的贪心搜索来迭代的找到给定候选样本上的预测的最重要的神经元。<br>具体方法为：</p><ol><li>线性层：将某个神经元的权重置0</li><li>卷积层：将某一层的整个通道置0</li></ol><p>为了更鲁棒的找到某个样本的重要神经元，论文采用如下的方法（按我的理解是通过修改参数翻转该样本的预测结果，然后记录翻转过程中神经元的变化）：</p><ol><li>保持预测结果不变：最小化训练集随机batch的损失，最大化待翻转样本的损失</li><li>添加高斯噪声：在输入上添加噪声，并基于五组噪声样本预测结果的均值进行搜索（避免产生对抗样本）<br>模型记为$\overline{\mathcal {F_d}}$对于每个样本，上述过程需要独立的执行30次，然后平均30次的执行结果。寻找重要神经元的优化目标可以形式化为 ：<script type="math/tex; mode=display">\left[z_l\right]_j^*=\arg \max _{l, j}\left[\nabla_{\theta_l}\left(\ell\left(\overline{\mathcal{F}_d}\left(\mathbf{x}_i\right), \mathbf{y}_i\right)-\frac{1}{n} \mathcal{L}\left(\mathcal{S}, \overline{\mathcal{F}_d}\right)\right)\right]_j</script></li></ol><p>在卷积层的情况下，$[z_l]_j​$代表的是整个通道的激活值；在线性层的情况下，代表的是单个神经元的激活值。我们将该激活值置零，然后继续前向传播，直至该样本的预测翻转到错误类别。这一过程将迭代进行，直到样本的预测发生翻转。记录屏蔽的神经元数量。</p><blockquote><p>意思是找到一个神经元，使得将该神经元屏蔽（激活后的值置为0）后argmax里面的优化项值最大。优化项的含义是该样本的损失和训练集平均损失的差值在神经元上的梯度值。如果屏蔽了这个神经元后样本标签没有发生偏转则继续执行此过程。</p></blockquote><p>实验结果如下图所示。可以发现干净样本需要置0 的神经元个数显著多于噪声样本。这说明只有极少数神经元负责记忆错误标注样本（噪声样本、记忆样本）。超过90%的错误标注样本需要小于10个神经元。<br><img src="/2025/03/22/can-neural-network-memorization-be-localized-yue-du-bi-ji/Pasted image 20250322200928.png" alt="img"><br>论文将对样本重要的神经元屏蔽，并计算剩余训练样本在模型上的准确率。结果发现记忆噪声样本的神经元对总体准确率的影响远小于记忆干净样本的神经元，且屏蔽噪声样本的神经元比屏蔽干净样本的神经元的准确率高10%</p><p>此外，观察关键神经元的分布情况可以发现，关键神经元并没有分布在特定层。干净样本和记忆化样本的重要是神经元可能往往属于相似的层，而不是来自专门的层。可能有某些层对模型整体很重要。关键神经元分散在多个层说明记忆并不局限于特定的层，而是多个神经元共同负责。</p><p>未来研究可以利用关键神经元移除作为检测噪声样本的方法（噪声样本的关键神经元较少）。使用这种方法检测错误标注样本的效果优于现有方法。</p><h4 id="2-4-2-样本绑定dropout：对特定神经元进行定向记忆"><a href="#2-4-2-样本绑定dropout：对特定神经元进行定向记忆" class="headerlink" title="2.4.2 样本绑定dropout：对特定神经元进行定向记忆"></a>2.4.2 样本绑定dropout：对特定神经元进行定向记忆</h4><p>根据前面的观察，正确预测错误标记的样本所需的信息通常比干净样本所需的信息要少得多。更有趣的是，通过将干净样本的预测通过归零个别激活来改变，通常会显著增加模型在同一训练集和测试集上的整体准确性损失。这些发现促使尝试将样本的记忆化过程引导到事先固定的神经元上。</p><p>为了实现将神经元子集与每个样本连接的想法，我们为每层分配一个固定比例的神经元（记为pgen），这些神经元永远不会被丢弃。我们将这些神经元称为泛化神经元。在卷积层的情况下，为了保持通道结构，我们同样选择一个固定比例的通道（记为pgen），这些通道永远不会被丢弃。在剩余的部分中，我们为每个样本分配一个小子集的pmem神经元，这些神经元在采样特定样本时会被激活。目标是将所有特征学习引导到pgen神经元集合上，并将与样本特定记忆化相关的神经元引导到pmem神经元集合上。在测试时，我们会丢弃对应于pmem的神经元，即将其激活归零，从而调整模型，使得可以去除记忆化的影响。</p><p>样本绑定丢弃的实验表明，确实可以将样本的记忆化限制在预先确定的神经元集合中。在ResNet 9模型的每一层后，我们添加一层“样本绑定丢弃”，并在MNIST、SVHN和CIFAR10数据集上进行训练，训练设置中有10%的均匀随机标签噪声。在丢弃记忆化神经元后，三个数据集的错误标记样本的准确率分别降至0.1%、1.4%和3.1%，尽管模型在训练时几乎完美拟合了所有的噪声数据。在所有情况下，对干净样本的有效影响分别为0.8%、4.2%和9.2%，同时降低了训练和测试性能之间的泛化差距。这表明，pgen比例的（泛化）神经元已经包含了分类数据集原型样本所需的特征。</p><p>为了理解在移除记忆化神经元后，干净样本准确率下降的原因，我们发现，大多数模型不再正确预测的样本实际上要么是标记错误和模糊不清的样本，要么是数据集中具有例外性和独特性的样本，模型必须记忆这些样本才能正确预测（见下图）。<br><img src="/2025/03/22/can-neural-network-memorization-be-localized-yue-du-bi-ji/Pasted image 20250322204446.png" alt="img"></p><p>为了评估pgen和pmem的取值对样本绑定丢弃方法在神经元中定位记忆化的能力的影响，我们在12种不同的参数组合上进行了网格搜索。表2展示了这些值变化对方法有效性变化的结果。我们发现，该方法对pgen和pmem的广泛取值（和组合）具有鲁棒性。一般来说，我们观察到一个趋势：随着泛化神经元容量的增加，记忆化神经元中的记忆化定位持续减少（即，在丢弃记忆化神经元后，噪声样本的准确率提高）。与此同时，这也导致了在丢弃记忆化神经元后，干净样本准确率的提高。</p><p><strong>和其它baseline的对比测试</strong>：对于给定的pgen值，我们选择两个等效的模型：（a）使用标准丢弃法，p = pgen；（b）使用稀疏网络，剩余参数比例 = pgen。然后，我们最终比较在CIFAR-10数据集上训练50个epoch后，模型在干净样本和噪声样本上的准确率，使用ResNet-9模型。在样本绑定丢弃的情况下，我们丢弃记忆化神经元，同时在表3中测量准确率。我们发现，在相同的稀疏性水平下，当样本绑定丢弃能够将记忆化引导到选定的神经元集时，稀疏和标准丢弃方法仍然能够过拟合训练集（包括错误标记的样本）。</p><h2 id="3-结论和收获"><a href="#3-结论和收获" class="headerlink" title="3. 结论和收获"></a>3. 结论和收获</h2><p>巧思的实验设计，严谨的论证过程，规范的行文结构。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 模型记忆性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Property Existence Inference against Generative Models 阅读笔记</title>
      <link href="/2025/03/10/property-existence-inference-against-generative-models-yue-du-bi-ji/"/>
      <url>/2025/03/10/property-existence-inference-against-generative-models-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Property-Existence-Inference-against-Generative-Models-阅读笔记"><a href="#Property-Existence-Inference-against-Generative-Models-阅读笔记" class="headerlink" title="Property Existence Inference against Generative Models 阅读笔记"></a>Property Existence Inference against Generative Models 阅读笔记</h1><p>论文题目：Property Existence Inference against Generative Models<br>发表时间：2024<br>发表期刊/会议：USENIX, CCF A</p><h2 id="内容简介"><a href="#内容简介" class="headerlink" title="内容简介"></a>内容简介</h2><p>这篇文章介绍了一种针对生成模型的属性存在攻击。该攻击的目标是判断部分特征是不是在训练集当中。<br>贡献：</p><ul><li><strong>首次提出属性存在性推理攻击</strong>：据我们所知，本研究是<strong>首个</strong>针对<strong>生成模型</strong>执行<strong>属性存在推理</strong>（Property Existence Inference, PEI）的工作，并强调<strong>应特别关注训练集中占比极低的属性</strong>。</li><li><strong>提出属性存在推理方法</strong>：我们利用<strong>生成样本与目标属性样本的相似性差异</strong>，判断目标属性是否存在于训练集中，从而实现<strong>高效的属性存在性推理攻击</strong>。</li><li><strong>在最先进的生成模型上进行全面评估</strong>：我们在<strong>包括 Stable Diffusion 在内的大规模生成模型</strong>上进行了<strong>系统性实验</strong>，评估了该方法的有效性，并探讨了<strong>哪些生成样本最容易泄露属性存在性信息</strong>。</li></ul><h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><h3 id="基础设定"><a href="#基础设定" class="headerlink" title="基础设定"></a>基础设定</h3><p><strong>理论框架</strong>：假设检验<br>定义1 属性存在推理:给定一个模型和两个关于训练集分布的假设，攻击者需要访问模型来判断这两个假设哪个是真的。</p><script type="math/tex; mode=display">\mathcal H_0: \text{The proportion of samples with target property P in the training set of the target model is 0}.\\</script><script type="math/tex; mode=display">\mathcal H_1:\text{The proportion of samples with target property P is larger than 0.}</script><p><strong>攻击者能力</strong>：黑盒，可以访问数据分布并从其中采样出$D_{out}$和$D_A$，分别表示与训练集属性不同的数据集与含有目标属性$\mathbb P$的数据集。攻击者可以利用目标生成模型采样出数据集$D_{gen}$</p><h3 id="攻击算法"><a href="#攻击算法" class="headerlink" title="攻击算法"></a>攻击算法</h3><p>首先训练一个属性提取器将$D_A$和$D_{gen}$根据不同的目标属性值映射到嵌入空间中，然后根据$D_A,D_{gen}$在嵌入空间的相似度对目标属性的存在性进行评分，最终利用影子模型设定攻击阈值。</p><p><strong>属性提取器训练</strong>：使用同样的分类标准去划分样本的标签。比如我想知道目标训练集中是否有红色头发的人，就把相同颜色头发的图像划为positive样本，不同头发颜色的图像划为negtive。使用三元组数据集训练属性提取器，三元组中有一个携带特定属性的基图像，以及该图像的postive样本和negtive样本各一个。属性提取器会将所有的图像映射到嵌入空间，并最小化基图像和pos样本的cos相似度，最大化基图像和neg样本的cos相似度。</p><p><strong>相似度计算</strong>：使用含有目标属性的数据和生成模型生成的数据之间的相似度来判断目标属性在不在训练集中。算法如下图。对于每张带有目标属性的图像（即对手收集的锚点图像，anchor image）计算其与所有生成图像（经过属性提取后）的余弦相似度（详见算法 1 的第 12-14 行）。随后，在所有生成图像的相似度分数中，选取 K 个最高相似度值（top K）并求和，作为该锚点图像的最终相似性评分（详见算法 1 的第 11-23 行）。这些top K 值代表了最可能受到目标属性影响的生成图像。这些相似性评分共同构成了针对目标属性的相似性评分。<br>在计算属性相似性评分的过程中采用了两种操作——相似性评分平滑（similarity score smoothing）和似然校准（likelihood calibration），以减少锚点图像和生成图像带来的不确定性。</p><p><img src="/2025/03/10/property-existence-inference-against-generative-models-yue-du-bi-ji/Pasted image 20250310120403.png" alt="img"></p><p><strong>区分度测试</strong>：确定阈值。使用$D_{out}$中的部分数据训练影子模型，选取用于训练的属性和未被用于训练的属性计算对应的相似性评分，并将两类评分建模为高斯分布:$\mathcal N(\mu_0,s_0^2), \mathcal N(\mu_1,s_1^2)$，前者表示未使用目标属性的分布，后者表示使用目标属性的分布。阈值确定为：</p><script type="math/tex; mode=display">T = \frac{\mu_0 s_1^2 - \mu_1 s_0^2}{s_1^2 - s_0^2} \pm 2 s_1 s_0 \sqrt{\left( \frac{\mu_1 - \mu_0}{2} \right)^2 + \frac{s_0^2}{s_1^2} \log \left( \frac{s_0}{s_1} \right) \frac{s_1^2}{s_0^2}}</script>]]></content>
      
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 属性存在推理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>High-Dimensional Probability 第一章笔记</title>
      <link href="/2025/02/27/high-dimensional-probability-bi-ji/"/>
      <url>/2025/02/27/high-dimensional-probability-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>书名：High-Dimensional Probability An Introduction with Applications in Data Science<br>. 这一张章主要介绍概率论的一些基础知识。笔记中只简单记录结论，证明过程可以参阅原书</p></blockquote><h1 id="1-1基础概念："><a href="#1-1基础概念：" class="headerlink" title="1.1基础概念："></a>1.1基础概念：</h1><ul><li>随机变量X，期望（均值）$\mathbb{E}(x)$，方差$Var(x)=\mathbb{E}(X-\mathbb{E}(X))^2$</li><li>矩母生成函数$M_X(t) = \mathbb{E}[e^{tX}],t\in\mathbb R$。更多信息参考<a href="https://blog.csdn.net/goldtimes/article/details/139370411">这篇博客</a></li><li>对于$p&gt;0$，$X$的p阶矩为$\mathbb{E}X^p$，p阶绝对矩为$\mathbb{E}|X|^p$</li><li>$X$的$L^p$范数为p阶绝对矩开p次方，即$||X||_{L^p}=(\mathbb{E}X^p)^{\frac{1}{p}}$</li><li>$X$的无穷范数被定义为$|X|$的本质上确界$|X|_{L^{\infty}}=\operatorname{ess} \sup |X|$</li><li>对于一个概率空间$(\Omega,\Sigma,\mathbb P)$和一个固定的$p$，经典向量空间$L^p=L^p(\Omega,\Sigma,\mathbb P)$包含$\Omega$中所有范数有穷的随机变量$X$，即$L^p=\left\{X:|X|_{L^p}&lt;\infty\right\}$</li><li>如果$p\in [1,\infty]$，则$|X|_{L^p}$是一个范数且$L^p$是一个巴拿赫（Banach）空间。如果p小于1，则三角不等式失效且$||X||_{L^p}$不是一个范数。</li><li>如果p=2，则$L^2$同时是一个希尔伯特空间，该空间中的内积和协方差为：$\langle X, Y\rangle_{L^2}=\mathbb{E} X Y$,$\operatorname{cov}(X, Y)=\mathbb{E}(X-\mathbb{E} X)(Y-\mathbb{E} Y)=\langle X-\mathbb{E} X, Y-\mathbb{E} Y\rangle_{L^2}$</li><li>当我们把希尔伯特空间中的随机变量看成是空间中的向量时，上面的公式说明了$X-\mathbb E(X)$和$Y-\mathbb E(Y)$越匹配，他们的协方差和内积就越大。</li></ul><h2 id="1-2一些基础不等式"><a href="#1-2一些基础不等式" class="headerlink" title="1.2一些基础不等式"></a>1.2一些基础不等式</h2><p><strong>詹森不等式（Jensen’s inequality）</strong>：对于任意一个随机变量$X$和一个凸函数$\varphi:\mathbb R \to \mathbb R$，都有：</p><script type="math/tex; mode=display">\varphi(\mathbb EX)\le\mathbb E\varphi(X)</script><p>$||X||_{}L^p$是一个关于p的单调上升函数，即:</p><script type="math/tex; mode=display">\|X\|_{L^p} \leq\|X\|_{L^q} \quad \text { for any } 0 \leq p \leq q \leq \infty</script><p><strong>闵可夫斯基不等式</strong>（Minkowski’s inequality）：对于任意$p\in[1,\infty]$和随机变量$X,Y\in L^p$，有：</p><script type="math/tex; mode=display">\|X+Y\|_{L^p} \leq\|X\|_{L^p}+\|Y\|_{L^p}</script><p><strong>柯西-施瓦兹不等式</strong>（Cauchy-Schwarz）：对于任意的随机变量$X,Y\in L^2$，有：</p><script type="math/tex; mode=display">|EXY|\le||X|_{L^2}||Y|_{L^2}</script><p>更一般的，根据<strong>赫尔德不等式</strong>，如果$p,q\in(1,\infty)$是共轭的，即$1/p+1/q=1$，有：</p><script type="math/tex; mode=display">|EXY|\le||X|_{L^p}||Y|_{L^q}</script><p>当$p=1,q=\infty$时同样成立</p><p>对于一个随机变量X，它的分布由累积分布函数（CDF）决定：</p><script type="math/tex; mode=display">F_X(t)=\mathbb P\{X\le t\}</script><p>在处理一些问题(关于期望或者矩)时，使用1-CDF会更方便，即$\mathbb P(X&gt;t)=1-F_X(t)$</p><p><strong>引理1.2.1</strong>：$X$是一个<strong>非负</strong>的随机变量，有：</p><script type="math/tex; mode=display">\mathbb{E} X=\int_0^{\infty} \mathbb{P}\{X>t\} d t</script><p><strong>练习1.2.2（引理1.2.1的泛化版）</strong>：对于任意一个随机变量，有：</p><script type="math/tex; mode=display">\mathbb E X=\int_0^\infty\mathbb P\{X>t\}dt-\int_{-\infty}^0\mathbb P\{X<t\}dt</script><p><strong>练习1.2.3</strong>：$X$是一个随机变量，$p\in(0,\infty)$，当下面的右式有穷时有：</p><script type="math/tex; mode=display">\mathbb E(|X|^p)=\int_0^\infty pt^{p-1}\mathbb P\{|X|>t\}dt</script><p><strong>马尔可夫不等式</strong>（提供了CDF尾部的bound）：对于任意一个<strong>非负</strong>随机变量$X$和一个$t&gt;0$，有：</p><script type="math/tex; mode=display">\mathbb P\{X\ge t\}\le \frac{\mathbb EX}{t}</script><p><strong>切比雪夫不等式</strong>（马尔可夫不等式的结论之一）：对于任意一个随机变量$X$和一个$t&gt;0$，有：</p><script type="math/tex; mode=display">\mathbb P\{|X-\mu|\ge t\}\le \frac{\sigma^2}{t^2}</script><h2 id="1-3-极限定理"><a href="#1-3-极限定理" class="headerlink" title="1.3 极限定理"></a>1.3 极限定理</h2><p><strong>定理1.3.1 大数定理</strong>：对于均值为$\mu$的独立同分布(i.i.d.)随机变量序列$X_1,X_2,\dots$，他们的和$S_N=\sum X_i$，当$N\to\infty$时，有：</p><script type="math/tex; mode=display">\frac{S_N}{N}\to\mu</script><p>原因是i.i.d.的随机变量序列满足$Var(\sum X_i)=\sum Var(X_i)$，两边同除以$N^2$有$Var(\frac{1}{N}\sum X_i)=\frac{\sigma^2}{N^2}$。当$N\to\infty$，该式趋近于0，故随机变量序列的均值强趋近于$\mu$。</p><p><strong>定理1.3.2 中心极限定理</strong>：$X_1,X_2,\dots$是i.i.d.的随机变量序列，均值和方差分别为$\mu,\sigma$。考虑$S_N=X_1+\dots+X_N$,并对其进行标准化：</p><script type="math/tex; mode=display">Z_N:=\frac{S_N-\mathbb ES_N}{Var(S_N)}=\frac{1}{\sigma\sqrt{N}}\sum_{i=1}^N(X_i-\mu)</script><p>当$N\to\infty$，在分布上$Z_N\to N(0,1)$<br>可以使用CDF尾部表示为，当$N\to\infty$时，有：</p><script type="math/tex; mode=display">\mathbb P\{Z_N\ge t\}\to \mathbb P\{g\ge t\}=\frac{1}{\sqrt{2\pi}}\int_t^\infty e^{-x^2/2}dx</script><p>其中$g\sim N(0,1)$</p><p><strong>练习1.3.3</strong>：令i.i.d.的随机变量序列$X_i$，均值为$\mu$且方差有穷。证明：</p><script type="math/tex; mode=display">\mathbb E \left|\frac{1}{N}\sum X_i -\mu \right|=O(\frac{1}{\sqrt{N}}) \quad as\quad N\to\infty</script><p>定理<strong>1.3.4泊松极限定理</strong>：令$X_{N,i},1\le i\le N$是独立随机变量，服从伯努利分布$Ber(p_{N,i})$。令$S_N=\sum_{i=1}^{N}X_{N,i}$，假设$N\to \infty$时有：</p><script type="math/tex; mode=display">\max_{i\le N}p_{N,i}\to0,\quad\mathbb ES_N=\sum_{i=1}^N p_{N,i}\to \lambda<\infty</script><p>则当$N\to\infty$时，有：</p><script type="math/tex; mode=display">S_N\to Pois(\lambda)</script>]]></content>
      
      
      <categories>
          
          <category> High-Dimensional Probability </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高维概率论 </tag>
            
            <tag> 阅读笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Denoising Diffusion Probabilistic Models 阅读笔记</title>
      <link href="/2025/02/17/denoising-diffusion-probabilistic-models-yue-du-bi-ji/"/>
      <url>/2025/02/17/denoising-diffusion-probabilistic-models-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Denoising-Diffusion-Probabilistic-Models-阅读笔记"><a href="#Denoising-Diffusion-Probabilistic-Models-阅读笔记" class="headerlink" title="Denoising Diffusion Probabilistic Models 阅读笔记"></a>Denoising Diffusion Probabilistic Models 阅读笔记</h1><p>论文题目：Denoising Diffusion Probabilistic Models</p><p>发表时间：2020</p><p>发表期刊/会议：NIPS  期刊   CCF-A<br>论文作者：Jonathan Ho et al.</p><h2 id="1-内容简介"><a href="#1-内容简介" class="headerlink" title="1. 内容简介"></a>1. 内容简介</h2><p>介绍扩散模型的基本内容。参考”Understanding Diffusion Models: A Unified Perspective”</p><h2 id="2-主要内容"><a href="#2-主要内容" class="headerlink" title="2. 主要内容"></a>2. 主要内容</h2><h3 id="2-1-理论基础"><a href="#2-1-理论基础" class="headerlink" title="2.1 理论基础"></a>2.1 理论基础</h3><p>扩散模型将模型的训练过程分为加噪过程和去噪过程。加噪过程需要在原始的图像$x_0$上添加T次高斯噪声形成一个纯噪声图像$x_T$，这个过程和模型无关。去噪过程可以看成是加噪过程的逆过程，模型需要逐步预测噪声并在$x_T$上将这些噪声减去从而恢复出原始图像$x_0$。加噪过程和去噪过程都是马尔可夫过程。</p><p>加噪过程遵循状态转移方程$x_{t}=\sqrt{\alpha_t}x_{t-1}+\sqrt{\beta_t}\epsilon_t,\epsilon_t \sim \mathcal{N}(0,I)$  其中$\alpha_t,\beta_t$都是预设的一组值，$\beta_t = 1-\alpha_t$ 。设$\bar{\alpha_t}=\prod_{i=1}^{i=t}\alpha_i$ ，$\bar{\beta_t}$同理。不断地迭代状态转移方程可以得到:</p><script type="math/tex; mode=display">x_t=\sqrt{\bar{\alpha_t}}x_{0}+\sqrt{1-\bar{\alpha_t}}\epsilon,\quad \epsilon \sim \mathcal{N}(0,I)</script><p>即$x_t$和$x_0$可以互相转化。</p><p>去噪过程是加噪过程的逆过程。由于模型对噪声建模比较容易，故去噪过程会让模型对第t步的噪声进行预测，再利用$x_t$和$x_{t-1}$的转移关系得到前一步图像。</p><p>下面介绍如何推导出模型的损失函数。参考论文”Understanding Diffusion Models: A Unified Perspective“进行讲解。<br>设加噪过程的状态转移服从分布$q(x_t|x_{t-1})$, 去噪过程的状态转移服从分布$p_\theta(x_{t-1}|x_t)$ 。我们的目标是模型经过t次的去噪后可以得到原始图像$x_0$，即：</p><script type="math/tex; mode=display">\underset{\theta}{max}\quad p_\theta(x_0)</script><p>贝叶斯展开得到：</p><script type="math/tex; mode=display">p_\theta(x_0)=\frac{p_\theta(x_{0:T})}{p_\theta(x_{1:T}|x_0)}</script><p>取对数然后取期望：</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}_{q(x_{1:T}|x_0)}\log p_\theta(x_0)&=\mathbb{E}_{q(x_{1:T}|x_0)}\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}-\mathbb{E}_{q(x_{1:T}|x_0)}\log\frac{p_\theta(x_{1:T}|x_0)}{q(x_{1:T}|x_0)}\\&=\mathbb{E}_{q(x_{1:T}|x_0)}\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}+KL(q(x_{1:T}|x_0)||p_\theta(x_{1:T}|x_0))\end{aligned}</script><p>由于KL散度恒大于等于0，故我们可以直接最大化下界$\mathbb{E}_{q(x_{1:T}|x_0)}\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}$。对当前结果取负值，即最小化$\mathbb{E}_{q(x_{1:T}|x_0)}-\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}$。</p><p>利用马尔可夫过程的无后效性，对分子分母分别进行展开：</p><script type="math/tex; mode=display">p_\theta(x_{0:T})=p(x_T)\prod_{t=1}^Tp_\theta(x_{t-1}|x_t)</script><script type="math/tex; mode=display">q(x_{1:T}|x_0)=\prod_{t=1}^Tq(x_t|x_{t-1})</script><p>得到：</p><script type="math/tex; mode=display">\begin{aligned}-\mathbb{E}_q\left[\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right]&=-\mathbb{E}_q\left[\log p(x_T)+\sum_{t\geq1}\log\frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\right]\\&=-\mathbb{E}_q\left[\log p(x_T)+\log\frac{p_\theta(x_0|x_1)}{q(x_1|x_0)}+\sum_{t\ge2}\log\frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\right]\end{aligned}</script><p>由于：</p><script type="math/tex; mode=display">\begin{aligned}q(x_t|x_{t-1}) &= q(x_t|x_0,x_{t-1})\\&=\frac{q(x_0,x_{t-1},x_t)}{q(x_0,x_{t-1})}\\&=\frac{q(x_0,x_{t-1},x_t)q(x_0,x_t)q(x_0)}{q(x_0,x_t)q(x_0)q(x_0,x_{t-1})}\\&=\frac{q(x_{t-1}|x_0,x_t)q(x_t|x_0)}{q(x_{t-1}|x_0)} \end{aligned}</script><blockquote><p>上面这是咋想出来的步骤……倒推容易，正推是咋分析出来的</p></blockquote><p>代入，裂项相消得到：</p><script type="math/tex; mode=display">\begin{aligned}-\mathbb{E}_q\left[\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right]&=-\mathbb{E}_{q}\left[\log\frac{p(x_{T})p_{\boldsymbol{\theta}}(x_{0}|x_{1})}{q(x_{T}|x_{0})}+\sum_{t=2}^{T}\log\frac{p_{\boldsymbol{\theta}}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_{0})}\right]\\&=-\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_{0})}\left[\log p_{\boldsymbol{\theta}}(x_{0}|x_{1})\right]-\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_{0})}\left[\log\frac{p(\boldsymbol{x}_{T})}{q(\boldsymbol{x}_{T}|x_{0})}\right]-\sum_{t=2}^{T}\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_{0})}\left[\log\frac{p_{\boldsymbol{\theta}}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_{0})}\right]\\&=\underbrace{-\mathbb{E}_{q(\boldsymbol{x}_1|\boldsymbol{x}_0)}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}_0|\boldsymbol{x}_1)\right]}_{\text{reconstruction term}}+\underbrace{D_{\mathrm{KL}}(q(\boldsymbol{x}_T|\boldsymbol{x}_0)\parallel p(\boldsymbol{x}_T))}_{\text{prior matching term}}+\sum_{t=2}^T\underbrace{\mathbb{E}_{q(\boldsymbol{x}_t|\boldsymbol{x}_0)}\left[D_{\mathrm{KL}}(q(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{x}_0)\parallel p_{\boldsymbol{\theta}}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t))\right]}_{\text{denoising matching term}}\\&=L_0+L_T+\sum_{t\ge2} L_{t-1}\end{aligned}</script><blockquote><p>没看懂期望角标的变化。这个<a href="https://blog.csdn.net/gaofeipaopaotang/article/details/139247321">链接</a>有一些推导但跟论文里的还不太一样</p></blockquote><p>结果左项表示重构项，可以用蒙特卡罗方法近似优化。中间项和模型参数无关，不管。最后一项是一个去噪匹配项。q项表示了真实的信号，定义了如何对样本进行去噪。其值越小说明去噪结果和真实信号越相近。</p><p>最右边的一项是优化重点。p是模型的输出，q可以使用高斯概率分布公式代入化简看出他本身还是一个高斯分布。过程如下：<br><img src="/2025/02/17/denoising-diffusion-probabilistic-models-yue-du-bi-ji/Pasted image 20250217012154.png" alt="img"><br>在给定$x_t$和$x_0$的情况下，q的均值是固定的。模型的输出可以直接看作是P的均值。故可以将该优化问题转化为最小化模型输出和分布均值之间的距离：</p><script type="math/tex; mode=display">L_{t-1} = \mathbb{E}_q||{\mu}_q(x_t,x_0),\mu_\theta(x_t,t)||^2</script><p>根据$x_t,x_0$之间的转换关系，将$x_0$用$x_t$表示并代入到$\mu_q$中，得到：</p><script type="math/tex; mode=display">\mu_q(x_t,x_0)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon\right)</script><p>$\mu_\theta$可以用同样形式的式子表示，其中$\hat{\epsilon}$是模型预测结果：</p><script type="math/tex; mode=display">\mu_\theta(x_t,t) = \frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \hat{\epsilon}\right)</script><p>代入$L_{t-1}$得到简化的损失函数，即直接计算预测噪声和真实噪声之间的距离：</p><script type="math/tex; mode=display">L_{t-1} = \mathbb{E}_{x_0,\epsilon}||\epsilon,\hat{\epsilon}||^2</script><h3 id="2-2-训练算法"><a href="#2-2-训练算法" class="headerlink" title="2.2 训练算法"></a>2.2 训练算法</h3><p><img src="/2025/02/17/denoising-diffusion-probabilistic-models-yue-du-bi-ji/Pasted image 20250217154744.png" alt="img"><br>观察可以看到采样过程第四步会再添加一次噪声z。根据<a href="https://www.bilibili.com/video/BV1mLbQeRExa">李宏毅的课程</a>得知在生成模型的生成过程中添加随机性可以提高生成的成功率。</p><h2 id="结论和收获"><a href="#结论和收获" class="headerlink" title="结论和收获"></a>结论和收获</h2><p>复习了一下数学，了解了扩散模型的理论基础。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 成员推理 </tag>
            
            <tag> 扩散模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Are Diffusion Models Vulnerable to Membership Inference Attacks 阅读笔记</title>
      <link href="/2024/12/29/are-diffusion-models-vulnerable-to-membership-inference-attacks-yue-du-bi-ji/"/>
      <url>/2024/12/29/are-diffusion-models-vulnerable-to-membership-inference-attacks-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Are-Diffusion-Models-Vulnerable-to-Membership-Inference-Attacks-阅读笔记"><a href="#Are-Diffusion-Models-Vulnerable-to-Membership-Inference-Attacks-阅读笔记" class="headerlink" title="Are Diffusion Models Vulnerable to Membership Inference Attacks 阅读笔记"></a>Are Diffusion Models Vulnerable to Membership Inference Attacks 阅读笔记</h1><p>论文题目：Are Diffusion Models Vulnerable to Membership Inference Attacks?<br>发表时间：2023<br>发表期刊/会议：ICML<br>论文作者：Duan et al.</p><h2 id="1-内容简介"><a href="#1-内容简介" class="headerlink" title="1. 内容简介"></a>1. 内容简介</h2><p>这篇文章通过实验说明现有的成员推理攻击对扩散模型几乎无效。同时设计了一种新的针对扩散模型的攻击方法。主演贡献如下：</p><ul><li>研究扩散模型在成员推理攻击上的脆弱性。总结了传统的MIAs方法，并评估了它们在扩散模型上的表现。结果表明，这些方法大多无效。</li><li>提出了SecMI，这是一种基于查询的MIA方法，依赖于对前向过程后验估计的误差比较。我们将SecMI应用于标准扩散模型（如DDPM）以及当前最先进的文本到图像扩散模型（如Stable Diffusion）。</li><li>在多个数据集上评估了SecMI，并报告了攻击性能，包括低假阳率（False-Positive Rate, FPR）下的真阳性率（True-Positive Rate, TPR）。实验结果表明，SecMI在所有实验设置中均能准确推断成员关系，平均攻击成功率（ASR）≥ 0.80，接收者操作特性曲线下面积（AUC）≥ 0.85。</li></ul><h2 id="2-主要内容"><a href="#2-主要内容" class="headerlink" title="2. 主要内容"></a>2. 主要内容</h2><h3 id="2-1-问题描述"><a href="#2-1-问题描述" class="headerlink" title="2.1 问题描述"></a>2.1 问题描述</h3><p>存在两个数据集$D_M,D_H$。M表示该数据集为成员集合（member set），H表示非成员集合（hold-out set）。待推理集合$D=D_M\cup D_M$ 。攻击者需要利用攻击算法$\mathcal{M}$推理出$\forall x_i \in D，x_i$是否在$D_M$中。</p><h3 id="2-2-分析现有攻击失败原因"><a href="#2-2-分析现有攻击失败原因" class="headerlink" title="2.2 分析现有攻击失败原因"></a>2.2 分析现有攻击失败原因</h3><ol><li>之前攻击使用了较少的训练集，使得模型过拟合严重。</li><li>扩散模型拟合的更好。前面工作假设如果一个样本更容易被生成器生成则它就有可能是成员。但是这个假设只有在模型过拟合的情况下才成立，即$d\left(p_\theta, p_{D_M}\right)&lt;d\left(p_\theta, p_{D_H}\right)$。$p_\theta,p_{D_M},p_{D_H}$分别表示的是生成器分布，成员分布和非成员分布。论文验证了25000个合成数据和25000个成员/非成员之间的距离，发现距离数值分别为9.66和9.85。说明扩散模型并不会对成员样本产生明显偏斜。</li></ol><h3 id="2-3-攻击方法"><a href="#2-3-攻击方法" class="headerlink" title="2.3 攻击方法"></a>2.3 攻击方法</h3><p>建立t-error评分方法。对于一个$x_0 \sim D$和它在第t步的反向结果$\tilde{x}_t = \Phi_\theta(x_0,t)$，第t步的后验估计误差为:</p><script type="math/tex; mode=display">\tilde{\ell}_{t, \boldsymbol{x}_0}=\left\|\psi_\theta\left(\phi_\theta\left(\tilde{\boldsymbol{x}}_t, t\right), t\right)-\tilde{\boldsymbol{x}}_t\right\|^2</script><p>如下图所示，非成员集合的t-error通常更高，尤其是t接近0的时候。原因是更小的t会触发模型更多的记忆。<br><img src="/2024/12/29/are-diffusion-models-vulnerable-to-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241229202440.png" alt="img"><br>基于此，改论文设计了两种攻击方法：</p><ol><li>基于统计的推理：如果t-error小于某个阈值，就将其判定为成员。</li><li>基于NN的推理：训练一个二分类神经网络进行推理</li></ol><h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><h3 id="3-1-数据集和模型"><a href="#3-1-数据集和模型" class="headerlink" title="3.1 数据集和模型"></a>3.1 数据集和模型</h3><p>使用四个数据集CIFAR-10/100, STL10-U, TinyIN训练模型。使用ResNet-18作为攻击的backbone。使用Stable Diffusion作为目标模型。</p><h3 id="3-2-攻击结果"><a href="#3-2-攻击结果" class="headerlink" title="3.2 攻击结果"></a>3.2 攻击结果</h3><p>在各个数据集上的攻击结果如下图所示。对比GAN-leak方法可以看到攻击效果有明显提升。<br><img src="/2024/12/29/are-diffusion-models-vulnerable-to-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241229203909.png" alt="img"><br><img src="/2024/12/29/are-diffusion-models-vulnerable-to-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241229210142.png" alt="img"></p><h3 id="3-3消融实验"><a href="#3-3消融实验" class="headerlink" title="3.3消融实验"></a>3.3消融实验</h3><p>总体而言算法表现稳定，方差较小，例如，所有试验中AUC和ASR的方差均不超过0.05。对于时间步 tSECt，尽管我们是基于经验来确定 tSECt_{\text{SEC}}tSEC​，但实验表明攻击性能对具体时间步并不敏感。只要 50≤tSEC≤150，攻击就会有效。这一特性在其他实验中也具有良好的泛化性。<br>查询次数 k会影响查询效率，但实验表明，即使只有3次查询，SecMI仍能实现显著的攻击效果。在权重调整（WA）方面，一个有趣的现象是，WA会带来一定程度的隐私泄露，这提醒研究社区在选择训练协议时需要更加谨慎。<br><img src="/2024/12/29/are-diffusion-models-vulnerable-to-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241229210302.png" alt="img"></p><p>下表展示了该攻击面对不同防御方法时的效果。攻击的指标都出现了不同程度的下降。<br><img src="/2024/12/29/are-diffusion-models-vulnerable-to-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241229210755.png" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 成员推理 </tag>
            
            <tag> 扩散模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Do Membership Inference Attacks Work on Large Language Models 阅读笔记</title>
      <link href="/2024/11/30/do-membership-inference-attacks-work-on-large-language-models-yue-du-bi-ji/"/>
      <url>/2024/11/30/do-membership-inference-attacks-work-on-large-language-models-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="论文研读笔记"><a href="#论文研读笔记" class="headerlink" title="论文研读笔记"></a>论文研读笔记</h2><p>论文题目：Do Membership Inference Attacks Work on Large Language Models?<br>发表时间：2024<br>发表期刊/会议：COLM<br>论文作者：Duan et al.</p><h2 id="1-内容简介"><a href="#1-内容简介" class="headerlink" title="1. 内容简介"></a>1. 内容简介</h2><p>论文针对大语言模型的成员推理攻击进行了广泛的测试。测试说明成员推理攻击对大语言模型的效果近似于随机猜测，并对该现象的产生做出了解释。</p><h2 id="2-主要内容"><a href="#2-主要内容" class="headerlink" title="2. 主要内容"></a>2. 主要内容</h2><p>改论文主要通过实验验证成员推理攻击对大语言模型的有效性。假设$\mathcal{M}$是目标模型，论文考虑了如下的几种攻击方法：</p><ul><li>基于损失LOSS：只要目标样本在模型上的损失小于某个阈值就判定其为成员</li><li>基于参考模型的攻击：如果样本在目标模型上的损失小于参考模型上的损失，就认为是成员</li><li>Zlib entropy：使用 zlib entropy作为损失</li><li>近邻攻击：对样本进行变形，如果样本本身的损失小于变形样本的均值就认为是成员</li><li>min-k% 概率：使用置信度向量中最低的k%分量计算损失</li></ul><h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><p><strong>模型</strong>：主要针对PYTHIA模型套件，包括：（1）五个PYTHIA模型（Biderman et al., 2023b），其参数分别为160M、1.4B、2.8B、6.7B和12B，训练数据为原始的Pile数据（Gao et al., 2020）；（2）五个PYTHIA-DEDUP模型（Biderman et al., 2023b），具有与PYTHIA相同的参数数量，但训练数据为去重后的Pile数据。我们还使用GPT-NEO家族的模型进行实验，以验证在不同模型家族中的发现，观察到大多数领域的趋势类似（见附录A.6）。</p><p><strong>数据集</strong>：Pile中包含的七个多样化数据源：通用网页（Pile-CC）、知识来源（维基百科）、学术论文（PubMed Central，ArXiv）、对话数据（HackerNews）以及特定领域数据（DM Math，Github）。</p><p><strong>评估指标</strong>：AUC， ROC，TPR@low%FPR</p><h3 id="3-1-主要结果"><a href="#3-1-主要结果" class="headerlink" title="3.1 主要结果"></a>3.1 主要结果</h3><p>由下表可知，现在所有的针对大模型的成员推理都接近于随机猜测。总体上基于参考模型的推理表现最佳，而且发现训练集的去重降低了MIA的表现。</p><p><img src="/2024/11/30/do-membership-inference-attacks-work-on-large-language-models-yue-du-bi-ji/Pasted image 20241130182805.png" alt="img"></p><h3 id="3-2-为什么MIA表现不佳"><a href="#3-2-为什么MIA表现不佳" class="headerlink" title="3.2 为什么MIA表现不佳"></a>3.2 为什么MIA表现不佳</h3><h4 id="3-2-1-LLM的特性"><a href="#3-2-1-LLM的特性" class="headerlink" title="3.2.1 LLM的特性"></a>3.2.1 LLM的特性</h4><p><strong>训练集规模</strong>：当前最先进的大语言模型都使用巨量的数据训练模型，而模型的泛化程度会随着数据集的变大而变好。在这论文使用PYTHIA-DEDUP模型套件中的中间检查点来评估不同规模训练数据的影响。论文对每个检查点的最近100个步骤中采样成员，以去除成员的时效性偏差影响。MIA的表现一开始接近随机猜测，后来迅速升高，最后逐渐下降（如下图左）。开始性能低可能是因为模型在热身，对成员和非成员的损失都很高。后面随着训练的进行，由于数据和参数的比例较小，模型会出现过拟合。但最后随着训练的进行模型的泛化能力会加强。</p><p><strong>训练轮次</strong>：增加训练轮次会增强MIA性能，但也会增加隐私泄露的风险（下图右）。论文对Datablations模型套件（Muennighoff et al., 2023）进行了MIA测试，该模型套件包含在不同epoch数量下，使用C4（Raffel et al., 2019）训练数据的子集训练的模型。<br><img src="/2024/11/30/do-membership-inference-attacks-work-on-large-language-models-yue-du-bi-ji/Pasted image 20241130184850.png" alt="img"></p><h4 id="3-2-2-LLM固有的模糊性"><a href="#3-2-2-LLM固有的模糊性" class="headerlink" title="3.2.2 LLM固有的模糊性"></a>3.2.2 LLM固有的模糊性</h4><p>自然语言中存在重复文本，包括相近的表达方式、相似文本的自然使用、特定领域内的固有说法等，这就导致成员和非成员之间的差异没有那么明显。论文使用n-gram重叠比来量化重叠。对于一个m个词组成的成员样本$x=x_1x_2\dots x_m$以及其中的一个n-gram$x_i\dots x_{i+n-1}$，x在数据集D上的n-gram重叠定义为：</p><script type="math/tex; mode=display">\frac1{m-n+1}\sum_{i=1}^{m-n+1}\mathbb{I}\{\exists y\in D:x_i...x_{i+n-1}\in y\}</script><p>这一指标表示非成员样本中的n-gram中有多少百分比至少能在一个成员样本中找到。下图展示了非成员样本对于整个训练集的7-gram重叠百分比。可以看到很多数据集的重叠比都很高。<br><img src="/2024/11/30/do-membership-inference-attacks-work-on-large-language-models-yue-du-bi-ji/Pasted image 20241130194213.png" alt="img"></p><p>论文重新采样非成员样本，使重叠率降低，重新报告了MIA的性能。可以看到MIA的表现显著提高。<br>论文指出n-gram是自然语言的固有属性，还指出，n-gram重叠分布分析有助于评估一组候选非成员样本在构建MIA基准测试时，是否能代表目标领域。论文强调需要考虑数据领域的特征，例如n-gram重叠，并理解它们可能对MIA性能的影响。<br><img src="/2024/11/30/do-membership-inference-attacks-work-on-large-language-models-yue-du-bi-ji/Pasted image 20241130194822.png" alt="img"></p><h3 id="3-3-候选集选择的重要性"><a href="#3-3-候选集选择的重要性" class="headerlink" title="3.3 候选集选择的重要性"></a>3.3 候选集选择的重要性</h3><p>最近的研究表明最先进的成员推理攻击在预训练模型上达到了&gt;0.7的AUC ROC。论文探讨了这些研究非成员候选选择方式如何导致非成员和成员之间固有、无意的分布偏移。以往的工作通常根据目标模型的知识截止日期来区分目标领域中的成员和非成员，成员为截止日期之前的样本，非成员反之。</p><p>论文在wikipedia领域进一步加大了成员和非成员之间的时间差距。下表显示该设置可以显著增强成员推理的性能。推测这是源于语言的变化，如新术语的引入。<br><img src="/2024/11/30/do-membership-inference-attacks-work-on-large-language-models-yue-du-bi-ji/Pasted image 20241130201059.png" alt="img"></p><p>下图展示了时间偏移前后7-gram的变化。可以看到时间偏移后7-gram重叠降低。论文建议在评估MIA性能时通过将候选非成员样本的n-gram重叠分布与从预训练语料库中剔除的样本集的n-gram重叠分布进行比较，来估算样本非成员集在成员领域中的代表性。<br><img src="/2024/11/30/do-membership-inference-attacks-work-on-large-language-models-yue-du-bi-ji/Pasted image 20241130201711.png" alt="img"></p><h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4. 结论"></a>4. 结论</h2><p>论文利用实验说明现在的成员推理攻击和随机猜测没什么差距。主要原因是1. 目前大模型的训练过程中使用的数据量大且训练轮次小，所以单个样本所能留下的痕迹很小；2. 成员和非成员之间具有很强的相似性（自然语料中可能包含很多重复的片段），导致他们之间的界限是模糊的。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Low-Cost High-Power Membership Inference Attacks 阅读笔记</title>
      <link href="/2024/11/29/low-cost-high-power-membership-inference-attacks-yue-du-bi-ji/"/>
      <url>/2024/11/29/low-cost-high-power-membership-inference-attacks-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="论文研读笔记"><a href="#论文研读笔记" class="headerlink" title="论文研读笔记"></a>论文研读笔记</h2><p>论文题目：Low-Cost High-Power Membership Inference Attacks<br>发表时间：2024<br>发表期刊/会议：ICML</p><h2 id="1-内容简介"><a href="#1-内容简介" class="headerlink" title="1. 内容简介"></a>1. 内容简介</h2><p>这篇文章通过重新设计评分函数，成功实现了一种资源消耗更低、TPR-FPR曲线上表现更好的成员推理攻击。</p><h2 id="2-主要内容"><a href="#2-主要内容" class="headerlink" title="2. 主要内容"></a>2. 主要内容</h2><h3 id="2-1-攻击者能力"><a href="#2-1-攻击者能力" class="headerlink" title="2.1 攻击者能力"></a>2.1 攻击者能力</h3><ul><li>知道目标模型架构</li><li>可以拥有同分布的数据</li><li>只有黑盒访问权限， 需要知道输出置信度</li></ul><h3 id="2-2-概念框架"><a href="#2-2-概念框架" class="headerlink" title="2.2 概念框架"></a>2.2 概念框架</h3><p>该论文将成员推理视为概率论中的假设检验。即存在两个假设：</p><script type="math/tex; mode=display">H_{in}: model\quad \theta \quad was \quad trained \quad on \quad x</script><script type="math/tex; mode=display">H_{out}: model\quad \theta \quad was\quad not \quad trained \quad on \quad x</script><p>攻击者被随机置入其中一个世界，他需要根据自己的知识收集足够多的证据来推断自己处于哪个世界中。</p><h3 id="2-3-攻击方法"><a href="#2-3-攻击方法" class="headerlink" title="2.3 攻击方法"></a>2.3 攻击方法</h3><p>提出了一种新颖的统计检验方法，将原假设$H_{out}$建模为由多个世界组成，其中目标数据x被替换成其它数据点z。接着组合多个成对的（x,z)似然比检验。为了拒绝原假设，需要对z进行充分采样来证明x在训练集时能够训练出$\theta$的概率大于z替换掉x后能够训练出$\theta$的概率。数据点$x,z$的似然比定义为：</p><script type="math/tex; mode=display">LR_{\theta}(x,z)=\frac{Pr(\theta|x)}{Pr(\theta|z)}</script><p>$Pr(\theta|x)$表示x在训练集时能够训练出模型$\theta$的概率。其中训练集的其余数据点由数据分布$\pi$均匀采样。</p><p>对似然比进行变形可以得到：</p><script type="math/tex; mode=display">\mathrm{LR}_\theta(x,z)=\left(\frac{\Pr(x|\theta)}{\Pr(x)}\right)\cdot\left(\frac{\Pr(z|\theta)}{\Pr(z)}\right)^{-1}</script><p>$Pr(x)$和$\pi(x)$的意义不同。$Pr(x)$是贝叶斯公式中的一个归一化常数。计算公式为：</p><script type="math/tex; mode=display">\begin{aligned}\mathrm{Pr}(x)& =\sum_{\theta^{\prime}}\Pr(x|\theta^{\prime})\Pr(\theta^{\prime}) \\&=\sum_{D,\theta^{\prime}}\Pr(x|\theta^{\prime})\Pr(\theta^{\prime}|D)\Pr(D)\end{aligned}</script><p>其中$Pr(x|\theta’)$表示x在模型$\theta’$上的输出置信度。在实际应用中，我们通过抽样参考模型θ′来计算$Pr(x | \theta’)$的均值作为$Pr ( x )$的经验值，每个参考模型都在从总体分布$\pi$中抽取的随机数据集D上训练得到。由于只使用一小部分的数据和模型，为了得到模型参数$\theta’$的无偏采样，x应该在一半模型的训练集中。这带来很高的计算代价。为了解决这个问题，论文的离线算法只在OUT模型（训练集没有x的模型）中通过平均$Pr(x|\theta’)$计算$Pr_{OUT}(x)$，然后通过放大$Pr_{OUT}(x)$得到$Pr_{IN}(x)$。</p><p>由此，可以得到样本x的评分函数：</p><script type="math/tex; mode=display">\operatorname{Score}_{\text {MIA }}(x ; \theta)=\operatorname{Pr}_{z \sim \pi}\left(\operatorname{LR}_\theta(x, z) \geq \gamma\right),\quad \gamma \ge 1</script><p>本论文提出的算法如下图:<br><img src="/2024/11/29/low-cost-high-power-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241129151751.png" alt="img"></p><h3 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h3><p>数据集：CIFAR-10、CIFAR-100、CINIC-10、ImageNet 和 Purchase-100 </p><p>baseline：Attack-P（Ye 等，2022） Attack-R（Ye 等，2022） LiRA（Carlini 等，2022）Quantile Regression（Bertran 等，2023）</p><p>测试目标：</p><ol><li>在有限计算资源下训练参考模型时攻击的表现。这包括将攻击限制为离线模式，其中参考模型是预训练的。</li><li>当能够无限训练参考模型时（在线模式），攻击的最终能力。</li><li>当目标数据点是分布外数据时，攻击在区分成员与非成员方面的强度。这反映了攻击作为预言机在将整个数据空间划分为成员和非成员时的鲁棒性和实用性。</li><li>对手的知识对攻击性能的影响（特别是关于人群数据的分布变化，以及目标模型与参考模型之间的网络架构不匹配）。<h3 id="3-1-低资源预算情况下的攻击效果"><a href="#3-1-低资源预算情况下的攻击效果" class="headerlink" title="3.1 低资源预算情况下的攻击效果"></a>3.1 低资源预算情况下的攻击效果</h3><strong>使用少量参考模型</strong>：下图显示的是当参考模型数量为1时所有攻击方法的TPR和FPR曲线。可以看到在任何情况本论文攻击(RMIA)都优于其它攻击。<br><img src="/2024/11/29/low-cost-high-power-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241129160529.png" alt="img"><br>下表比较了在低成本场景下攻击的结果，结果显示RMIA在所有数据集上都严格优于其它的工作。甚至离线的RMIA攻击由于在线的LiRA，可以在低FPR的区间获得最好的效果。<br><img src="/2024/11/29/low-cost-high-power-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241129161117.png" alt="img"></li></ol><p><strong>更大的模型和更大规模的数据集</strong>：模型和训练集规模增加后，攻击效果可能会下降。上表展示了当使用Imagenet作为数据集时RMIA的攻击效果对比。可以看到改论文效果优于其他攻击。</p><p><strong>离线模式下使用更多的参考模型</strong>：下表展示了当使用更多参考模型（127个）时的对比实验情况。RMIA仍然是最好的。同时可以看到即使参考模型增多，RMIA攻击性能也没有太多的变化。说明参考模型数量对RMIA影响不大，RMIA可以是一种低资源消耗的攻击。<br><img src="/2024/11/29/low-cost-high-power-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241129173018.png" alt="img"></p><h3 id="3-2-面对离群样本（OOD，Out-Of-Distribution）时的鲁棒性"><a href="#3-2-面对离群样本（OOD，Out-Of-Distribution）时的鲁棒性" class="headerlink" title="3.2 面对离群样本（OOD，Out Of Distribution）时的鲁棒性"></a>3.2 面对离群样本（OOD，Out Of Distribution）时的鲁棒性</h3><p>一个强大的MIA应该可以排除所有非成员，即使他来自不同的分布。所示MIA应该在分布外数据上保持高准确性。但尽管与分布内样本相比，OOD样本通常表现出较低的置信度，但仅根据置信度进行过滤会导致算法拒绝“较难推理”的成员。</p><p>论文使用cifar10训练模型，使用CINIC-10作为OOD样本。结果如下图。可以看到RMIA具有明显的优势，而LiRA的表现和随机猜测差距不大。<br><img src="/2024/11/29/low-cost-high-power-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241129181553.png" alt="img"></p><h3 id="3-3-分析RMIA的参数"><a href="#3-3-分析RMIA的参数" class="headerlink" title="3.3 分析RMIA的参数"></a>3.3 分析RMIA的参数</h3><p>下表展示了使用不同数量的z样本所带来的性能变化。可以看到使用2500个样本和使用25000个样本所展现出的结果差异不大。即使只参考250个参考样本，低FPR时的TPR仍然很高。<br><img src="/2024/11/29/low-cost-high-power-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241129182756.png" alt="img"></p><h3 id="攻击其它ML算法"><a href="#攻击其它ML算法" class="headerlink" title="攻击其它ML算法"></a>攻击其它ML算法</h3><p>测试了该算法对Gradient Boosting Decision Tree (GBDT)的攻击效果。该论文算法效果一致优于其它算法。</p><h2 id="结论和收获、"><a href="#结论和收获、" class="headerlink" title="结论和收获、"></a>结论和收获、</h2><p>这篇论文通过重新设计评分函数，实现了一种高性能，低代价的离线成员推理攻击方法。通过和其它sota的对比，证明了它性能的优越性。<br>论文在实验章节的开头就介绍了后续会测试哪些实验，后续写论文可以参考。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 成员推理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OSLO: One-Shot Label-Only Membership Inference Attacks 阅读笔记</title>
      <link href="/2024/11/23/oslo-one-shot-label-only-membership-inference-attacks-yue-du-bi-ji/"/>
      <url>/2024/11/23/oslo-one-shot-label-only-membership-inference-attacks-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="论文研读笔记"><a href="#论文研读笔记" class="headerlink" title="论文研读笔记"></a>论文研读笔记</h1><p>论文题目：OSLO: One-Shot Label-Only Membership Inference Attacks<br>发表时间：2024<br>发表期刊/会议：To appear at NeurIPS 2024</p><h2 id="1-内容简介"><a href="#1-内容简介" class="headerlink" title="1. 内容简介"></a>1. 内容简介</h2><p>在只能访问目标模型一次的前提下提出了一种新的Label-only成员推理攻击方法。在低假阳率的前提下，该方法和其它Label-only方法进行对比，获得了很大的性能提升。该论文还测试了多种不同的防御方法对该攻击的防御性能。</p><h2 id="2-主要内容"><a href="#2-主要内容" class="headerlink" title="2. 主要内容"></a>2. 主要内容</h2><h3 id="2-1-攻击者能力"><a href="#2-1-攻击者能力" class="headerlink" title="2.1 攻击者能力"></a>2.1 攻击者能力</h3><ul><li>攻击者只能访问目标模型一次</li><li>攻击者不需要知道目标模型架构</li><li>攻击者可以拥有同分布的数据</li><li>攻击者只有黑盒访问权限，且只能获得模型返回的标签</li></ul><h3 id="2-2-攻击Intuition"><a href="#2-2-攻击Intuition" class="headerlink" title="2.2 攻击Intuition"></a>2.2 攻击Intuition</h3><p>成员样本和非成员样本在面对对抗扰动时有两种现象：</p><ul><li>从整体平均来看，成员样本相比非成员样本需要更大的扰动</li><li>对于每个单独的样本来说，成员样本需要更大的扰动。<blockquote><p>没看懂这俩有啥区别</p></blockquote></li></ul><p>前人工作只利用了第一条，而改论文利用了第二条</p><h3 id="2-3-攻击方法"><a href="#2-3-攻击方法" class="headerlink" title="2.3 攻击方法"></a>2.3 攻击方法</h3><p>整体的攻击思路分为三步：</p><ol><li>生成源模型和验证模型：攻击者利用手头多余的数据训练多个不同架构不同训练集的代理模型，并将其分为源模型和验证模型。源模型用来生成对抗样本，验证模型用来验证对抗效果</li><li>利用源模型生成对抗样本：攻击者利用代理模型生成待推理模型的对抗样本。每个样本尽可能少添加扰动，使其恰好将验证模型的预测置信度降到某个阈值下即可</li><li>利用对抗样本的可迁移性测试生成的对抗样本能否绕过目标模型。如果可以绕过则判定其为成员</li></ol><p>对抗样本生成的算法如下图所示。和一般的对抗样本生成算法的区别在于该算法利用迭代不断的扩大扰动大小限制，直到刚好可以让模型置信度降到阈值以下。</p><p><img src="/2024/11/23/oslo-one-shot-label-only-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241123192216.png" alt="img"></p><h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><h3 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1 实验设置"></a>3.1 实验设置</h3><p>使用cifar10/100、SVHN数据集，ResNet18 、 DenseNet121模型。使用目前最好的五种label-only攻击作为baseline。使用真阳率、假阳率、精准率、召回率作为评价指标。主要关注低假阳率的区间内真阳率的表现。</p><h3 id="3-2-实验结果"><a href="#3-2-实验结果" class="headerlink" title="3.2 实验结果"></a>3.2 实验结果</h3><h4 id="低假阳率区间内真阳率表现"><a href="#低假阳率区间内真阳率表现" class="headerlink" title="低假阳率区间内真阳率表现"></a>低假阳率区间内真阳率表现</h4><p>如下图所示，OSLO 在性能上大幅优于此前的所有仅标签成员推断攻击（MIAs）。例如，与之前的攻击相比，OSLO 在 ResNet18 上的 0.1% FPR 下，TPR 提高了 5 倍至 67 倍；在 1% FPR 下，TPR 提高了 3 倍至 16 倍（评估于三个数据集）。在 DenseNet121 上，1% FPR 下的提升范围为 2 倍至 12 倍。可以观察到，以往的攻击在低 FPR 区间表现不佳，而 OSLO 即使在低 FPR 下也能成功识别部分成员。例如，在 CIFAR-100 数据集上使用 ResNet18 时，0.1% FPR 下，其他最好的仅标签 MIA 的 TPR 仅为 0.3%，而 OSLO 达到了 6.7%。在 1% FPR 下，OSLO 的 TPR 为 18.9%，而其他攻击最高仅能达到 2.7%。</p><p><img src="/2024/11/23/oslo-one-shot-label-only-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241123202451.png" alt="img"></p><h4 id="精准率和召回率分析"><a href="#精准率和召回率分析" class="headerlink" title="精准率和召回率分析"></a>精准率和召回率分析</h4><p>如下图所示，OSLO 是唯一能够有效在召回率与精度之间进行权衡的攻击方法。例如，通过牺牲召回率，OSLO 在以 ResNet18 为目标模型的所有三个数据集上都实现了 90% 以上的精度。具体而言，在 CIFAR-10、CIFAR-100 和 SVHN 数据集上，当召回率分别为 5%、44.7% 和 9% 时，OSLO 达到了 96.2%、92.0% 和 90.9% 的精度。相比之下，在相似召回水平下，其他攻击方法的最高精度仅为 67.4%、82.5% 和 65.2%。这表明 OSLO 能够以高精度识别成员，而之前的所有仅标签攻击方法都无法匹敌。</p><p><img src="/2024/11/23/oslo-one-shot-label-only-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241123203105.png" alt="img"></p><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p><strong>不同对抗样本生成算法的影响</strong>：论文在 OSLO 框架内使用六种不同的基于迁移的对抗技术评估了成员推断的性能，包括 TI、DI、MI、Admix以及可组合方法 TDMI 和 TMDAI。结果显示这些改变对攻击性能影响不大。说明攻击成功的原因是框架设计的成功。</p><p><img src="/2024/11/23/oslo-one-shot-label-only-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241123203541.png" alt="img"></p><p><strong>本地代理模型训练算法的影响</strong>：代理模型使用和目标模型不同的优化算法，得到结果如下表。可见即使优化算法不同对结果影响也不大。</p><p><img src="/2024/11/23/oslo-one-shot-label-only-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241123203841.png" alt="img"></p><p><strong>测试验证模型的影响</strong>：移除验证模型，使用统一的扰动范围生成对抗样本。结果如下表。结果显示一同操作下来性能收到了严重影响。<br><img src="/2024/11/23/oslo-one-shot-label-only-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241123204326.png" alt="img"></p><p><strong>多次查询的效果</strong>：下图展示了 OSLO 在不同对抗样本生成阈值 $\tau$下的精度和召回率。n-shot表示如果所有n次对抗样本均未成功欺骗目标模型，则将样本判定为成员。论文发现增加查询次数并未带来显著提升。这一结果可以归因于以下观察：即使采用多次查询，只有最低$\tau$的那次查询结果具有决定性作用。<br><img src="/2024/11/23/oslo-one-shot-label-only-membership-inference-attacks-yue-du-bi-ji/Pasted image 20241123214450.png" alt="img"></p><h2 id="结论和收获"><a href="#结论和收获" class="headerlink" title="结论和收获"></a>结论和收获</h2><p>这篇文章介绍了一种one-shot的label-only攻击，主要攻击思路是利用本地代理模型生成对抗样本，然后利用成员样本不易产生对抗性来实现成员推理。攻击性能的提升主要得益于对每个样本进行单独的扰动大小限制。</p><p>本文的消融实验设置方法值得学习，可以作为一个模板使用。同时这篇文章还是需要攻击者获得同分布的数据，还存在改进的点。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 成员推理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly 阅读笔记</title>
      <link href="/2024/09/22/a-survey-on-large-language-model-llm-security-and-privacy-the-good-the-bad-and-the-ugly-yue-du-bi-ji/"/>
      <url>/2024/09/22/a-survey-on-large-language-model-llm-security-and-privacy-the-good-the-bad-and-the-ugly-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="论文研读笔记（2-3页）"><a href="#论文研读笔记（2-3页）" class="headerlink" title="论文研读笔记（2-3页）"></a>论文研读笔记（2-3页）</h1><p>论文题目：A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly</p><p>发表时间：2024</p><p>发表期刊/会议：High-Confidence Computing  期刊   引用量可观</p><p>笔记作者：Yifan Yao et al.</p><h2 id="1-内容简介"><a href="#1-内容简介" class="headerlink" title="1.  内容简介"></a>1.  内容简介</h2><p>本文为大语言模型安全领域的综述。主要涉及三个方面：</p><ul><li>Good: AI for security，即使用LLM进行隐私保护、代码审查等</li><li>Bad：使用LLM进行攻击</li><li>Ugly：LLM本身的安全问题</li></ul><h2 id="2-主要内容"><a href="#2-主要内容" class="headerlink" title="2.  主要内容"></a>2.  主要内容</h2><h3 id="2-1-LLM对安全和隐私的积极影响"><a href="#2-1-LLM对安全和隐私的积极影响" class="headerlink" title="2.1 LLM对安全和隐私的积极影响"></a>2.1 LLM对安全和隐私的积极影响</h3><h4 id="2-1-1-代码安全性"><a href="#2-1-1-代码安全性" class="headerlink" title="2.1.1 代码安全性"></a>2.1.1 代码安全性</h4><p><strong>安全编写代码</strong>：主要介绍在安全代码编程（生成）的背景下使用LLM。研究发现使用LLM辅助编写代码并不会严重增加安全隐患。后续有人提出SVEN提升模型生成代码的安全性；提出SALLM评估LLM生成代码的安全性。</p><p><strong>测试用例生成</strong>：LLM可以成功生成测试用例，也可以用来帮助提升模糊测试的覆盖率。</p><p><strong>脆弱代码检测</strong>：研究表明GPT-4 识别的漏洞数量大约是传统静态代码分析器的四倍。但也有研究表明LLM具有较高的误报率。现在有一些研究者将LLM和高级机制结合到一起，大大提升了检测性能。如一种二进制污点分析方法LATTE超越了现有的先进技术，而且成本更低。</p><p><strong>恶意代码检测</strong>：研究表明基于LLM的恶意代码检测可以补充人工审查（但无法完全取代）。一个使用LLM进行恶意代码分析的工具Apiiro。</p><p><strong>漏洞修复</strong>：多篇论文提出LLM可以有效的修复代码漏洞。而且Pearce等人观察到即使LLM没有经过专门的训练也可以修复代码。</p><h4 id="2-1-2-数据隐私和安全"><a href="#2-1-2-数据隐私和安全" class="headerlink" title="2.1.2 数据隐私和安全"></a>2.1.2 数据隐私和安全</h4><p><strong>数据完整性</strong>：主要确保数据在其生命周期中保持不变且未被损坏。一些研究探讨了使用LLM制定勒索软件网络安全策略，还有一些研究使用LLM检测异常行为。</p><p><strong>数据机密性</strong>：一些研究使用LLM将文本中的身份标记替换为通用标记，也有研究使用ChatGPT实现密码学算法。</p><p><strong>数据可靠性</strong>：使用ChatGPT检测包含钓鱼内容的网站，具有较高的召回率和精准率。也有研究将其用于钓鱼邮件的检测。</p><p><strong>数据可追溯性</strong>：指在单一系统或跨多个系统内追踪和记录数据的来源、流动和历史的能力。一些工作使用ChatGPT帮助分析操作系统日志等文件，也可以用来创造逼真的蜜罐来诱骗攻击者。水印技术也被应用于LLM帮助人们监控内容的使用。</p><h3 id="2-2-LLM对隐私和安全的消极影响"><a href="#2-2-LLM对隐私和安全的消极影响" class="headerlink" title="2.2 LLM对隐私和安全的消极影响"></a>2.2 LLM对隐私和安全的消极影响</h3><p><strong>硬件层面攻击</strong>：通过LLM分析物理系统或实现过程中非故意泄露的信息来推断秘密信息。</p><p><strong>操作系统层面攻击</strong>：LLM可以分析从操作系统收集到的信息。有研究建立了一个LLM和易受攻击的虚拟机循环，LLM会分析虚拟机状态，识别漏洞并提出具体的攻击策略，这些策略会在虚拟机中自动执行。</p><p><strong>软件层面攻击</strong>：LLM可以知道攻击者部署恶意软件。也有研究展示了如何诱骗ChatGPT生成勒索软件。</p><p><strong>网络层面攻击</strong>：LLM可以用于网络钓鱼攻击。可以使用LLM修改邮件内容，使其更具说服力。</p><p><strong>用户层面攻击</strong>：主要包括造谣、社会工程、学术不端、欺诈等</p><h3 id="2-3-LLM的脆弱性和威胁"><a href="#2-3-LLM的脆弱性和威胁" class="headerlink" title="2.3 LLM的脆弱性和威胁"></a>2.3 LLM的脆弱性和威胁</h3><p><img src="/2024/09/22/a-survey-on-large-language-model-llm-security-and-privacy-the-good-the-bad-and-the-ugly-yue-du-bi-ji/Pasted image 20240922165536.png" alt=""></p><h4 id="2-3-1-脆弱性和威胁"><a href="#2-3-1-脆弱性和威胁" class="headerlink" title="2.3.1 脆弱性和威胁"></a>2.3.1 脆弱性和威胁</h4><p>首先是AI固有的脆弱性：<br><strong>对抗攻击</strong>： 包括数据投毒和后门等。</p><p><strong>推理攻击</strong>：属性推理、成员推理等。</p><p><strong>萃取攻击</strong>：旨在直接获取特定资源(例如,模型梯度、训练数据等)或机密信息。目前观察到训练数据提取攻击可能对LLMs有效，它是一个攻击者试图通过策略性地查询机器学习模型，从模型的训练数据中检索特定的个体样本的方法。</p><p><strong>偏见与不公平的利用</strong>：LLM可能表现出带有偏见的结果或歧视性行为的现象。</p><p><strong>指令微调攻击</strong>：指令微调攻击指的是针对经过指令微调的LLM的一类攻击或操控行为。这些攻击旨在利用那些使用特定指令或示例进行微调的LLM中的漏洞或局限性。包括：</p><ul><li>越狱攻击：指绕过安全功能，使其能够对本应受限或不安全的问题做出回应，从而解锁通常受安全协议限制的能力。</li><li>提示注入：一种操控LLM行为的方法，以引发意外且可能有害的回应。此技术通过设计输入提示，绕过模型的安全措施或触发不良输出。</li><li>拒绝服务攻击：利用大模型耗费资源多的特点，利于攻击者进行DoS攻击。</li></ul><p>接着介绍非AI固有的漏洞：<br><strong>远程代码执行</strong>：如果LLM集成到网络服务中且该服务的的底层技术设施或者代码存在RCE漏洞，可能导致LLM环境的安全性受损。</p><p><strong>侧信道攻击</strong>：有研究利用侧信道攻击快速提取了隐私信息，该研究提出了覆盖整个机器学习模型生命周期的四种测信道攻击。</p><p><strong>供应链攻击</strong>：LLM可能因为使用供应链的不安全插件、数据、预训练模型导致产生新的安全隐患。大多数此类攻击研究的是LLM插件的安全性。</p><h4 id="2-3-2-防御"><a href="#2-3-2-防御" class="headerlink" title="2.3.2 防御"></a>2.3.2 防御</h4><p><strong>基于模型架构的防御</strong>：模型架构决定了知识和概念是如何存储、组织和上下文交互的，这对于大型语言模型的安全性至关重要。研究表明使用较大参数的模型可以更好的以差分隐私的形式进行训练。也有研究表明可以将多种认知架构融入LLM来提高人工智能的鲁棒性。同时也有研究者利用知识图谱增强LLM的推理能力，从而建立对AI的信任关系。</p><p><strong>模型训练时的防御</strong>：</p><ul><li>语料清洗：LLM的训练数据质量对LLM安全性有着很大的影响。网络上收集的语料被认为有不公平、有毒性、不真实、侵犯隐私等缺陷，目前有大量工作为清洗数据做贡献。主要流程有：语种识别、去毒、除偏、隐私去除、去重等。</li><li>优化方法：优化目标对于指导LLM如何从训练数据中学习，影响哪些行为受到鼓励或惩罚至关重要。主要方法有：对抗训练、鲁棒微调</li></ul><p><strong>模型推理时的防御</strong>：</p><ul><li>指令处理（预处理）：对用户发送的指令进行处理，去除其中潜在的破坏性行为。指令预处理方法有：指令篡改、净化、防御性演练等。</li><li>恶意检测（执行中处理）：提供了对LLM中间结果（如神经元激活情况）的深度检查，如利用掩码敏感度区分正常指令和中毒指令；根据可疑词的文本相关性识别可疑词；根据多代之间的语义一致性来检测对抗样本。除了LLMs的内在特性外，还有一些工作利用了语言统计特性，例如检测离群词。</li><li>结果处理（后处理）：检查模型输出并在必要时进行修改。可以通过比较多个模型输出来降低输出毒性，也可以使用另一个单独的LLM来检查输出的危害性。</li></ul><h2 id="3-结论收获（总结自己通过本文学到的知识）"><a href="#3-结论收获（总结自己通过本文学到的知识）" class="headerlink" title="3.  结论收获（总结自己通过本文学到的知识）"></a>3.  结论收获（总结自己通过本文学到的知识）</h2><p>LLM安全研究现状，了解到了一些前沿的研究点</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 综述 </tag>
            
            <tag> 大语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Watermark Stealing in Large Language Models 阅读笔记</title>
      <link href="/2024/09/21/watermark-stealing-in-large-language-models-yue-du-bi-ji/"/>
      <url>/2024/09/21/watermark-stealing-in-large-language-models-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="论文研读笔记（2-3页）"><a href="#论文研读笔记（2-3页）" class="headerlink" title="论文研读笔记（2-3页）"></a>论文研读笔记（2-3页）</h1><ul><li><p>论文题目：Watermark Stealing in Large Language Models</p></li><li><p>发表时间：2024</p></li><li><p>发表期刊/会议：ICML （CCF A）</p></li></ul><h2 id="1-内容简介（简要概括文章，不可以直接翻译摘要）"><a href="#1-内容简介（简要概括文章，不可以直接翻译摘要）" class="headerlink" title="1.  内容简介（简要概括文章，不可以直接翻译摘要）"></a>1.  内容简介（简要概括文章，不可以直接翻译摘要）</h2><p>部署在网络中的LLM可能通过替换文本的方式在模型的输出中注入水印。水印的用途有知识产权保护和AI文本检测等。本文设计了一个模型水印窃取攻击，即训练一个本地的模型水印策略，使其水印添加方式和目标模型一致。本文还设计了两个下游攻击：水印伪造和水印擦除。</p><h2 id="2-主要贡献（记录文章最核心的思路、方法）"><a href="#2-主要贡献（记录文章最核心的思路、方法）" class="headerlink" title="2.  主要贡献（记录文章最核心的思路、方法）"></a>2.  主要贡献（记录文章最核心的思路、方法）</h2><h3 id="2-1-标记"><a href="#2-1-标记" class="headerlink" title="2.1 标记"></a>2.1 标记</h3><div class="table-container"><table><thead><tr><th>记号</th><th>含义</th></tr></thead><tbody><tr><td>$T_t$</td><td>LM第$t$步的输出</td></tr><tr><td>$H$</td><td>哈希函数</td></tr><tr><td>$\xi$</td><td>被攻击者所持有的水印密钥</td></tr><tr><td>$V$</td><td>词库</td></tr><tr><td>$f$</td><td>伪随机函数PRF</td></tr><tr><td>$l$</td><td>logit向量</td></tr><tr><td>$\mathcal{P}$</td><td>幂集</td></tr></tbody></table></div><h3 id="2-2-水印添加算法"><a href="#2-2-水印添加算法" class="headerlink" title="2.2 水印添加算法"></a>2.2 水印添加算法</h3><p>大语言模型的水印添加方式本质上就是词替换。本文主要攻击的是目前最新的几种水印添加算法。下面对这几种算法进行简单介绍。</p><ol><li>KGW-Soft/Hard：假设$T_t$是第$t$步模型的输出，该算法首先把上一步输出的哈希值$H(T_{t-1})$和一个密钥$\xi$作为随机数种子，利用伪随机函数$f$将词库$V$划分为$V_{green}$和$V_{red}$  。soft版本的算法通过增加green词的logit来增加该类词的输出概率，而hard版本的算法则是禁止输出red词（$l_T=-\infty\quad for\quad T \in V_{red}$）。</li><li>KGW2-SELFHASH：在前面工作的基础上，水印算法产生了几种变体。第一个是让PRF的输入中包含更长的上下文内容$T_{t-h},\dots,T_{t-1}$；第二个是self-seeding，即PRF的种子中引入了$T_t$本身，如果生成的$T_t$不在$V_{green}$中则拒绝采样。KGW2-SELFHASH使用h=3和self-seeding，种子的计算方式为$min\{H(T_{t-h},\dots,H(T_{t-1}),H(T_t)\}\cdot \xi \cdot H(T_t)$ 。</li><li>KGW2-SUM：h=3, 种子计算方式为加和。</li><li>Unigram 水印：h=0，即只使用$\xi$产生$V_{green}$  ，且固定不变。</li></ol><p>为了检测输出是否加盖水印，需要逐个统计该输出中存在的green词个数，然后计算z-统计量$z=(n_{green}-\gamma L)/\sqrt{L\gamma(1-\gamma)}$ ，最后利用假设检验计算p值判断水印是否存在。</p><h3 id="2-3-攻击者能力"><a href="#2-3-攻击者能力" class="headerlink" title="2.3 攻击者能力"></a>2.3 攻击者能力</h3><ol><li>水印检测器访问权限。有些模型提供了检测API，可以返回一个bit指示某段话是否加盖了该模型的水印。攻击者可以利用该API检验攻击的有效性并指导攻击的进行。<pre><code> D0：没有API访问权限。前人工作大多采用此场景 D1：有API访问权限。这也符合现实场景。SynthID提供了类似的API</code></pre></li><li>基模型输出的访问权限。即攻击者是否可以收集目标模型未部署水印算法时的输出语料（不一定是水印输出的未水印版本）。<pre><code>B0：不可以B1：可以</code></pre></li></ol><p>该论文可以在上述四种场景中进行攻击</p><h3 id="2-4-攻击算法"><a href="#2-4-攻击算法" class="headerlink" title="2.4 攻击算法"></a>2.4 攻击算法</h3><p>该攻击算法遵循Kerckhoffs原则，即假设攻击者知道被攻击者所使用水印的所有细节，除了密钥$\xi$。</p><p>该论文的攻击并不直接窃取$\xi$的具体数值，而是在给定待判定文本时，从中截取出一个序列$T_1,T_2,T_3,T$，计算一个得分函数$s(T|ctx)$，该分数指示了上下文ctx存在的情况下，生成的$T$在$V_{green}$中的置信度。为了得到该得分，攻击者首先需要计算<strong>目标模型</strong>的条件分布$\hat{p}_w(T|ctx),\quad \forall ctx \in \mathcal{P}(\{T_1,T_2,T_3\})$和<strong>一般模型</strong>的条件分布$\hat{p}_b(T|ctx)\quad \forall ctx \in \mathcal{P}(\{T_1,T_2,T_3\})$。这个条件分布指示了上下文存在的情况下模型下一步生成$T$的概率。一般模型可以为$B_1$场景下的基模型，也可以为攻击者的私有模型$LM_{att}$。s函数表示如下：</p><script type="math/tex; mode=display">s(T,ctx)=\begin{cases}\frac{1}{c}\min\left(\frac{\hat{p}_w(T|ctx)}{\hat{p}_b(T|ctx)},c\right)&\text{if}\frac{\hat{p}_w(T|ctx)}{\hat{p}_b(T|ctx)}\geq1,\\0&\text{otherwise.}\end{cases}</script><p><strong>稀疏性挑战</strong>：为了降低计算复杂度，如果$\hat{p}_w(T_4|ctx)$ 和 $\hat{p}_b(T_4|ctx)$只从较少的样本计算得出，则直接将其置为0。</p><h2 id="3-实验结果（仅记录关键结果，重点在于对实验的分析）"><a href="#3-实验结果（仅记录关键结果，重点在于对实验的分析）" class="headerlink" title="3.  实验结果（仅记录关键结果，重点在于对实验的分析）"></a>3.  实验结果（仅记录关键结果，重点在于对实验的分析）</h2><p>正文的实验环节主要介绍最严格场景（D0,B0）时攻击KGW2-SELFHASH的结果。</p><p>攻击者在访问目标模型时使用的数据集为C4数据集的RealNewsLike子集，最终获取了长度小于800的30000个回答。c设置为2。</p><p><strong>两个关键指标</strong>：水印强度和文本质量。对于文本质量，使用GPT4对生成文本的准确性和一致性进行打分，如果较低则水印伪造失败；使用P-SP评分估计生成文本是否完全捕捉了原文内容，如果较低则水印去除失败。<br>文章将两个目标合并为一个成功率指标。对于水印伪造攻击，报告FPR<em>@f，即成功被假阳率为f的水印检测器检测为带有水印的生成样本的比例（忽略GPT分数小于6.5的样本）；对于水印去除攻击，报告FNR</em>@f，即被假阴率为f的检测器检测为不带水印的转译样本的比例。</p><p>伪造攻击的攻击结果如下（PPL好像是一种对话质量评价指标，文中没有解释。p-val即为前文水印添加算法中介绍的假设检验的p值）。该实验使用了不同的($LM_{mo}$ , $LM_{att}$）组合。结果现实，即使在低假阳率设置下，该文的方法仍然可以实现高质量文本的伪造。尽管之前认为KGW2-SELFHASH可以防止伪造，但该文方法明显做出了突破。<br><img src="/2024/09/21/watermark-stealing-in-large-language-models-yue-du-bi-ji/Pasted image 20240920233801.png" alt="img"></p><p>水印抹除效果提升效果如下。可以看到使用本文的水印窃取算法可以明显提升DIPPER的抹除效果。<br><img src="/2024/09/21/watermark-stealing-in-large-language-models-yue-du-bi-ji/Pasted image 20240920234000.png" alt="img"><br><img src="/2024/09/21/watermark-stealing-in-large-language-models-yue-du-bi-ji/Pasted image 20240920233906.png" alt="img"></p><h2 id="4-写作技巧（文章在章节划分、遣词造句方面的特点）"><a href="#4-写作技巧（文章在章节划分、遣词造句方面的特点）" class="headerlink" title="4.  写作技巧（文章在章节划分、遣词造句方面的特点）"></a>4.  写作技巧（文章在章节划分、遣词造句方面的特点）</h2><p>详细的介绍了现有的最新水印算法，新手不用再翻别的论文也可以有一个大致的了解。<br>但部分关键内容还是不太好理解</p><h2 id="5-结论收获（总结自己通过本文学到的知识）"><a href="#5-结论收获（总结自己通过本文学到的知识）" class="headerlink" title="5.  结论收获（总结自己通过本文学到的知识）"></a>5.  结论收获（总结自己通过本文学到的知识）</h2><p>主要了解了一些最新的LLM水印算法，以及窃取水印的新思路。<br>了解到了Kerckhoffs原则，在某些问题里可以不给自己加太多的限制。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 论文笔记 </tag>
            
            <tag> 大语言模型 </tag>
            
            <tag> 水印窃取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning-Based Black-Box Model Inversion Attacks 阅读笔记</title>
      <link href="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/"/>
      <url>/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Reinforcement-Learning-Based-Black-Box-Model-Inversion-Attacks"><a href="#Reinforcement-Learning-Based-Black-Box-Model-Inversion-Attacks" class="headerlink" title="Reinforcement Learning-Based Black-Box Model Inversion Attacks"></a>Reinforcement Learning-Based Black-Box Model Inversion Attacks</h1><h2 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h2><ul><li><strong>Type</strong>: ConferencePaper</li><li><strong>Title</strong>: Reinforcement Learning-Based Black-Box Model Inversion Attacks, </li><li><strong>Author</strong>: Han, Gyojin; Choi, Jaehyun; Lee, Haeil; Kim, Junmo, </li><li><strong>Year</strong>: 2023 ;</li><li><strong>Pages</strong>: 20504-20513</li><li><strong>Publisher</strong>: IEEE,</li><li><strong>Location</strong>: Vancouver, BC, Canada,</li><li><strong>DOI</strong>: 10.1109/CVPR52729.2023.01964</li></ul><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Model inversion attacks are a type of privacy attack that reconstructs private data used to train a machine learning model, solely by accessing the model. Recently, white-box model inversion attacks leveraging Generative Adversarial Networks (GANs) to distill knowledge from public datasets have been receiving great attention because of their excellent attack performance. On the other hand, current blackbox model inversion attacks that utilize GANs suffer from issues such as being unable to guarantee the completion of the attack process within a predetermined number of query accesses or achieve the same level of performance as whitebox attacks. To overcome these limitations, we propose a reinforcement learning-based black-box model inversion attack. We formulate the latent space search as a Markov Decision Process (MDP) problem and solve it with reinforcement learning. Our method utilizes the conﬁdence scores of the generated images to provide rewards to an agent. Finally, the private data can be reconstructed using the latent vectors found by the agent trained in the MDP. The experiment results on various datasets and models demonstrate that our attack successfully recovers the private information of the target model by achieving state-of-the-art attack performance. We emphasize the importance of studies on privacy-preserving machine learning by proposing a more advanced black-box model inversion attack.</p><p>模型逆向攻击是一种隐私攻击，通过仅访问模型，重新构建用于训练机器学习模型的私有数据。最近，利用生成对抗网络（GANs）从公共数据集中提取知识的白盒模型逆向攻击引起了广泛关注，因为其具有卓越的攻击性能。另一方面，当前利用GAN的黑盒模型逆向攻击存在一些问题，比如无法保证在预定数量的查询访问内完成攻击过程，或无法达到与白盒攻击相同水平的性能。为了克服这些限制，我们提出了一种基于强化学习的黑盒模型逆向攻击。我们将潜在空间搜索问题制定为马尔可夫决策过程（MDP）问题，并通过强化学习来解决它。我们的方法利用生成图像的置信度分数来为代理提供奖励。最终，通过在MDP中训练的代理找到的潜在向量，可以重构私有数据。在各种数据集和模型上的实验证明，我们的攻击成功地恢复了目标模型的私有信息，实现了最先进的攻击性能。我们强调通过提出更先进的黑盒模型逆向攻击，进行隐私保护机器学习研究的重要性。</p><hr><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>随着人工智能的迅速发展，深度学习应用正在各个领域不断涌现，如计算机视觉、医疗保健、自动驾驶和自然语言处理。随着需要使用私有数据来训练深度学习模型的案例数量增加，对包括敏感个人信息在内的私有数据泄漏的关注正在上升。特别是关于隐私攻击的研究[21]表明，恶意用户可以从训练过的模型中提取个人信息。其中一种对机器学习模型最具代表性的隐私攻击是模型逆向攻击，它通过仅访问模型就能重构目标模型的训练数据。</p><p>模型逆向攻击分为三类，即1）白盒攻击，2）黑盒攻击和3）仅标签攻击，这取决于目标模型的信息量。白盒攻击可以访问模型的所有参数。黑盒攻击可以访问由置信度分数组成的软推理结果，而仅标签攻击只能以硬标签形式访问推理结果。随着对深度学习模型隐私攻击的关注增加，加强在开发和部署深度学习模型时的隐私保护机制显得尤为重要。研究人员和实践者正在积极探索提高机器学习模型隐私性的方法，包括差分隐私、联邦学习和安全多方计算等技术。随着领域的不断发展，解决隐私挑战将对确保人工智能技术的负责和道德使用至关重要。</p><p>白盒模型逆向攻击[5, 25, 27]通过使用生成对抗网络（GANs）[10]成功地恢复了包括个人信息在内的高质量私有数据。首先，它们在独立的公共数据上训练GANs以学习私有数据的一般先验。然后，由于可以访问已训练白盒模型的参数，它们利用基于梯度的优化方法搜索并找到代表特定标签数据的潜在向量。然而，这些方法无法应用于保护模型参数的机器学习服务，如亚马逊的Rekognition [1]。要从这些服务中重构私有数据，需要进行关于黑盒和仅标签的模型逆向攻击的研究。</p><p>与白盒攻击不同，这些攻击需要能够探索GANs的潜在空间的方法，以便利用它们，因为无法进行基于梯度的优化。最近提出的深度学习网络的模型逆向（MIRROR）[2]使用遗传算法在从黑盒目标模型获取的置信度分数中搜索潜在空间。此外，边界驱逐模型逆向攻击（BREPMI）[14]通过使用基于决策的零阶优化算法进行潜在空间搜索，在仅标签的设置下取得了成功。</p><p>尽管存在这些尝试，但每种方法都存在显著问题。BREP-MI从第一个生成被分类为目标类别的图像的潜在向量开始潜在空间搜索的过程。这并不保证在通过随机采样找到第一个潜在向量之前需要多少次查询访问，并且在最坏的情况下，可能无法为某些目标类别启动搜索过程。对于MIRROR，尽管它使用了置信度分数进行攻击，但其表现较仅标签攻击BREP-MI更差。因此，我们提出了一种新方法，即基于强化学习的黑盒模型逆向攻击（RLB-MI），作为解决上述问题的方案。我们整合强化学习，以从置信度分数中获得有关潜在空间探索的有用信息。更具体地说，我们将在GAN中潜在空间的探索形式化为马尔可夫决策过程（MDP）的问题。然后，我们基于生成图像的置信度分数为代理提供奖励，并使用回放内存中的更新步骤使代理能够逼近包括潜在空间在内的环境。基于这些信息选择的代理操作比现有方法更有效地导航潜在向量。最终，我们可以通过从潜在向量中经过GAN重构私有数据。我们在各种数据集和模型上进行了实验。攻击性能与三类各种模型逆向攻击进行了比较。结果表明，所提出的攻击能够通过优于所有其他攻击的性能成功地恢复有关私有数据的有意义信息。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="模型逆向攻击"><a href="#模型逆向攻击" class="headerlink" title="模型逆向攻击"></a>模型逆向攻击</h3><p>模型逆向攻击是一种针对机器学习模型的隐私攻击，其目的是重构用于训练的数据。早期的研究主要关注白盒条件，即已训练的模型是完全可访问的。Fredrikson等人通过从低复杂性模型中提取敏感属性[8]和面部图像[7]，展示了模型逆向攻击的严重性。然而，早期的白盒模型逆向攻击由于无法从复杂模型中重构高维数据而存在明显的局限性。</p><p>为解决这些限制，许多白盒模型逆向攻击利用了生成模型。Zhang等人[27]提出了一种名为生成模型逆向攻击（GMI）的攻击，使用在公共数据上训练的GAN [10]。他们通过搜索GAN的潜在空间而不是图像空间来重构图像。GAN的流形逼近能力有效地缩小了攻击模型的搜索空间。知识丰富的分布模型逆向攻击（KED-MI）[5]通过使用一个具有反演特定功能的GAN来改进GMI，其中鉴别器执行多类推断。反演专用GAN的鉴别器可以区分真实数据和伪造数据，并且可以预测目标网络的输出。这使得反演专用GAN能够从公共数据中提取目标模型的有用知识。此外，KED-MI通过提取目标类别数据的分布，而不仅仅是目标类别的一个训练数据，可以创建多样化的图像。变分模型逆向攻击（VMI）[25]将模型逆向攻击形式化为包含深度标准化流和styleGAN [15]框架的变分推断问题。</p><p>不幸的是，白盒攻击并不适用于在训练模型的参数没有信息的情况下。因此，提出了仅需要访问软标签的黑盒模型逆向攻击。基于学习的模型逆向攻击（LBMI）[26]使用类似于自动编码器的结构来训练一个反演模型，该模型以与目标网络相反的方式行为。反演模型不仅需要访问攻击者构造的查询，还需要访问用户输入数据的置信度分数，该数据是反演的目标。Salem等人[22]提出了一种攻击，通过计算模型更新前后输出的差异来泄露用于更新的训练数据的信息。这两种方法都有一个局限，即它们需要攻击者通常难以访问的信息。深度学习网络的模型逆向（MIRROR）[2]表明可以利用GANs在黑盒模型逆向攻击中使用遗传算法。此外，提出了一种仅标签的模型逆向攻击，边界排斥模型逆向攻击（BREPMI）[14]。BREP-MI在反演过程中仅使用硬标签。BREP-MI从当前潜在向量为中心的球面上的点的标签估计梯度。使用估计的梯度更新潜在向量，使得GAN从潜在向量中重构目标类别的代表性图像。</p><h3 id="深度增强学习"><a href="#深度增强学习" class="headerlink" title="深度增强学习"></a>深度增强学习</h3><p>使用深度神经网络进行强化学习（RL）的研究开始引起关注，因为深度Q网络（DQN）[18]在Atari 2600游戏中展现出与人类专家相媲美的性能[4]。DQN通过使用深度神经网络来估计动作值函数，成功解决了具有高维状态输入（如原始像素）的任务。然而，DQN不能立即应用于具有高维、连续动作空间的问题，因为它通过找到最大化动作值函数值的动作来工作。为了解决具有高维、连续动作空间的问题，提出了深度确定性策略梯度（DDPG）[16]。DDPG是一种无模型和离策略算法，使用基于深度策略梯度（DPG）[23]的演员-评论家方法。通过将DQN的回放缓冲和目标网络的思想应用于演员-评论家方法，DDPG稳定了学习过程。即使在DDPG之后，许多深度强化学习方法已被提出以改进DDPG。为了克服过度估计偏差问题，提出了双延迟深度确定性策略梯度（TD3）[9]算法。TD3通过使用一些技巧，如剪切的双Q学习和延迟的策略更新，解决了这个问题。此外，提出了Soft Actor-Critic（SAC）[12]来处理复杂环境的问题。SAC通过将熵最大化项添加到标准最大奖励强化学习目标中，显著提高了探索性能和稳定性。</p><h2 id="提出的方法"><a href="#提出的方法" class="headerlink" title="提出的方法"></a>提出的方法</h2><p>在本节中，我们将介绍我们的方法，即基于强化学习的黑盒模型逆向攻击（RLB-MI）。RLB-MI的概述如图1所示。<br><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img1.png" alt="图 1"></p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p><strong>攻击者的目标</strong>：模型逆向的目标是从使用数据集$D_{pvt}$训练的模型$T$中恢复出类别$y$的样本。目标模型$T: x\to [0,1]^K$学习的是样本到类别的映射。用$K$表示$D_{pvt}$的大小，$d$表示输入的维度。</p><p><strong>攻击者的知识</strong>：由于我们的方法处理的是黑盒设置，攻击者只能访问由攻击者输入的数据和与数据相对应的软标签组织的查询。此外，攻击者了解目标模型的目的。正如先前的研究中所提到的[2, 5, 14, 25, 27]，提供的模型或服务的任务信息不仅是可用的，而且还可以从输出的类别中轻松推断出来。基于对任务的这种了解，攻击者可以访问相应任务的公共数据集。</p><p><strong>概述</strong>：给定在私有数据集$D_{pvt}$上训练的黑盒模型$T$，一个黑盒的模型逆向攻击目的就是恢复出这个私有的数据集。和最近的模型逆向攻击一致，我们也使用了一个公开数据集$D_{pub}$训练了一个Gan  $G$。攻击者只能知道一个模型输出的软标签，但对模型的参数和结构一无所知。因此，我们的方法就是通过搜索能够使模型输出相应类别的高置信度样本的隐向量。我们把隐向量的搜索过程形式化为马尔可夫过程（MDP），然后使用强化学习来解决该问题。更具体的，我们把MDP的状态空间定义为生成器$G$的隐空间，于是每一步$t$的状态$s_t$都可以用隐空间中的向量来表示。状态$s_t$经过动作 $a_t$后会变为状态$s_{t+1}$。任务的目标就是最大化目标模型的置信度向量。</p><h3 id="隐空间搜索中的马尔卡弗过程"><a href="#隐空间搜索中的马尔卡弗过程" class="headerlink" title="隐空间搜索中的马尔卡弗过程"></a>隐空间搜索中的马尔卡弗过程</h3><p>我们描述了用于潜在空间搜索的MDP的组成部分：状态，动作，状态转换和奖励。</p><p><strong>状态</strong>：MDP的状态空间就是生成器G的隐空间（输入空间）。对于每一个eposode，第一个状态$s_0$是一个$k$维的标准化随机向量：</p><script type="math/tex; mode=display">s_0 \sim \mathcal{N}_k(0,1),s_0 \in \mathbb{R}^k,</script><p>在每一步$t$，状态$s_t$会被动作$a_t$更新。</p><p><strong>动作</strong>：我们希望动作可以让随机产生的初始状态向着高奖励的最终状态前进。从宏观的视角来看，我们可以把这种问题理解为基于强化学习的路径查找问题。在传统的路径查找问题中，动作被定义为了$s_t$到$s_{t+1}$的位移$\triangle s$。然而，与寻路中的有界二维空间不同，潜在空间是一个不受限制的高维空间。在我们的阐述中，当将行为定义为位移向量时，由于与潜在空间相比存在较大的状态方差和狭窄的探索区域，强化学习代理可能无法收敛并达到局部最小值或完全失败。因此，我们将行为空间视为整个潜在空间。我们将一个潜在矢量形状的行为定义为一个引导矢量。如图1所示，根据定义，在相同的空间中选择行为。这使得能够广泛探索整个潜在空间，防止代理陷入局部最小值并保证代理的收敛。每一步$t$的动作都由强化代理$A:\mathbb{R}^k \to \mathbb{R}^k$来产生：</p><script type="math/tex; mode=display">a_t=A(s_t),a_t\in \mathbb{R}^k</script><p><strong>状态转移</strong>：我们通过在每一步将状态朝着行为移动来更新状态，使用一个差异因子 α 作为状态转移中当前状态的权重，以确定移动距离。在步骤 t 的状态转移如下：</p><script type="math/tex; mode=display">s_{t+1} = \alpha \cdot s_t + (1-\alpha) \cdot a_t</script><p>我们将 $\alpha$ 称为多样性因子的原因是，我们将 $\alpha$ 提出作为一个超参数，允许我们控制生成图像的多样性。正如Wang等人所提到的[25]，在重构图像时，模型逆向攻击存在准确性和多样性之间的权衡。我们可以通过 $\alpha$ 调整准确性和多样性之间的权衡。 $\alpha$ 越高，随机初始潜在向量对每个episode的影响越大，代理就越专注于生成具有高多样性的图像。例如，如果 $\alpha$ 的值为零，下一个状态将与当前步骤中的行为相同，因此代理只专注于生成目标分类器概率最高的一张图像。另一方面，随着 $\alpha$ 的增加，随机初始潜在向量的影响增大，代理被训练以找到具有高概率属于目标类别的各种图像。</p><p><strong>奖励</strong>：在通过行为更新状态后，代理从环境中接收一个奖励。生成器 $G$ 从更新后的潜在向量生成一张图像，通过使用目标网络 $T$ 进行推理，我们可以得到图像目标类别 $y$ 的置信度分数。由于行为指导了状态的移动方向，从行为生成的图像也应该接近目标类别空间。为了将状态和行为置于靠近目标类别空间的位置，我们需要在状态和行为具有较高置信度分数时提供较高的奖励。因此，我们构建一个奖励，其中包含状态分数和行为分数，这些分数是由每个矢量创建的图像的置信度分数的对数值计算而得。这些分数的计算如下：</p><script type="math/tex; mode=display">r_1 = log[T_y(G(s_{t+1}))]</script><script type="math/tex; mode=display">r_2 = log[T_y(G(a_{t}))]</script><p>其中$r_1$使状态分数，$r_2$是动作分数。此外，我们希望重构的图像具有目标类别的特征，使其与其他类别的图像有所区别。因此，我们提出了一个额外的项 $r_3$，对其他类别图像的高置信度分数进行惩罚。我们计算目标类别的置信度分数与其他类别的最大置信度分数之间的差异。与先前的分数一样，对计算值应用对数。由于对于小于或等于零的数字，对数是未定义的，因此取对数为减去的值和一个小正数 $\epsilon$ 中的较大者。项 $r3$ 的表达式如下：</p><script type="math/tex; mode=display">r_3=\log[\max\{\epsilon,T_y(G(s_{t+1}))-\max_{i\neq y}T_i(G(s_{t+1}))\}].</script><p>每一步的总奖励 $Rt$ 计算如下：</p><script type="math/tex; mode=display">\begin{aligned}R_t&=w_1\cdot r_1+w_2\cdot r_2+w_3\cdot r_3,\end{aligned}</script><p>其中$w_n$是分数$r_n$的权重。</p><h3 id="使用强化学习解决MDP"><a href="#使用强化学习解决MDP" class="headerlink" title="使用强化学习解决MDP"></a>使用强化学习解决MDP</h3><p>我们通过强化学习来解决被提出的潜在空间搜索问题，该问题被制定为马尔可夫决策过程（MDP）。由于所提出的由 G 和 T 组成的 MDP 的环境非常复杂，我们需要在复杂环境中具有鲁棒性的强化学习代理。此外，我们需要一个能够处理连续行为空间的代理，因为MDP中的行为空间被定义为 G 的潜在空间。因此，我们使用满足所有上述点的 Soft Actor-Critic (SAC) [12] 来解决MDP。我们训练一个SAC代理来从给定状态中选择适当的行为。在训练之后，通过将随机初始向量提供给训练过的代理作为初始状态，我们可以获得每个episode的重构图像。代理训练的整个过程如 算法 1 所示。</p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/al1.png" alt="算法 1"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p><strong>数据集</strong>：我们对使用代表性人脸数据集 CelebFaces 属性数据集（CelebA）[17]、FaceScrub 数据集[19] 和 PubFig83 数据集[20] 训练的目标分类器进行了攻击评估。我们将每个数据集分为一个用于训练目标分类器的私有数据集和一个用于训练生成模型的公共数据集。公共数据集与私有数据集之间没有类别交集，因此生成模型无法学习目标分类器的特定类别信息。</p><p>对于 CelebA，私有数据集包括 30,027 张属于 1,000 个身份的图像，而公共数据集包括从其余的 9,177 个身份中随机选择的 30,000 张图像，与先前的研究[5, 14]一样。对于 FaceScrub，总共 530 个身份中，随机选择的 200 个身份的所有图像用作私有数据集，剩余 330 个身份的所有图像用作公共数据集。对于 PubFig83，总共 83 个身份中，随机选择的 50 个身份的所有图像用作私有数据集，剩余 33 个身份的所有图像用作公共数据集。此外，我们使用 Flickr-Faces-HQ 数据集（FFHQ）[15] 作为公共数据集，考虑到公共数据集和私有数据集之间存在分布转移的情况。在这些实验中，从 FFHQ 随机选择的 30,000 张图像被用作公共数据集。所有人脸图像都经过居中裁剪，然后调整大小为 64 × 64。</p><p><strong>模型</strong>：为了进行公正比较，我们尝试对几种流行的网络结构进行攻击。与先前的研究[5, 14, 27]类似，我们在实验中使用了三种网络结构，分别是 VGG16 [24]、ResNet-152 [13] 和 Face.evoLVe [6]。</p><p><strong>评估指标</strong>：Zhang 等人[27]提出了可以定量评估模型逆向攻击的指标，与之前依赖于视觉检查的定性评估不同。我们简要描述评估指标，包括攻击准确率、K 最近邻距离（KNN Dist）和特征距离（Feat Dist）。</p><p>攻击准确率：为了评估重构图像的攻击准确率，我们在私有数据集上训练评估分类器。评估分类器必须与目标分类器不同，因为重构图像可能会过拟合目标分类器。我们使用了 Cheng 等人[6]提出的架构作为评估分类器。我们在 MS-Celeb-1M [11] 上对一个预训练的分类器进行微调，分别在 PubFig83、FaceScrub 和 CelebA 数据集上实现了 98%、99% 和 96% 的测试准确率。</p><p>KNN Dist：KNN Dist 是一种度量，用于测量重构图像的特征与目标标签图像中最近样本的特征之间的平均 L2 距离。特征是在评估分类器的全连接层之前获取的。</p><p>Feat Dist：Feat Dist 是一种度量，用于测量重构图像的特征与目标标签图像的特征中心的 L2 距离。特征是在评估分类器的全连接层之前获取的。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><strong>基线</strong>：我们将我们的攻击性能与三个类别中包括的代表性模型逆向攻击进行比较：白盒攻击、黑盒攻击和仅标签攻击。白盒攻击的基线是 GMI [27] 和 KED-MI [5]。GMI 是使用 GAN 的第一个模型逆向攻击，而 KED-MI 在白盒攻击中表现出最高的攻击性能。黑盒和仅标签的基线是 LB-MI [26]、MIRROR [2] 和 BREP-MI [14]。为了公平比较，除了 LB-MI 和 KED-MI 之外，攻击中使用了相同的 GAN，因为 LBMI 不使用 GAN，而 KED-MI 使用其自己的生成模型，名为反演专用 GAN。我们使用每个周期的代理生成图像，并选择对目标分类器具有最高置信度分数的图像。</p><p><strong>在各种模型上的性能评估</strong>：表1显示了RLB-MI和基线在使用CelebA训练的三个模型（VGG16、ResNet-152和Face.evoLVe）上的评估结果。每个目标模型的测试准确率分别为88%、91%和89%。尽管现有的黑盒模型逆向攻击LB-MI和MIRROR可以访问软标签，但它们的性能明显低于BREP-MI，这是一种仅标签的模型逆向攻击方法。然而，提出的黑盒攻击RLB-MI通过适当利用置信度分数的信息，远远超过了BREP-MI。此外，我们的攻击在ResNet-152和Face.evoLVe的情况下超过了最先进的白盒模型逆向攻击KED-MI，尽管无法访问梯度信息。即使在VGG16的情况下，通过RLB-MI重构的图像也捕捉到目标类的信息特征，具有与真实样本的K最近邻距离和特征距离最小。还可以看到目标分类器的更高预测性能导致攻击性能的提高。这个结果是合理的，因为性能更好的分类器包含关于训练数据特征的更准确和关键的信息。我们通过提供基线和我们的方法生成的真实样本和攻击图像，对定性评估结果进行展示，因为评估分类器可能不能完全代表人类判断。<br><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table1.png" alt="表1"></p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img2.png" alt="图2"></p><p><strong>在各种数据集上的性能评估</strong>：我们测量了在使用PubFig83、FaceScrub和CelebA训练的Face.evoLVe模型上进行攻击的性能。每个目标模型的测试准确率分别为96%、97%和89%。表2显示，RLB-MI的攻击准确率优于所有基线。尽管LB-MI显示出与其他方法竞争的特征距离，但它获得了非常低的攻击准确率。这是由于LB-MI反演模型学习来自公共数据集的一般先验能力的限制引起的。<br><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table2.png" alt="表2"></p><p><strong>公共数据集差异的影响</strong>：在大多数实际情境中，公共和私有数据集具有相同分布的机会很少，因此我们通过在与私有数据集不同分布的公共数据集上训练生成模型来进行实验。我们使用Flickr-Faces-HQ数据集（FFHQ）[15]训练的生成模型进行攻击评估。这些实验中使用的目标分类器是使用PubFig83、FaceScrub和CelebA进行训练的Face.evoLVe。在使用FFHQ作为公共数据集的实验中，模型逆向攻击显示出了降低的攻击性能。然而，即使在公共和私有数据集之间存在分布偏移的情况下，如表3所示，我们的攻击仍然实现了最先进的攻击性能。我们认为性能下降可能是因为每个数据集中对齐或裁剪面部的方式不同，每个数据集中包含的性别或年龄分布也不同。在拍照时，由光照条件或背景引起的图像分布差异可能也会影响攻击性能。</p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table3.png" alt="表3"></p><h3 id="对攻击配置进行实验"><a href="#对攻击配置进行实验" class="headerlink" title="对攻击配置进行实验"></a>对攻击配置进行实验</h3><p><strong>准确性和多样性之间的权衡</strong>：我们根据多样性因子$\alpha$的变化测量训练代理的攻击准确性和多样性。我们为各种$\alpha$值训练代理，并为特定身份的每个代理生成1,000张图像。为了分别评估生成图像的保真度和多样性，我们使用Density and Coverage (D&amp;C) [3]作为度量标准。在这些实验中使用的目标分类器是使用CelebA对Face.evoLVe进行训练的。图3显示了攻击准确性和多样性之间的权衡。随着$\alpha$的增加，攻击准确性减小，表示多样性的覆盖率增加。此外，从图3b可以看出，密度对$\alpha$的变化具有鲁棒性，这意味着代理生成的图像的保真度不受$\alpha$变化的影响。当$\alpha$为0.00和0.97时生成的图像在图4中可视化。</p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img3.png" alt="图3"></p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table4.png" alt="图4"></p><p><strong>攻击性能随最大轮数的变化趋势</strong>：为了观察攻击性能随最大轮数的变化，我们按照从1,000到40,000的步长报告攻击准确性。实验结果见图5。在图5a中，我们绘制了使用CelebA训练的各种目标模型的结果，而在图5b中，我们绘制了使用不同数据集训练的Face.evoLVe模型的结果。攻击性能每达到最大轮数就迅速增加，然后趋于稳定。即使更改目标分类器的结构，当数据集相同时，攻击准确性的趋势没有显著差异。此外，目标数据集的目标类别数越少，饱和点就越早出现，之后攻击准确性由于过拟合而下降。对于有50个目标类别的PubFig83，饱和点出现在25,000轮，而对于有200个目标类别的FaceScrub，饱和点出现在36,000轮。因此，根据目标数据集设置适当的最大轮数是很重要的。<br><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img5.png" alt="图5"></p><p><strong>对各种RL代理的实验</strong>：为了确定RL代理的效果，我们尝试使用DDPG [16]和TD3 [9]代替SAC进行潜在空间搜索。在这个实验中使用的目标分类器是使用CelebA训练的Face.evoLVe。表4中的结果显示，当在我们的攻击中使用SAC时，其性能明显高于DDPG和TD3。我们认为这些结果的原因在于SAC对嘈杂和复杂环境的鲁棒性。在我们的攻击中，每一轮都会重新给出一个高维随机潜在向量，而目标身份的置信度对这样的潜在向量变化非常敏感。因此，对于嘈杂和复杂环境具有鲁棒性的强化学习算法，如SAC，是可取的。此外，我们在图6中展示了每个RL代理的奖励变化。</p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table4.png" alt="表4"></p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img6.png" alt="图6"></p><h3 id="伦理的考虑"><a href="#伦理的考虑" class="headerlink" title="伦理的考虑"></a>伦理的考虑</h3><p>如果恶意用户滥用所提出的黑盒模型逆向攻击，可能会带来侵犯需要保护的个人信息等负面社会影响。然而，揭示当前系统的漏洞对于安全性的发展是不可或缺的。通过这项研究，我们提高了对机器学习隐私问题的关注，并敦促社区开发算法或系统以防范所提出的漏洞。我们相信，我们的工作将在安全性方面产生积极的影响，其益处将超过潜在的风险。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们提出了一种基于强化学习的新型黑盒模型逆向攻击，利用生成对抗网络（GAN）。我们将潜在空间的探索问题形式化为马尔可夫决策过程（MDP）问题，并训练一个强化学习代理来解决MDP，即使缺乏关于目标模型的信息，如权重和梯度。所提出的攻击解决了先前黑盒攻击存在的问题。此外，实验结果表明，我们的攻击成功地重构了目标模型的私有数据。我们的攻击不仅优于最先进的黑盒攻击，还优于所有其他方法，包括白盒和仅标签攻击。我们希望这项研究能够推动对黑盒模型逆向攻击和防御的研究。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> 模型逆向 </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CADE: Detecting and Explaining Concept Drift Samples for Security Applications 阅读笔记</title>
      <link href="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/"/>
      <url>/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="CADE-Detecting-and-Explaining-Concept-Drift-Samples-for-Security-Applications"><a href="#CADE-Detecting-and-Explaining-Concept-Drift-Samples-for-Security-Applications" class="headerlink" title="CADE: Detecting and Explaining Concept Drift Samples for Security Applications"></a>CADE: Detecting and Explaining Concept Drift Samples for Security Applications</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Concept drift poses a critical challenge to deploy machine learning models to solve practical security problems. Due to the dynamic behavior changes of attackers (and/or the benign counterparts), the testing data distribution is often shifting from the original training data over time, causing major failures to the deployed model.</p><p>To combat concept drift, we present a novel system CADE aiming to 1) detect drifting samples that deviate from existing classes, and 2) provide explanations to reason the detected drift. Unlike traditional approaches (that require a large number of new labels to determine concept drift statistically), we aim to identify individual drifting samples as they arrive. Recognizing the challenges introduced by the high-dimensional outlier space, we propose to map the data samples into a low-dimensional space and automatically learn a distance function to measure the dissimilarity between samples. Using contrastive learning, we can take full advantage of existing labels in the training dataset to learn how to compare and contrast pairs of samples. To reason the meaning of the detected drift, we develop a distance-based explanation method. We show that explaining “distance” is much more effective than traditional methods that focus on explaining a “decision boundary” in this problem context. We evaluate CADE with two case studies: Android malware classification and network intrusion detection. We further work with a security company to test CADE on its malware database. Our results show that CADE can effectively detect drifting samples and provide semantically meaningful explanations.</p><p>概念漂移对于部署机器学习模型以解决实际安全问题构成了重大挑战。由于攻击者（和/或良性对手）的动态行为变化，测试数据分布通常会随时间从原始训练数据中漂移，导致已部署模型的重大故障。</p><p>为了应对概念漂移，我们提出了一个新颖的系统 CADE，旨在实现以下两个目标：1）检测偏离现有类别的漂移样本，以及2）提供解释来推断检测到的漂移的原因。与传统方法不同（传统方法通常需要大量新标签来统计性地确定概念漂移），我们的目标是在漂移样本到达时识别它们。考虑到高维度离群值空间引入的挑战，我们提出将数据样本映射到低维空间，并自动学习一个距离函数来度量样本之间的差异。通过对比学习，我们可以充分利用训练数据集中的现有标签，以学习如何比较和对比样本对。为了推断检测到的漂移的含义，我们开发了一种基于距离的解释方法。我们证明，在这个问题背景下，解释“距离”要比传统方法（集中于解释“决策边界”）更有效。我们通过两个案例研究对 CADE 进行了评估：Android恶意软件分类和网络入侵检测。我们还与一家安全公司合作，在其恶意软件数据库上测试了 CADE。我们的结果表明，CADE 能够有效地检测漂移样本并提供有意义的解释。</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>在本文中，我们提出了一种新的方法，用于检测漂移样本，并配以一种解释检测决策的新方法。总体而言，我们构建了一个名为“CADE（Contrastive Autoencoder for Drifting Detection and Explanation）”的系统。关键挑战在于推导一个有效的距离函数来衡量样本的不相似度。与随意选择距离函数不同，我们利用对比学习的思想来从现有的训练数据中（基于现有标签）学习距离函数。给定原始分类器的训练数据（多个类别），我们将训练样本映射到低维潜在空间。映射函数通过对比样本来学习，以扩大不同类别样本之间的距离，同时减小相同类别样本之间的距离。我们展示了潜在空间中产生的距离函数能够有效地检测和排名漂移样本。<br>为了解释漂移样本，我们确定了一小组重要的特征，这些特征区分了这个样本与其最近的类别。一个关键观察是传统的（监督）解释方法效果不佳。洞察力在于监督解释方法需要漂移样本和现有类别都有足够的样本来估计它们的分布。然而，鉴于漂移样本位于训练分布之外的稀疏空间，这个要求很难满足。相反，我们发现基于距离变化的解释更有效，即导致漂移样本与其最近类别之间距离发生最大变化的特征。</p><p>贡献：</p><ol><li>我们提出 CADE 来补充现有的基于监督学习的安全应用，以应对概念漂移。我们引入了一种基于对比表示学习的有效方法，用于检测漂移样本。</li><li>我们阐明了监督解释方法在解释离群样本方面的局限性，并为这种情境引入了一种基于距离的解释方法。</li><li>我们对提出的方法进行了广泛的评估，涉及两个应用领域。我们与一家安全公司进行的初步测试表明，CADE是有效的。我们已经在此处发布了CADE的代码，以支持未来的研究。</li></ol><h2 id="Background-and-Problem-Scope"><a href="#Background-and-Problem-Scope" class="headerlink" title="Background and Problem Scope"></a>Background and Problem Scope</h2><h3 id="概念漂移"><a href="#概念漂移" class="headerlink" title="概念漂移"></a>概念漂移</h3><p>概念漂移指的是测试集分布会随着时间变化，导致真实的决策边界发生偏移。这通常会让模型的预测性能随着时间降低。<br>为了检测概念漂移，研究人员提出了各种技术，主要涉及收集新的数据集以对模型行为进行统计评估。在一些研究中，这些工作还需要进行数据标注的努力。在安全应用中，首先了解新攻击的存在并收集与之相关的数据是具有挑战性的。此外，标注数据是耗时且需要相当专业知识的工作。因此，假设大部分输入数据都能被充分标注是不切实际的。</p><h3 id="问题空间"><a href="#问题空间" class="headerlink" title="问题空间"></a>问题空间</h3><p>与使用充分检测和充分标注的数据来检测概念漂移不同，我们对单个样本进行研究，从而发现那些偏离原始样本的数据。这使得我们可以在这种漂移样本到来时检测出概念漂移并标注它们。一旦我们收集到了足够多的数据，就可以直接启动模型重训练。</p><p>在多分类问题中，存在着两种概念漂移：</p><ol><li>新类别的引入</li><li>类别内的变化<br>在这篇文章中，我们主要关注第一类漂移</li></ol><h3 id="可能的解决方案及限制"><a href="#可能的解决方案及限制" class="headerlink" title="可能的解决方案及限制"></a>可能的解决方案及限制</h3><ol><li>第一个方法是使用原始分类器的预测概率。更具体地说，一个监督分类器通常会输出一个预测概率（或置信度），作为预测标签的附带产品。例如，在深度神经网络中，通常使用softmax函数来生成预测概率，该概率表示给定样本属于每个现有类别的可能性（总和为1）。因此，低的预测概率可能表明输入样本与现有的训练数据不同。然而，我们认为在我们的问题背景下，预测概率不太可能有效。原因是该概率反映了相对于现有类别的适应度（例如，样本在类别A中的适应程度优于类别B）。如果样本来自一个全新的类别（既不属于类别A也不属于类别B），那么预测概率可能会极具误导性。许多先前的研究已经证明，来自新类别的测试样本可能会导致误导性的概率分配（例如，将一个错误的类别与高概率关联）。从根本上讲，预测概率仍然继承了分类器的“封闭世界假设”，因此不适合检测漂移样本。</li><li>与预测概率相比，一个更有前景的方向是直接评估样本与给定类别的适应度（按我的理解应该就是相似度）。这个想法是，我们不是评估样本在类别A中的适应程度优于类别B，而是评估该样本在类别A中与其他训练样本的适应程度。例如，可以使用自编码器基于重构误差来评估样本对给定分布的适应度。然而，作为一种无监督方法，当忽略标签时，自编码器很难学习到训练分布的准确表示（详见第4节）。在最近的一项工作中，Jordaney等人引入了一个名为Transcend的系统。它将“非一致性度量”定义为适应度评估。Transcend使用可信度p值来量化测试样本x与属于同一类别的训练样本的相似程度。p是该类别中至少与同一类别的其他样本不相似的样本比例。虽然这个度量可以确定漂移样本，但这样的系统在很大程度上依赖于对“不相似性”的良好定义。正如我们将在第4节中展示的那样，任意的不相似性度量（特别是在数据维度高的情况下）可能导致性能不佳。</li></ol><h2 id="Designing-CADE"><a href="#Designing-CADE" class="headerlink" title="Designing CADE"></a>Designing CADE</h2><p>我们提出了一个叫做CADE的漂移样本检测和解释系统。我们首先描述了设计背后的直觉和见解，然后是每个组件的技术细节。</p><h3 id="设计直觉"><a href="#设计直觉" class="headerlink" title="设计直觉"></a>设计直觉</h3><p>如图一所示，我们的系统由两部分组成：</p><ol><li>检测模块：检测漂移样本</li><li>解释模块：帮助研究人员理解此次漂移意味着什么</li></ol><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231011145903.png" alt="img"></p><p>通过开始的分析，我们发现这两个任务有一个相同的挑战：漂移样本位于稀疏的离群空间，这让开发有意义的距离函数变得非常困难。</p><p>首先，检测漂移样本需要学习一个良好的距离函数，以度量“漂移样本”与现有分布的差异。然而，异常值空间具有无限大且稀疏的特点。对于高维数据，由于“维数诅咒” ，距离的概念开始失去效力。其次，解释的目标是识别一个重要特征的小子集，最有效地区分漂移样本和训练数据。因此，我们还需要一个有效的距离函数来度量这些差异。</p><p>在接下来的部分中，我们设计了一个漂移检测模块和一个解释模块，共同解决这些挑战。在高层次上，我们首先使用对比学习来学习训练数据的压缩表示。对比学习的一个关键优势是它可以利用现有标签，相对于无监督方法如自动编码器和主成分分析（PCA），实现了更好的性能。这使我们能够从训练数据中学习距离函数以检测漂移样本（第3.2节）。至于解释模块，我们将描述一种基于距离的解释公式，以解决前面提到的挑战（第3.3节）。</p><h3 id="漂移样本检测"><a href="#漂移样本检测" class="headerlink" title="漂移样本检测"></a>漂移样本检测</h3><p>漂移检测模型通过检测输入样本实现离群点的检测。</p><h4 id="对比学习实现特征隐表示"><a href="#对比学习实现特征隐表示" class="headerlink" title="对比学习实现特征隐表示"></a>对比学习实现特征隐表示</h4><p>我们探讨了对比学习的思想，以学习训练数据的良好表示。对比学习利用训练数据中的现有标签来学习一个有效的距离函数，以度量不同样本之间的相似性（或对比）。与监督分类器不同，对比学习的目标不是将样本分类到已知类别，而是学习如何比较两个样本。</p><p>如图二所示，对比学习的目的是将输入映射到低维的隐空间。经过映射后，同一类的样本距离更小，不同类的样本距离更大。因此，隐空间中的距离度量可以反映出样本对的差异。任何与所有现有类别表现出较大距离的新样本都有可能是漂移样本。</p><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231011175350.png" alt="img"></p><p>为了实现这个想法，我们使用了一个带有对比损失的自动编码器。自编码器是学习给定输入分布的压缩表示(具有降低的维度)的有用工具。形式上，让$x\in\mathbb{R}^{1\times 1}$是训练集中的一个样本。我们训练一个包含编码器$f$和解码器$h$的自动编码器。$f,h$的参数分别为$\Theta,\Phi$。损失函数如下：</p><script type="math/tex; mode=display">\min_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathbb{E}_{\boldsymbol{x}}\|\boldsymbol{x}-\hat{\boldsymbol{x}}\|_2^2+\lambda\mathbb{E}_{\boldsymbol{x}_i,\boldsymbol{x}_j}\left[(1-y_{ij})d_{ij}^2+y_{ij}(\boldsymbol{m}-d_{ij})_+^2\right]</script><p>该损失函数包含两项：</p><ol><li>重建损失$\mathbb{E}_{\boldsymbol{x}}|\boldsymbol{x}-\hat{\boldsymbol{x}}|_2^2$。即自动编码器对原样本进行重建后得到的样本与原样本距离的二范数。在这个自动编码器中，编码器$f$将$x$映射到一个低维向量$z=f(x,\Theta)$。自动编码器保证这个向量可以以较小的重建损失重建这个样本。</li><li>对比损失$\lambda\mathbb{E}_{\boldsymbol{x}_i,\boldsymbol{x}_j}\left[(1-y_{ij})d_{ij}^2+y_{ij}(\boldsymbol{m}-d_{ij})_+^2\right]$。$(x_i,x_j)$是一个样本对。$y_{i,j}$表示两个样本之间的关系。如果两个样本来自不同的类别，则值为1，否则为0。$(.)_+$是$max(0,.)$的缩写。$d_{i,j}$是样本隐表示$z_i=(x_i;\Theta),z_j=f(x_j;\Theta)$之间的欧几里得距离。$z\in \mathbb{R}^{d\times 1}$。使用该损失，模型会最小化统一类别的样本距离，同时最大化不同类别之间的样本距离（这个距离最大被限制到m）。</li></ol><p>经过对比学习后，$f$可以把相同类别的样本映射为一个较紧的样本簇。在隐空间中使用距离函数就可以把漂移样本检测出来。</p><h4 id="基于MAD的漂移样本检测"><a href="#基于MAD的漂移样本检测" class="headerlink" title="基于MAD的漂移样本检测"></a>基于MAD的漂移样本检测</h4><p>在训练好自动编码器之后，我们就可以使用它来检测漂移样本了。给定$K$个测试样本$\{x_t^{(k)}\}（k=1,\dots,K)$,我们寻求一种方法可以找到这些样本中的漂移样本。检测方法如算法1所示。</p><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231011191936.png" alt="img"></p><p>假设训练集有$N$个类别，每个类别有$n_i$个训练样本。我们首先把所有的训练样本映射到隐空间中(2-4行)。对于每个类别$i$，我们可以计算出一个中心$c_i$（就是直接取样本均值）。对于待检测的测试样本$x_t^{(k)}$，我们同样需要把它映射到隐空间中得到$z_t^{(k)}$（第14行）。接着我们计算测试样本和每个中心之间的欧几里得距离$d_i^{(k)}$。我们基于这个距离判断测试样本是否为离群点。在这我们不使用离测试样本最近的样本点的原因是，训练集中的离群点会影响判断。</p><p>不同的类别有不同的紧凑程度，故对于不同的类别需要使用不同的阈值。我们没有为每个类别手动设定阈值，而是使用了一种叫做“绝对中位差（Median Absolute Deviation，MAD）“的方法。计算出某一类的中心后，需要计算该类每个样本与中心的距离。距离中位数记为$\tilde{d}_i$。则：</p><script type="math/tex; mode=display">MAD_i = b*median(|d_i^{(j)}-\tilde{d}_i|),j=1,\dots,n_i</script><p>基于$MAD_i$，我们就可以决定$d_i^{(k)}$是不是所有类别的离群点。MAD的优势是每个类别都有一个独立的距离阈值，而且这个阈值和每一类的分布相关。举例来说，如果一个类别更分散，则这个阈值会更大。</p><p>需要注意的是，当一个类没有足够的样本时，MAD可能会受到影响，因为它的中值可能容易受噪声影响。在我们的设计中，对比学习可以帮助缓解这个问题，因为每个类被映射到潜在空间中的一个紧凑区域，这有助于稳定中位数。</p><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><p>正如图一所示，研究人员或许需要对漂移样本进行进一步的研究来解释漂移的含义。在研究时间有限的情况下，对每个漂移样本进行排序是非常有必要的。我们使用了一个非常朴素的方法进行排序，即每个样本与最近类别中心的距离（这个距离由27行计算得到）。这让我们可以优先研究距离中心最远的样本。</p><blockquote><p>排序是否也要考虑不同类别之间的离散程度呢？按照我的理解，本文应该是在各个中心的周围选最远的漂移样本。也就是说n个类别就会有n个最远的漂移样本。</p></blockquote><h3 id="解释漂移样本"><a href="#解释漂移样本" class="headerlink" title="解释漂移样本"></a>解释漂移样本</h3><blockquote><p>前面论文中说本文关注的是两种概念漂移中的第一种，即本文中的概念漂移指的是出现了新的类别，而不是类内分布发生变化。但这里却尝试解释漂移样本为啥从某一个类别中漂出来了。按理说新类别中的样本应该和其它类别没啥关系。</p></blockquote><p>解释模型的目的是找到致使样本产生漂移的最重要的特征（重要指的是对漂移现象重要）。给定一个漂移样本$x_t$和离它最近的类别$y_t$，我们需要找出来一个使得$x_t$成为$y_t$离群点的特征的子集。为了实现这个目标，直觉上我们可以把问题转化为监督学习模型的可解释性。举个例子，我们可以把漂移检测器当作是一个分类器（二分类，漂移或者没漂），然后利用现有的对于分类器的解释手段来解释漂移现象。然而，由于利群空间的高度稀疏性，我们发现想让一个漂移样本跨国决策边界是很难的，从而导致解释失败。受此启发，我们针对漂移检测设计了一种新的解释方法。这种解释方法解释了漂移样本和类内样本之间的距离，而不是决策边界。下面，我们首先分析直觉上的想法，然后再解释我们的方法。</p><p>直觉上的方法：略</p><h4 id="我们的方法：基于距离的解释"><a href="#我们的方法：基于距离的解释" class="headerlink" title="我们的方法：基于距离的解释"></a>我们的方法：基于距离的解释</h4><p>与有监督分类器不同，漂移检测器的决策基于样本与类别中心的距离。于是我们的目标就是从原始特征中找到使概念漂移发生的（特征）子集。在这种设定下，我们不需要让$x_t$跨越决策边界。我们只需要在原始样本上添加扰动，并观察隐空间中样本的变化。</p><blockquote><p>在这种设定下，选取出来的特征似乎和编码器的映射方式有很大的关系。</p></blockquote><p>为了实现这种想法，我们需要设计一种扰动机制。大多数现存的机制都是为图像数据设计的。而我们需要一种既可以作用于连续数据，也可以作用于离散数据的扰动机制。为了满足这种要求，我们通过把$x_t$的特征修改为$x_{yt}^{(c)}$的对应特征从而实现扰动。$x_{yt}^{(c)}$为距离类别c最近的样本点。因此，我们的目的就是寻找哪些样本修改之后会对隐空间对应的样本点造成最大的影响。将特征修改为$x_{yt}^{(c)}$的特征也保证了样本点会大致向着类别中心靠近。</p><p>我们用$m\in \mathbb{R}^{q\times 1}$表示重要特征。$m$是一个掩码，值为1表示此处的特征被替换。 每一个$m_i$都可以以概率$p_i$从伯努利分布中采样。下面我们的问题就转化为了求解$p_{1:q}$使下面的式子最小：</p><script type="math/tex; mode=display">\begin{aligned}&\mathbb{E}_{\boldsymbol{m}\sim Q(\boldsymbol{p})}\|\hat{\boldsymbol{z}}_t-\boldsymbol{c}_{y_t}\|_2+\lambda_1R(\boldsymbol{m},\boldsymbol{b}), \\&\hat{z}_t=f(x_t\odot(1-m\odot b)+x_{y_t}^{(c)}\odot(m\odot b)), \\&R(m,b)=\|m\odot b\|_1+\|m\odot b\|_2,\quad Q(p)=\prod_{i=1}^qp(m_i|p_i)\end{aligned}</script><p>$\tilde{z}_t$是扰动样本的隐空间向量。$R(m,b)$控制了$m$中非0元素的个数。为了加快速度，用$b$表示一个pre-filter，如果$(x_t)_i=(x_{yt}^{(c)})_i$ ，则$(b)_i=0$。也就是不对 漂移样本和中心附近的样本相同的分量 做优化。</p><p>注意到伯努利分布是离散的，我们不能直接用梯度下降的方式解决这个问题，所以我们使用了伯努利分布的连续近似分布来进行求解。</p><h2 id="评估：漂移检测"><a href="#评估：漂移检测" class="headerlink" title="评估：漂移检测"></a>评估：漂移检测</h2><p>我们使用两个数据集进行评估</p><ol><li>Drebin：Android 恶意软件检测数据集。我们从中选取了8类样本，每一类样本至少有100条记录（总共3317个样本）。在实验之前，随机选取一类作为漂移样本，漂移样本只存在于测试集中。我们的目标是在测试阶段正确的识别出漂移样本。训练集和测试集按照时间顺序8-2分。通过特征筛选，最后得到了1340条特征。为了使实验结果更具有普适性，我们迭代的选择每一个类别作为漂移类并重复实验。</li></ol><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012165437.png" alt="img"></p><ol><li>网络入侵检测系统：CICIDS-2018。为了加速实验，我们选择了10%的流量作为实验数据集。其它数据集分割及实验方式和上述一致。</li></ol><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012165437.png" alt="img"></p><p>使用精准率、召回率、F1-score等作为评价指标。使用两种方法作为baseline：</p><ol><li>Vanilla autoencoder。用它来进行数据降维，使用本文的方法做漂移检测。用来说明对比学习的优势。</li><li>Transcend。Transcend定义了一个”非一致性测度”来量化传入样本与预测类别的吻合程度，并计算可信度p值来判断传入样本是否为漂移样本。</li></ol><h3 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h3><p>漂移样本检测性能。我们首先使用一个实验设置来解释我们的评估过程。以Drebin数据集为例。假设我们将Iconosys家族作为测试集中以前未见的家族。在训练检测模型（不包含任何Iconosys样本）之后，我们使用该模型来检测和排名漂移样本。为了评估排名列表的质量，我们模拟了一位分析师从列表顶部检查样本的过程。</p><p>图4a显示，当我们检查更多的漂移样本（最多150个样本）时，精确度保持在较高水平（超过0.97），而召回率逐渐达到100%。结合精确度和召回率，最高的F1得分为0.98。在150个样本之后，精确度将下降，因为剩余集合中没有更多未见类别的样本了。这证实了排名列表的高质量，意味着几乎所有来自未见家族的样本都排在了最前面。</p><p>与之相比，Transcend和Vanilla AE的排名列表并不令人满意。对于Transcend（图4b），前150个样本的精确度和召回率都很低，表明排名靠前的样本并不来自未见家族。在检查了150个样本之后，我们开始看到更多来自未见家族的样本。在检查了350个样本之后，Transcend已经涵盖了大部分来自未见家族的样本（即召回率接近1.0），但精确度仅为0.46。这意味着分析师检查的样本中超过一半与问题无关。最佳的F1得分为0.63。如图4c所示，Vanilla AE的性能更差。即使在检查了600个样本之后，召回率仅略高于0.8。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012182526.png" alt="img"></p><p>表3证实了CADE能够准确检测漂移样本，并且优于两个基准模型。在Drebin数据集上，CADE的平均F1得分为0.96，而基准模型的F1得分分别为0.80和0.72。对于IDS2018数据集，可以得出类似的结论。此外，CADE的标准差要小得多，表明在不同的实验设置中具有更一致的性能。最后，我们展示了CADE具有较低的归一化检查工作量，这证实了排名的高质量。<br>请注意，Transcend基准模型在某些情况下确实表现良好。例如，在IDS2018数据集中将DoS-Hulk设置为未见家族时，其F1得分为99.69%（与我们的系统类似）。然而，问题在于Transcend在不同的设置中表现不稳定，这在表3中的高标准差中得到了体现。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012183835.png" alt="img"></p><p>对对比学习的影响。为了了解性能提升的来源，我们研究了对比学习的影响。首先，我们在图7中呈现了Drebin数据集的训练样本和来自选择的未见家族（FakeDoc）的测试样本的t-SNE图。t-SNE  是一种非线性降维技术，可以将数据样本投影到二维图中进行可视化。为了可视化我们的数据样本，我们将样本从原始空间（1,340维）映射到二维空间（图7a）。同时，我们还将样本从潜空间（7维）映射到二维空间进行比较（图7b和图7c）。我们可以观察到，CADE的潜空间中的样本形成了更紧密的聚类，这使得更容易将现有样本与未见家族区分开来。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012182204.png" alt="img"></p><p>为了从统计的角度观察不同的实验设置，我们绘制了图8。与之前类似，我们迭代地将一个家族作为Drebin数据集中的未见家族。然后，我们测量测试样本到其在原始特征空间中最近质心的距离（图8a），以及CADE生成的潜空间中的距离（图8b）。IDS2018数据集的结果得出了相同的结论，为了简洁起见，省略了在此展示。我们展示了漂移样本和非漂移样本在原始空间中更难分离。经过对比学习后，潜空间中的分离更加明显。原因在于对比学习学习到了一个适合的距离函数，可以将不同类别的样本拉得更远，从而更容易检测到未见家族。</p><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012184117.png" alt="img"></p><p>案例研究：CADE的局限性。在大多数设置中，CADE的表现良好。然而，我们发现在某些情况下，CADE的性能受到影响。例如，当将FakeInstaller作为未见家族时，我们的检测精确度仅为82%，而召回率达到100%。我们注意到，许多来自GingerMaster和Plankton家族的测试样本被检测为漂移样本。经过仔细检查，我们发现，当将FakeInstaller作为未见家族时，为了保持整体的80:20的训练-测试比例，在GingerMaster和Plankton家族尚没有足够的训练样本时，我们需要在这个时间点拆分数据集。因此，许多来自GingerMaster和Plankton家族的测试样本与这两个家族中少量训练样本（基于潜空间距离）非常不同。外部证据也表明，这两个家族有许多变种[5, 70]。虽然这些恶意软件变种不属于新的家族（根据我们的定义是误报），但它们对于研究理解同一家族内的恶意软件变异也具有价值。</p><h2 id="评估：解释漂移样本"><a href="#评估：解释漂移样本" class="headerlink" title="评估：解释漂移样本"></a>评估：解释漂移样本</h2><p>为了评估解释模块，在每个数据集中随机选择一个类（例如，在Drebin数据集中选择FakeDoc，在IDS2018数据集中选择Infiltration）作为漂移样本。其他设置的结果得出相同的结论，为了简洁起见，省略了这些结果。在这个设置下，我们为检测到的漂移样本生成解释，并对解释结果进行定量和定性评估。</p><p>使用三种方法作为baseline：</p><ol><li>random。随机选择特征作为重要特征</li><li>基于边界。如前文所说</li><li>无监督的解释方法COIN。COIN构建了一组本地的LinearSVM分类器，用于将一个个体离群值与其在分布邻域样本中进行分离。由于LinearSVM分类器是自解释的，它们可以指出对离群值分类起重要作用的特征。为了进行公平比较，我们选择了与我们的方法相同数量的顶部特征作为基线方法。这些基线方法的实现和超参数可以在附录B中找到。请注意，我们没有选择现有的黑盒解释方法（例如，LIME 和SHAP）作为我们的比较基线。这是因为白盒方法通常比黑盒方法表现更好，这要归功于它们对原始模型的访问权限。</li></ol><p>评估指标：定量的情况下，我们直接评估选择特征带来的距离变化。给定一个测试样本$x_t$和解释方法，我们可以得到一个特征子集$m_t$。我们通过这个指标来量化这个解释结果的精确性：$d_{xt}’=||f(x_t\odot(1-m_t)+x_{yt}^{(c)}\odot m_t)-c_{yt}||_2$。这个值表示了隐空间中扰动样本和它最近的样本中心之间的距离。如果这个特征真的是重要的，那么扰动样本和最近的样本之间的距离会变小。</p><p>除了这个$d_{xt}’$指标之外，我们还使用传统的度量指标（第5.2节）来检查能够穿越决策边界的扰动样本的比例。</p><h3 id="精确性评价结果"><a href="#精确性评价结果" class="headerlink" title="精确性评价结果"></a>精确性评价结果</h3><p>特征对距离的影响。表4显示了所有漂移样本的$d_{xt}’$的均值和标准差（即扰动样本到最近质心之间的距离）。我们有四个关键观察结果。首先，基于随机选择的特征扰动漂移样本几乎不会影响潜空间距离（比较行2和行3）。其次，基于边界的解释方法可以在两个数据集上降低26%–47%的距离（比较行2和行4）。这表明这种策略具有一定的有效性。然而，绝对距离值仍然很高。第三，COIN减小了IDS2018数据集中的潜空间距离（比较行2和行5），但在Drebin数据集中平均距离却有所增加。实质上，COIN是一种专门的基于边界的方法，它使用一组LinearSVM分类器来逼近决策边界。我们发现COIN在高维空间上效果不好，并且很难将漂移样本拉过边界（将在第5.3节讨论）。最后，我们在CADE中的解释模块具有最低的距离度量均值和标准差。距离与原始距离相比显著降低（即在Drebin上降低了98.8%，在IDS2018上降低了79.9%，比较行2和行6）。特别是，CADE在边界解释方法方面表现出色。由于我们的方法克服了样本稀疏性和不平衡的问题，它能够准确指出对距离（影响漂移检测决策）有更大影响的有效特征。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012194227.png" alt="img"></p><p>选择的特征数量。总体而言，选择的特征数量较少，这使得手动解释成为可能。如前所述，我们将所有方法配置为选择相同数量的重要特征（与CADE相同）。对于Drebin数据集，平均选择的特征数量为44.7，标准差为6.2。这在1000多个特征中只占很小一部分（3%）。类似地，IDS2018数据集的平均选择特征数量为16.2，约占所有特征的20%。</p><h3 id="跨越决策边界"><a href="#跨越决策边界" class="headerlink" title="跨越决策边界"></a>跨越决策边界</h3><p>上述评估结果证实了所选特征对距离度量的影响，而这恰恰是CADE旨在优化的内容。为了提供另一个视角，我们进一步研究了所选特征对穿越决策边界的影响。具体而言，我们计算了成功穿越决策边界的扰动样本的比例。如表6所示，我们确认在漂移检测的上下文中，大多数情况下穿越边界是困难的。特别是，CADE能够将97.64%的扰动样本推动穿越Drebin数据集的检测边界，但只有1.41%的样本能够穿越IDS2018数据集的边界。相比之下，基线方法在原始特征空间中很少能够成功扰动漂移样本使其穿越边界。通过放宽这个条件并专注于距离变化，我们的方法在确定重要特征方面更加有效。</p><h3 id="样例学习"><a href="#样例学习" class="headerlink" title="样例学习"></a>样例学习</h3><p>为了证明我们的方法确实捕捉到了有意义的特征，我们展示了一些案例研究。在表5中，我们展示了Drebin数据集的一个案例研究。我们选择了FakeDoc是未见过的家族，并随机选择一个漂移样本来运行解释模块。在1000多个特征中，我们的解释模块确定了42个重要特征，其中27个特征的值为“1”（表示该样本包含这些特征）。如表5所示，最接近的家族是GingerMaster。</p><p>我们手动检查这些特征，以确定它们是否具有正确的语义含义。虽然很难获取“地面真相”的解释，但我们收集了有关FakeDoc恶意软件和GingerMaster的外部分析报告[68, 70]。根据这些报告，与GingerMaster相比，FakeDoc恶意软件的一个关键区别是它通常通过短信订阅高级服务并向受害用户收费。如表5所示，许多选定的特征与读取、写入和发送短信的权限和API调用有关。我们突出显示了与短信相关功能匹配的这些特征。其他相关特征也被突出显示。例如，“RESTART_PACKAGES”权限允许恶意软件终止后台进程（例如显示传入短信的进程），以避免提醒用户。权限“DISABLE_KEYGUARD”允许恶意软件在不解锁屏幕的情况下发送高级短信消息。“WRITE_SETTINGS”有助于偷偷写入系统设置以发送短信。”<a href="https://ws.tapjoyads.com/">https://ws.tapjoyads.com/</a>“ 是FakeDoc通常使用的广告库。再次强调，这小部分特征是从1000多个特征中选择出来的。我们得出结论，这些特征高度指示了该样本与最近的已知家族有何不同</p><h2 id="评估：类内漂移"><a href="#评估：类内漂移" class="headerlink" title="评估：类内漂移"></a>评估：类内漂移</h2><p>到目前为止，我们的评估主要集中在一种概念漂移类型（类型A），即漂移样本来自以前未见过的家族。接下来，我们探索如何调整我们的解决方案以应对另一种概念漂移类型（类型B），即漂移样本来自现有类别。我们在一个二元分类设置下进行了一项简要的实验。</p><p>更具体地说，我们首先使用Drebin数据集训练了一个二元SVM分类器，用于将恶意样本与良性样本进行分类。该分类器在Drebin数据集上表现出很高的准确性，训练时的F1得分为0.99。我们想测试该分类器在另一个名为Marvin 的Android恶意软件数据集上的表现。相对于Drebin数据集（2010年至2012年），Marvin数据集略微更新（2010年至2014年）。我们首先移除Marvin数据集中与Drebin重叠的样本，以确保Marvin样本是真正的先前未见过的。这样，Marvin数据集中剩下了9,592个良性样本和9,179个恶意样本。</p><p>在这个实验中，我们将Marvin数据集随机分成验证集和测试集（50:50）。对于这两个集合，我们保持恶意样本和良性样本的平衡比例。我们将原始的分类器（在Drebin数据上训练）应用于Marvin的测试集。我们发现由于可能存在恶意类别和/或良性类别内的评估问题，测试准确率不再很高（F1得分为0.70）。</p><p>为了解决类内漂移的问题，我们在Marvin的验证集上应用CADE和Transcend来识别少量的漂移样本（它们可以是良性或恶意的）。我们通过使用它们的“真实标签”来模拟对这些样本进行标记，然后将这些带有标签的漂移样本添加回Drebin的训练数据中，重新训练二元分类器。最后，我们在Marvin的测试集上测试重新训练的分类器。</p><p>根据表7的数据，我们发现CADE仍然明显优于Transcend。例如，通过添加仅150个漂移样本（占Marvin验证集的1.7%）进行重新训练，CADE将二元分类器的F1得分提升至0.92。而对于Transcend来说，相同数量的样本只能将F1得分提升至0.74。此外，我们还发现CADE的速度更快：CADE的运行时间为1.2小时（而Transcend需要10小时）。这个实验证实了CADE可以适应处理二元恶意软件分类器的类内演化问题。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012204614.png" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 概念漂移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码执行日志记录</title>
      <link href="/2023/10/05/dai-ma-zhi-xing-ri-zhi-ji-lu/"/>
      <url>/2023/10/05/dai-ma-zhi-xing-ri-zhi-ji-lu/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>忘了在哪个开源项目里看到了一个执行日志系统。它会在每次运行时备份代码。对于我这种随时会忘掉实验结果和实验设置的人来说，这种东西应该会很有用……吧。凭印象和自己的喜好复现了一下，具体如下。</p><p>首先，项目应该保持如下的结构。code路径下存放所有的代码，任何数据和中间运行结果都<strong>不</strong>应该放在这里。log目录下存放所有的日志记录。其它的目录根据自己喜好设置。<br><img src="/2023/10/05/dai-ma-zhi-xing-ri-zhi-ji-lu/path.png" alt="代码路径"></p><p>在code/util路径下，新建一个code_logger.py。粘贴如下内容。该日志模块下集成了代码备份、参数保存、日志输出与保存的功能。</p><pre class=" language-lang-python"><code class="language-lang-python">import osfrom datetime import datetimeimport shutilimport jsonimport loggingclass CodeLogger:    def __init__(self, comment=None, args=None):        self.run_time = datetime.now()        self.time_fmt = "%Y-%m-%d-%H-%M-%S"        # self.time_fmt = "%Y%m%d%H%M%S"        self.comment = comment        self.log_dir = self.run_time.strftime(self.time_fmt)        if comment is not None:            self.log_dir = f"{self.log_dir}-{comment}"        self.log_dir = os.path.join("../log", self.log_dir)        self.args = args        # create log dir if not exists        if not os.path.exists(self.log_dir):            os.mkdir(self.log_dir)        # back up codes        shutil.copytree("../code/", os.path.join(self.log_dir, "code/"))        if self.args is not None:            self.save_args(args)        self.logger = self.init_logger()    def save_args(self, args):        with open(os.path.join(self.log_dir, "args.json"), "w") as f:            json.dump(args.__dict__, f)    def init_logger(self):        logger = logging.getLogger(self.comment)        logger.setLevel(logging.INFO)        log_format = logging.Formatter(            "%(asctime)s %(levelname)-8s %(message)s")        filename = os.path.join(self.log_dir, "run.log")        log_handler = logging.FileHandler(filename, mode="w")        log_handler.setLevel(logging.INFO)        log_handler.setFormatter(log_format)        logger.addHandler(log_handler)        console_logger = logging.StreamHandler()        console_logger.setLevel(logging.INFO)        console_logger.setFormatter(log_format)        logger.addHandler(console_logger)        return logger</code></pre><p>在使用该模块时，需要先import该模块。演示如下：</p><pre class=" language-lang-python"><code class="language-lang-python">from util.models import get_architecturefrom util.code_logger import CodeLoggerimport argparseparser = argparse.ArgumentParser()parser.add_argument("--model", type=str, default="resnet")parser.add_argument("--dataset", type=str, default="cifar10")parser.add_argument("--epochs", type=int, default=150)parser.add_argument("--batch_size", type=int, default=256)parser.add_argument("--lr", type=float, default=0.01)parser.add_argument("--verbose", action="store_true")args = parser.parse_args()model_name = args.modeldataset = args.datasetepochs = args.epochsbatch_size = args.batch_sizelr = args.lrverbose = args.verbosecl = CodeLogger(comment=f"train model {model_name}_{dataset}", args=args)''' 上面是日志模块初始化，初始化会做三件事：1. 在log文件夹下新建一个文件夹，文件夹名称为“当前时间-comment内容”。2. 把code路径下所有文件保存到新建的文件夹中3. 把运行参数args存放到新建文件夹的args.json中'''cl.logger.info(f"test output") # 控制台输出，同时会把输出内容保存到新建文件夹下的run.log中</code></pre><p>如果再发生没有记录实验结果，或者忘掉实验设置时，就可以通过查找log目录下的代码备份或者参数备份来一键找回了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 日志系统 </tag>
            
            <tag> 执行日志 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023安全顶会AI安全相关汇总</title>
      <link href="/2023/09/06/2023-an-quan-ding-hui-ai-an-quan-xiang-guan-hui-zong/"/>
      <url>/2023/09/06/2023-an-quan-ding-hui-ai-an-quan-xiang-guan-hui-zong/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="NDSS"><a href="#NDSS" class="headerlink" title="NDSS"></a>NDSS</h2><h3 id="对抗样本防御"><a href="#对抗样本防御" class="headerlink" title="对抗样本防御"></a>对抗样本防御</h3><ol><li><a href="https://www.ndss-symposium.org/ndss-paper/adversarial-robustness-for-tabular-data-through-cost-and-utility-awareness/">Adversarial Robustness for Tabular Data through Cost and Utility Awareness</a></li><li><a href="https://www.ndss-symposium.org/ndss-paper/bars-local-robustness-certification-for-deep-learning-based-traffic-analysis-systems/">BARS: Local Robustness Certification for Deep Learning based Traffic Analysis Systems</a>    流量相关</li></ol><h3 id="后门攻击"><a href="#后门攻击" class="headerlink" title="后门攻击"></a>后门攻击</h3><ol><li><a href="https://www.ndss-symposium.org/ndss-paper/backdoor-attacks-against-dataset-distillation/">Backdoor Attacks Against Dataset Distillation</a></li></ol><h2 id="S-amp-P"><a href="#S-amp-P" class="headerlink" title="S&amp;P"></a>S&amp;P</h2><h3 id="AI伦理"><a href="#AI伦理" class="headerlink" title="AI伦理"></a>AI伦理</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179333">How technical do you get? I’m an English teacher”: Teaching and Learning Cybersecurity and AI Ethics in High School</a></li></ol><h3 id="AI-amp-差分隐私"><a href="#AI-amp-差分隐私" class="headerlink" title="AI&amp;差分隐私"></a>AI&amp;差分隐私</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179409">A Theory to Instruct Differentially-Private Learning via Clipping Bias Reduction</a></li><li><a href="https://ieeexplore.ieee.org/document/10179466">Continual Observation under User-level Differential Privacy</a></li><li><a href="https://ieeexplore.ieee.org/document/10179389">Locally Differentially Private Frequency Estimation Based on Convolution Framework</a></li><li><a href="Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering">Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering</a></li></ol><h3 id="对抗样本攻击"><a href="#对抗样本攻击" class="headerlink" title="对抗样本攻击"></a>对抗样本攻击</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179473">AI-Guardian: Defeating Adversarial Attacks using Backdoors</a></li><li><a href="https://ieeexplore.ieee.org/document/10179303">SoK: Certified Robustness for Deep Neural Networks</a></li></ol><h3 id="后门攻击-1"><a href="#后门攻击-1" class="headerlink" title="后门攻击"></a>后门攻击</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179375">Redeem Myself: Purifying Backdoors in Deep Learning Models using Self Attention Distillation</a></li><li><a href="https://ieeexplore.ieee.org/document/10179308">Disguising Attacks with Explanation-Aware Backdoors</a></li></ol><h3 id="推理攻击"><a href="#推理攻击" class="headerlink" title="推理攻击"></a>推理攻击</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179334">SNAP: Efficient Extraction of Private Properties with Poisoning</a></li><li><a href="https://ieeexplore.ieee.org/document/10179463">Accuracy-Privacy Trade-off in Deep Ensemble: A Membership Inference Perspective</a></li><li><a href="https://ieeexplore.ieee.org/document/10179281">SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</a></li></ol><h3 id="模型萃取攻击"><a href="#模型萃取攻击" class="headerlink" title="模型萃取攻击"></a>模型萃取攻击</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179406">D-DAE: Defense-Penetrating Model Extraction Attacks</a></li></ol><h3 id="机器学习可解释性"><a href="#机器学习可解释性" class="headerlink" title="机器学习可解释性"></a>机器学习可解释性</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179321">Everybody’s Got ML, Tell Me What Else You Have: Practitioners’ Perception of ML-Based Security Tools and Explanations</a></li></ol><h2 id="USENIX"><a href="#USENIX" class="headerlink" title="USENIX"></a>USENIX</h2><h3 id="对抗样本攻击-1"><a href="#对抗样本攻击-1" class="headerlink" title="对抗样本攻击"></a>对抗样本攻击</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/wu-xinghui">KENKU: Towards Efficient and Stealthy Black-box Adversarial Attacks against ASR Systems</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/bethany">Towards Targeted Obfuscation of Adversarial Unsafe Images using Reconstruction and Counterfactual Super Region Attribution Explainability</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhu">TPatch: A Triggered Physical Adversarial Patch</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-shibo">CAPatch: Physical Adversarial Patch against Image Captioning Systems</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/tao">Hard-label Black-box Universal Adversarial Patch Attack</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/sheatsley">The Space of Adversarial Strategies</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/liu-aishan">X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yu-zhiyuan-smack">SMACK: Semantically Meaningful Adversarial Audio Attack</a> audio</li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/eykholt">URET: Universal Robustness Evaluation Toolkit (for Evasion)</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yuan-yuanyuan-certification">Precise and Generalized Robustness Certification for Neural Networks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-jiawei">DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing</a></li></ol><h3 id="成员推理攻击"><a href="#成员推理攻击" class="headerlink" title="成员推理攻击"></a>成员推理攻击</h3><h3 id="后门攻击-2"><a href="#后门攻击-2" class="headerlink" title="后门攻击"></a>后门攻击</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/qi">Towards A Proactive ML Approach for Detecting Backdoor Poison Samples</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-zhuo-pelican">PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis</a> 二进制代码分析自动发现后门</li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/lv">A Data-free Backdoor Injection Approach in Neural Networks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/tian">Sparsity Brings Vulnerabilities: Exploring New Metrics in Backdoor Attacks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/wei-chengan">Aliasing Backdoor Attacks on Pre-trained Models</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/pan">ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/bai">VILLAIN: Backdoor Attacks Against Vertical Split Learning</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/fu-chong">FreeEagle: Detecting Complex Neural Trojans in Data-Free Cases</a></li></ol><h3 id="投毒攻击"><a href="#投毒攻击" class="headerlink" title="投毒攻击"></a>投毒攻击</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zeng">Meta-Sift: How to Sift Out a Clean Subset in the Presence of Data Poisoning?</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/li-xiaoguang">Fine-grained Poisoning Attack to Local Differential Privacy Protocols for Mean and Variance Estimation</a></li></ol><h3 id="比特翻转-bit-flip-攻击"><a href="#比特翻转-bit-flip-攻击" class="headerlink" title="比特翻转(bit-flip)攻击"></a>比特翻转(bit-flip)攻击</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/wang-jialai">Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/liu-qi">NeuroPots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks</a></li></ol><h3 id="差分隐私"><a href="#差分隐私" class="headerlink" title="差分隐私"></a>差分隐私</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/nanayakkara">What Are the Chances? Explaining the Epsilon Parameter in Differential Privacy</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/nasr">Tight Auditing of Differentially Private Machine Learning</a></li></ol><h3 id="模型水印"><a href="#模型水印" class="headerlink" title="模型水印"></a>模型水印</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yan">Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation</a></li></ol><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/niu">CodexLeaks: Privacy Leaks from Code Generation Language Models in GitHub Copilot</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/christou">IvySyn: Automated Vulnerability Discovery in Deep Learning Frameworks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/mink">“Security is not my field, I’m a stats guy”: A Qualitative Root Cause Analysis of Barriers to Adversarial Machine Learning Defenses in Industry</a>  对抗训练相关</li></ol><h2 id="CCS"><a href="#CCS" class="headerlink" title="CCS"></a>CCS</h2><h3 id="后门攻击-3"><a href="#后门攻击-3" class="headerlink" title="后门攻击"></a>后门攻击</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616617">Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information</a></li></ol><h3 id="模型窃取"><a href="#模型窃取" class="headerlink" title="模型窃取"></a>模型窃取</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616652">Stealing the Decoding Algorithms of Language Models</a>语言模型的解码算法、超参数窃取</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616653">Stolen Risks of Models with Security Properties</a> 强化学习模型隐私风险验证</li></ol><h3 id="差分隐私-amp-机器学习"><a href="#差分隐私-amp-机器学习" class="headerlink" title="差分隐私&amp;机器学习"></a>差分隐私&amp;机器学习</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616593">DPMLBench: Holistic Evaluation of Differentially Private Machine Learning</a></li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623142">Geometry of Sensitivity: Twice Sampling and Hybrid Clipping in Differential Privacy with Optimal Gaussian Noise and Application to Deep Learning</a></li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623165">Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation</a> 图神经网络</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616592">DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass</a> 语言模型</li></ol><h3 id="其它-1"><a href="#其它-1" class="headerlink" title="其它"></a>其它</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623116">Stateful Defenses for Machine Learning Models Are Not Yet Secure Against Black-box Attacks</a> 黑盒攻击增强策略</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623069">Prediction Privacy in Distributed Multi-Exit Neural Networks: Vulnerabilities and Solutions</a></li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623173">Devil in Disguise: Breaching Graph Neural Networks Privacy through Infiltration</a> 对图神经网络的攻击</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623189">Evading Watermark based Detection of AI-Generated Content</a> 生成模型水印检测规避</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616681">Interactive Proofs For Differentially Private Counting</a> 交互式差分隐私证明（可能没有AI相关的内容）</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623076">SalsaPicante: A Machine Learning Attack on LWE with Binary Secrets</a> 用机器学习攻击量子密码系统</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623117">Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting</a> 攻击恶意软件检测模型</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623130">“Get in Researchers; We’re Measuring Reproducibility”: A Reproducibility Study of Machine Learning Papers in Tier 1 Security Conferences</a> 论文可复现性检查</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616588">DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models</a> 生成图像检测</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623177">Attack Some while Protecting Others: Selective Attack Strategies for Attacking and Protecting Multiple Concepts</a> </li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623093">Unforgeability in Stochastic Gradient Descent</a> SGD执行的可伪造性</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 顶会 </tag>
            
            <tag> 汇总 </tag>
            
            <tag> AI安全 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SplineCam 阅读笔记</title>
      <link href="/2023/08/07/splinecam/"/>
      <url>/2023/08/07/splinecam/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="SplineCam-Exact-Visualization-and-Characterization-of-Deep-Network-Geometry-and-Decision-Boundaries"><a href="#SplineCam-Exact-Visualization-and-Characterization-of-Deep-Network-Geometry-and-Decision-Boundaries" class="headerlink" title="SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries"></a>SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries</h1><h2 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h2><ul><li><strong>Type</strong>: Journal Article</li><li><strong>Title</strong>: SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries, </li><li><strong>Author</strong>: Humayun, Ahmed Imtiaz; Balestriero, Randall; Balakrishnan, Guha; Baraniuk, Richard G, </li></ul><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Current Deep Network (DN) visualization and interpretability methods rely heavily on data space visualizations such as scoring which dimensions of the data are responsible for their associated prediction or generating new data features or samples that best match a given DN unit or representation. In this paper, we go one step further by developing the first provably exact method for computing the geometry of a DN’s mapping – including its decision boundary – over a specified region of the data space. By leveraging the theory of Continuous Piece-Wise Linear (CPWL) spline DNs, SplineCam exactly computes a DN’s geometry without resorting to approximations such as sampling or architecture simplification. SplineCam applies to any DN architecture based on CPWL activation nonlinearities, including (leaky) ReLU, absolute value, maxout, and maxpooling and can also be applied to regression DNs such as implicit neural representations. Beyond decision boundary visualization and characterization, SplineCam enables one to compare architectures, measure generalizability, and sample from the decision boundary on or off the data manifold. Project website: bit.ly/splinecam.</p><p>当前深度网络(DN)的可视化和解释方法主要依赖于数据空间的可视化，例如评估哪些数据维度对其相关预测负责，或生成与给定的DN单元或表示最匹配的新数据特征或样本。在本文中，我们进一步发展了第一个可证明的方法，用于计算在数据空间的特定区域内，DN的映射几何结构，包括其决策边界。通过利用连续分段线性(CPWL)样条DN的理论，SplineCam可以精确计算DN的几何结构，而无需采用采样或架构简化等近似方法。SplineCam适用于基于CPWL激活非线性的任何DN架构，包括（leaky）ReLU、绝对值、maxout和最大池化，并且还可以应用于回归DN，例如隐式神经表示。除了决策边界的可视化和特征描述外，SplineCam还可以用于比较架构、测量泛化能力，并从数据流形上或离数据流形上的决策边界中进行抽样。项目网站：bit.ly/splinecam。</p><hr><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>在本文中，我们专注于使用连续分段线性(CPWL)激活函数$\sigma$的深度网络(DNs)，例如(leaky-)ReLU、绝对值和最大池化。在这种设置下，整个深度网络本身成为一个CPWL算子，即在其定义域的划分区域内，其映射是仿射的。之前的研究专注于估计这类CPWL DNs的划分，并将实证结果与可解释性相结合。例如，Raghu等人[34]表明划分密度提供了DN表现能力的度量，Hanin等人[13]将DN划分密度与学习函数的复杂性联系起来，Jordan等人[20]近似DN的划分以提供稳健性证书，Zhang等人[43]解释了与DN划分相关的dropout影响，Balestriero等人[3]提出改进批归一化以进一步适应数据几何的DN划分，Humayun等人[17, 18]提出通过近似DN划分来控制预训练生成网络的输出分布，Chen等人[25]提出了一种基于划分统计的神经网络架构搜索方法。尽管这些方法都取得了成功，但它们都依赖于DN划分的近似。</p><p>我们提出了SplineCam，一种无需采样的方法来精确计算DN的划分。我们的方法在输入空间的二维域上计算划分，可以轻松适应DN的宽度和深度，可以处理卷积层和跳跃连接，并且可以扩展到发现大量的区域，与之前的方法相比具有更高的可扩展性。我们的方法还允许基于划分统计对输入空间进行局部特征描述，并且能够有效地采样任意多个样本，这些样本可以证明位于DN的决策边界上 - 为可视化和解释提供了新的途径。我们总结我们的贡献如下：</p><ul><li>我们开发了一种可扩展的枚举方法，它可以在给定有界的DN输入空间的二维域的情况下，计算DN的输入空间划分（也称为线性区域）和决策边界。</li><li>我们开发了SplineCam，利用我们新的枚举方法直接可视化DN的输入空间划分，计算划分统计量，并从决策边界进行采样。</li><li>我们进行了定量分析，证明了SplineCam对于表征DN、比较架构选择和训练方式的能力。我们展示了SplineCam在不同深度网络架构和训练策略下的性能，并通过具体指标和对比实验来量化其优势。这些结果证明了SplineCam作为一种强大的工具，可以帮助深度学习研究人员更好地理解和分析深度网络的性质和性能。<h2 id="The-Exact-Geometry-and-Decision-Boundary-of-Continuous-Piece-Wise-Linear-Deep-Networks"><a href="#The-Exact-Geometry-and-Decision-Boundary-of-Continuous-Piece-Wise-Linear-Deep-Networks" class="headerlink" title="The Exact Geometry and Decision Boundary of Continuous Piece-Wise Linear Deep Networks"></a>The Exact Geometry and Decision Boundary of Continuous Piece-Wise Linear Deep Networks</h2></li></ul><h3 id="Deep-Networks-as-Continuous-Piece-Wise-Linear-Operators"><a href="#Deep-Networks-as-Continuous-Piece-Wise-Linear-Operators" class="headerlink" title="Deep Networks as Continuous Piece-Wise Linear Operators"></a>Deep Networks as Continuous Piece-Wise Linear Operators</h3><p>非线性函数最基本的函数形式之一来自于多项式，尤其是样条运算符。一般来说，样条是一种映射，在输入空间分区Ω的每个区域ω上具有局部度为D的多项式，并且有额外的约束条件，即这些多项式的前D-1阶导数在整个定义域上连续，也就是在从一个区域到其任何相邻区域时施加了平滑性约束。更正式地说，在深度网络的背景下，我们将特别关注仿射样条，即具有D = 1且仅约束在整个定义域上保持连续性的样条运算符。</p><p><strong>引理一</strong>：深度网络的第1层到$l$层的复合,记为$S^l$，其输出空间为$\mathbb R^l$，可以表示为：</p><script type="math/tex; mode=display">S^{\ell}(x)=\sum_{\omega\in\Omega}\left(A_{\omega}^{\ell}x+b_{\omega}^{\ell}\right)\mathbb{1}_{\{x\in\omega\}},</script><p>其中</p><script type="math/tex; mode=display">\begin{aligned}&A_{u}^{\ell}&& =\prod_{i=1}^{\ell}\operatorname{diag}\left(\boldsymbol{q}_{\omega}^{i}\right)\boldsymbol{W}^{i},\\&b_{\omega}^{\ell}=&& \mathrm{diag}\left(\boldsymbol{q}_{\omega}^{\ell}\right)\boldsymbol{b}^{\ell}+\sum_{i=1}^{\ell-1}\Bigg(\prod_{j=i+1}^{\ell}\mathrm{diag}\left(\boldsymbol{q}_{\omega}^{j}\right)\boldsymbol{W}^{j}\Bigg)\mathrm{diag}\left(\boldsymbol{q}_{\omega}^{i}\right)\boldsymbol{b}^{i}. \end{aligned}</script><p>其中，$q_\omega^l$是激活函数$\sigma$在预激活$W^lz^{l-1}+b^l$处的逐点导数。根据Balestriero等人的定理，$q_\omega^l$对于任意一个区域$\omega \in\Omega$是唯一的。<br>这种对DN的表达方式之前已经被用于使理论研究适用于实际DN而无需任何简化，同时利用了关于样条理论的丰富文献，例如逼近理论、最优控制、统计学及相关领域。通过这种方式，可以更好地理解和分析DN的性质，并将理论研究与实际应用相结合。</p><h3 id="Exact-Computation-of-Their-Partition-and-Decision-Boundary"><a href="#Exact-Computation-of-Their-Partition-and-Decision-Boundary" class="headerlink" title="Exact Computation of Their Partition and Decision Boundary"></a>Exact Computation of Their Partition and Decision Boundary</h3><p>假设$w_i^l$和$b_i^l$分别$W^l,b^l$的第i行。通过下面的引理2可以把第$l$层输入空间$\mathbb R^{l-1}$的一个超平面$h^l_i$反向映射到输入空间$R^S$中。$h_i^l$的定义如下：</p><script type="math/tex; mode=display">h_i^\ell\triangleq\{z\in\mathbb{R}^{\ell-1}:\langle w_i^\ell,z\rangle+b_i^\ell=0\}.</script><blockquote><p>为啥把一层的超平面定义成这样呢？我觉得是因为CPWL每一层的激活函数都在0处分界，故这个超平面就是这一层的分界平面。</p></blockquote><p><strong>引理2</strong>：给定一个超平面$h^l_i\in \mathbb R ^{l-1}$，这个超平面可以被映射到源域$\omega\in \Omega\in \mathbb R^S$中，as：</p><script type="math/tex; mode=display">proj_\omega(h_i^\ell)=\{\boldsymbol{x}\in\mathbb{R}^S:\langle\boldsymbol{w}_i^\ell,\boldsymbol{A}_\omega^{\ell-1}\boldsymbol{x}+\boldsymbol{b}_\omega^{\ell-1}\rangle+\boldsymbol{b}_i^\ell=0\}.</script><blockquote><p>其实把前$l-1$层整体看作是一个CPWL，把CPWL的表达式代入h的定义就可以了。</p></blockquote><p><strong>定理1</strong>：假设$S$是一个二分类的DN，因此其输出是一个单一的神经元。在$\mathbb R^{L-1}$空间中，它的决策边界是超平面$h_1^L$。$\mathbb R^S$中的决策平面是$\bigcup_{\omega\in \Omega}\{proj_\omega(h_1^L)\cap\omega\}$ 。</p><p>尽管如此，为了方便可视化，我们还是想在一个二维多面体$P \in R^S$上计算$\Omega$。</p><p><strong>SplineCam</strong>:记第$1 \to l$层组成的输入空间中的划分为$\Omega_l$。使用算法1，SplineCam首先利用超平面$h_i^1$把$P$分割为$\Omega_1$。接着，对于$\forall \omega\in \Omega_1$，使用引理1和引理2获得$proj_w(h_i^2)$。接着在每个区域$\omega$上使用算法1来获得下一步的划分$\Omega_2$。以此类推可以得到$\Omega_L$。</p><blockquote><p>算法1利用给定的边界和线来计算图中的点和边。算法2 利用算法1的结果计算图中的环，即$\omega$。</p></blockquote><p><img src="/2023/08/07/splinecam/a_1.png" alt="算法1"></p><p><img src="/2023/08/07/splinecam/a_2.png" alt="算法2"></p><h2 id="Visualizing-and-Understanding-Implicit-Neural-Representations"><a href="#Visualizing-and-Understanding-Implicit-Neural-Representations" class="headerlink" title="Visualizing and Understanding Implicit Neural Representations"></a>Visualizing and Understanding Implicit Neural Representations</h2><p>下面开始探讨深度网络的几何学，首先从隐式神经表示（INRs）入手。INRs通常是多层感知器（MLPs），经过训练能够将一维、二维或三维信号坐标映射为对应坐标处信号的强度。在3D视图合成和逆问题等应用中，它们被广泛使用。由于INRs具有低输入维度和基本函数的可解释性，因此它们是定性验证SplineCam的良好背景。目前，对于现有INR实践缺乏理论理解[42]。例如，虽然ReLU MLP主要用于NeRF[26]等最流行的INR应用，但当前的实践已经转向使用周期性激活来编码输入坐标，并随后使用ReLU MLP。在本节中，我们将研究周期性编码的影响，并可视化INR学习的区域几何结构。</p><h3 id="Decision-Boundary-of-Signed-Distance-Functions"><a href="#Decision-Boundary-of-Signed-Distance-Functions" class="headerlink" title="Decision Boundary of Signed Distance Functions"></a>Decision Boundary of Signed Distance Functions</h3><p>有符号距离函数（SDF）是一个隐式的连续表面或边界表示；SDF的输出是输入距离所表示边界的有符号距离。因此，SDF的零级集表示函数的表面或边界。将INR训练成有符号距离函数本质上是一个回归任务，模型通过拟合地面真实距离场来隐式地学习连续边界。我们训练了一个2D和一个3D的SDF，并使用我们的方法可视化了解析的零级集（类似于决策边界），并在图1和图5中提供了函数学习的样条划分。</p><blockquote><p>SDF 的概念可以参考这个<a href="https://zhuanlan.zhihu.com/p/536530019">知乎专栏</a>。属于2d或者3d模型的隐式存储方式。这篇论文使用神经网络来做SDF，具体来说就是使用神经网络记住空间中每个点与模型表面的距离。如果这个点在模型的内部，则这个距离为负数，如果点在外部，则这个距离为正数。如果点正好的模型的表面，则距离为0。</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 样条理论 </tag>
            
            <tag> SplineCam </tag>
            
            <tag> 可解释性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Spline Theory of Deep Learning 阅读笔记</title>
      <link href="/2023/07/22/a-spline-theory-of-deep-learning/"/>
      <url>/2023/07/22/a-spline-theory-of-deep-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="A-Spline-Theory-of-Deep-Learning"><a href="#A-Spline-Theory-of-Deep-Learning" class="headerlink" title="A Spline Theory of Deep Learning"></a>A Spline Theory of Deep Learning</h1><h2 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h2><ul><li><strong>Type</strong>: ConferencePaper</li><li><strong>Title</strong>: A Spline Theory of Deep Learning, </li><li><strong>Author</strong>: Balestriero, Randall; baraniuk,, </li><li><strong>Year</strong>: 2018 ;<ul><li><strong>Pages</strong>: 374-383</li><li><strong>Publisher</strong>: PMLR,</li></ul></li></ul><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.</p><p>我们通过样条函数和运算符在深度网络（DNs）和逼近理论之间建立了严谨的桥梁。我们的关键结果是，大部分的深度网络可以被写成最大仿射样条运算符（MASOs）的组合形式，这为我们观察和分析其内部工作提供了强大的途径。例如，对于给定的输入信号，MASO DN的输出可以被表示为输入的简单仿射变换。这意味着DN构建了一组依赖于信号的、类别特定的模板，通过简单的内积与信号进行比较；我们探索了与经典的通过匹配滤波器进行最优分类理论的联系以及数据记忆的影响。进一步地，我们提出了一个简单的惩罚项，可以添加到任何DN学习算法的损失函数中，强制模板彼此正交；这将显著提高分类性能，并减少过拟合，而无需改变DN的架构。输入信号空间的样条分区为我们研究DN如何以分层方式组织信号提供了一条新的几何途径。作为一个应用，我们开发并验证了一种用于信号的新的距离度量，用于量化它们分区编码之间的差异。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>深度学习极大地提升了我们解决各种困难的机器学习和信号处理问题的能力。当今的机器学习领域被深度（神经）网络（DNs）所主导，它们由大量简单参数化的线性和非线性变换组成。最近普遍的情况是将深度网络作为黑盒子插入应用程序中，在大量训练数据上进行训练，然后在性能上显著超越传统方法。</p><p>尽管取得了实证方面的进展，但深度学习能够如此出色地工作的确切机制仍然相对不太清楚，给整个领域增添了一丝神秘感。目前对建立严格的数学框架的努力大致可分为五个阵营：(i) 探测和测量深度网络以可视化其内部工作机制 (Zeiler &amp; Fergus, 2014)；(ii) 分析深度网络的性质，如表达能力 (Cohen et al., 2016)、损失曲面几何 (Lu &amp; Kawaguchi, 2017; Soudry &amp; Hoffer, 2017)、干扰管理 (Soatto &amp; Chiuso, 2016)、稀疏化 (Papyan et al., 2017) 和泛化能力；(iii) 新的数学框架，与深度网络有一些（但不是全部）共同特征 (Bruna &amp; Mallat, 2013)；(iv) 可以导出特定深度网络的概率生成模型 (Arora et al., 2013; Patel et al., 2016)；以及 (v) 信息理论界限 (Tishby &amp; Zaslavsky, 2015)。</p><p>在本文中，我们通过样条函数和运算符在深度网络（DNs）和逼近理论之间建立了严谨的桥梁。我们证明了一大类DNs，包括卷积神经网络（CNNs）（LeCun，1998）、残差网络（ResNets）（He等，2016；Targ等，2016）、跳跃连接网络（Srivastava等，2015）、全连接网络（Pal＆Mitra，1992）、循环神经网络（RNNs）（Graves，2013）等，可以被写成样条运算符的形式。特别是当这些网络采用当前的标准实践分段仿射凸非线性（例如ReLU，最大池化等）时，它们可以被写成最大仿射样条运算符（MASOs）的组合形式（Magnani＆Boyd，2009；Hannah＆Dunson，2013）。我们在这里重点关注这样的非线性函数，但请注意，我们的框架也适用于非分段仿射非线性函数，通过标准的逼近论证可以实现。</p><p>最大仿射样条连接为使用逼近理论和函数分析工具来观察和分析深度网络内部工作提供了一个强大的途径。以下是我们的主要贡献的总结：</p><p>a) 我们证明了大部分深度网络可以被写成最大仿射样条运算符（MASOs）的组合形式，由此可立即得出结论：在给定输入信号的条件下，深度网络的输出是输入的简单仿射变换。在第4节中，我们通过推导卷积神经网络（CNN）的输入/输出映射的闭式表达式来进行说明。<br>b) 仿射映射公式使我们能够将MASO深度网络解释为构建了一组依赖于信号的、类别特定的模板，通过简单的内积与信号进行比较。在第5节中，我们将深度网络与经典的通过匹配滤波器进行最优分类理论直接相关联，并提供了关于数据记忆效应的见解（Zhang等，2016）。<br>c) 我们提出了一个简单的惩罚项，可以添加到任何深度网络学习算法的损失函数中，以强制模板彼此正交。在第6节中，我们展示了这将显著提高在标准测试数据集（如CIFAR100）上的分类性能，并减少过拟合，而无需对深度网络的架构进行任何改变。<br>d) MASO所引发的输入空间的分区将深度网络与矢量量化（VQ）和K均值聚类理论联系起来，为研究深度网络如何以分层方式对信号进行聚类和组织提供了一条新的几何途径。第7节研究了MASO分区的性质。<br>e) 利用事实：如果两个信号位于相同的MASO分区区域中，那么深度网络将他们视为相似的。我们在第7.3节中开发了一种新的信号距离，用于衡量它们分区编码之间的差异。该距离可以通过反向传播轻松计算。</p><p>补充材料（SM）中的一些附录包含了数学设置和证明。关于这些内容的大幅扩展及众多新结果的详细说明可在（Balestriero＆Baraniuk，2018）中找到。</p><h2 id="Background-Problem-Statement"><a href="#Background-Problem-Statement" class="headerlink" title="Background / Problem Statement"></a>Background / Problem Statement</h2><h3 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h3><p>深度神经网络是一种算子，该算子将输入的信号$x\in \mathbb R^D$映射到预测值$\hat{y} \in \mathbb R^C$ ，即$f_{\Theta}:\mathbb R^D \to \mathbb R^C$。所有的神经网络可以写作是L个中间映射的组合：</p><script type="math/tex; mode=display">f_{\Theta}=(f_{\theta^{(1)}}^{(L)}\circ\cdots\circ f_{\theta^{(1)}}^{(1)}),</script><p>其中$\Theta=\left\{\theta^{(1)},\ldots,\theta^{(L)}\right\}$是每一层的网络参数集合。整体上来说，这种映射的组合是非线性且不可交换的。</p><h3 id="样条算子基础"><a href="#样条算子基础" class="headerlink" title="样条算子基础"></a>样条算子基础</h3><p>逼近论是研究如何以及多好的函数可以最好地用更简单的函数来逼近的理论。这篇文章主要讨论仿射样条（线性样条），即每一段都用线性函数去拟合。</p><blockquote><p>举例：样条函数<br>在逼近理论中，样条函数是一种使用分段的多项式函数来逼近复杂函数的工具。比如下图的$y=x^2$曲线，我们可以把x轴分割为三段，每一段分别用AG、AB、BH三条直线去拟合。下面所示的样条函数有两类需要优化的参数：1. 分段位置  2. 每一段分别用什么函数去拟合。<br><img src="/2023/07/22/a-spline-theory-of-deep-learning/20230730155321.png" alt="img"></p></blockquote><p><strong>多元仿射样条</strong>(Multivariate Affine Splines)：考虑对域$\mathbb R^D$的一个分割$\Omega=\{\omega_1,\dots,\omega_R\}$，和一系列的本地映射$\Phi=\{\phi_{1},\ldots,\phi_{R}\}$。本地映射和子域一一对应，并将子域中的点$x\in \omega_r$映射到$\mathbb R$。形式上，$\phi_r(\boldsymbol{x}):=\langle[\alpha]_{r,\cdot},\boldsymbol{x}\rangle+[\beta]_r$。其中$\alpha\in \mathbb R^{R\times D},\beta \in \mathbb R^R$。$[\alpha]_r$表示由$\alpha$的第r行组成的列向量。在此设定下，多元仿射样条的定义如下:</p><script type="math/tex; mode=display">\begin{gathered}s[\alpha,\beta,\Omega](x) =\sum_{r=1}^R\left(\langle[\alpha]_{r,\cdot},x\rangle+[\beta]_r\right)\mathbf{1}(x\in\omega_r) \\=:\langle\alpha[x],\boldsymbol{x}\rangle+\beta[\boldsymbol{x}], \end{gathered}</script><p>$\mathbf{1}(x\in \omega_r)$是一个指示函数，当满足括号里的条件时其值为1，否则为0。这样定义的样条是分段仿射且分段凸的。只有在R=1时，其为全局仿射和全局凸。我们称这种情况下的多元放射样条为退化样条。</p><p><strong>最大仿射样条函数</strong>(Max-Affine Spline Functions)：用仿射函数进行逼近的主要挑战是我们需要同时优化样条参数$\alpha,\beta$和输入域的分割$\Omega$。但是，如果我们限制仿射样条为全局凸的，则这个仿射样条可以被写为最大仿射样条：</p><script type="math/tex; mode=display">s[\alpha,\beta,\Omega](\boldsymbol{x})=\max_{r=1,\ldots,R}\langle[\alpha]_r,.,\boldsymbol{x}\rangle+[\beta]_r.</script><p>这种样条的一个非常有用的特点是它完全由它的参数$\alpha,\beta$决定，而不需要指定划分$\Omega$。</p><blockquote><p>举例：最大仿射样条<br>还是上面的图。如果我们对仿射样条加以限制，使其为全局凸的，假设直线AG为$y=a_1x+b_1$，直线AB为$y=a_2x+b_2$，直线BH为$y=a_3x+b_3$ ，则该仿射样条可以写作：$y=MAX\{y=a_1x+b_1,y=a_2x+b_2,y=a_3x+b_3\}$。这种情况下，对参数$a_i,b_i$的改变同时会影响到分割$\Omega$。故只需要对$\alpha,\beta$进行优化。</p></blockquote><p><strong>最大仿射样条算子</strong>(Max-Affine Spline Operators)：是最大仿射样条函数的一种拓展，他会产生多元的输出。这种算子由K个最大仿射样条函数的拼接得到。一个由参数$A\in\mathbb R^{K\times R\times D}, B\in\mathbb R^{K\times R}$定义的MASO可以表示为：</p><script type="math/tex; mode=display">\begin{aligned}S[A,B](x)& =\begin{bmatrix}\max_{r=1,...,R}\langle[A]_{1,r,.},\boldsymbol{x}\rangle+[B]_{1,r}\\\vdots\\\max_{r=1,...,R}\langle[A]_{K,r,.},\boldsymbol{x}\rangle+[B]_{K,r}\end{bmatrix}  \\&=:A[\boldsymbol{x}]\boldsymbol{x}+B[\boldsymbol{x}].\end{aligned}</script><p>最大仿射样条函数和算子关于每个输出维度始终是分段仿射和全局凸的(因此也是连续的)。反之，任何分段仿射和全局凸函数/算子都可以写成一个极大仿射样条。此外，利用标准的逼近论点，很容易证明一个MASO可以任意逼近任意在每个输出维数上是凸的(非线性)算子。 </p><h3 id="深度神经网络是仿射算子的组合"><a href="#深度神经网络是仿射算子的组合" class="headerlink" title="深度神经网络是仿射算子的组合"></a>深度神经网络是仿射算子的组合</h3><p>虽然MASO仅适用于逼近凸函数/算子，但我们现在展示了几乎所有现今的深度网络(DNs)都可以被写成MASO的复合形式，每一层都对应一个MASO。这样的复合在一般情况下是非凸的，因此可以逼近更广泛的函数/算子类别。有趣的是，在某些广泛条件下，这种复合仍然是分段仿射样条算子，从而为深度网络(DNs)提供了各种洞察力。</p><h4 id="深度神经网络的算子是MASO"><a href="#深度神经网络的算子是MASO" class="headerlink" title="深度神经网络的算子是MASO"></a>深度神经网络的算子是MASO</h4><p><strong>命题一</strong>：任意一个全连接算子 $f^{(l)}_W$是仿射算子，因此也是一个退化的MASO $S\Big[A_{\boldsymbol{W}}^{(\ell)},B_{\boldsymbol{W}}^{(\ell)}\Big],R=1.[A_{\boldsymbol{W}}^{(\ell)}]_{k,1,}.=\left[W^{(\ell)}\right]_{k},[B_{\boldsymbol{W}}^{(\ell)}]_{k,1}={\left[b_{\boldsymbol{W}}^{(\ell)}\right]}_{k}$。卷积层同理。</p><p><strong>命题二</strong>：任意一个满足分段仿射且凸的激活函数是一个MASO  $S\Big[A_\sigma^{(\ell)},B_\sigma^{(\ell)}\Big],R=2$。$\left[B_{\sigma}^{(\ell)}\right]_{k,1}=\left[B_{\sigma}^{(\ell)}\right]_{k,2}=0 \quad \forall k$.</p><p>对于ReLU：$\left[A_{\sigma}^{(\ell)}\right]_{k,1,\cdot}=0,\left[A_{\sigma}^{(\ell)}\right]_{k,2,\cdot}=e_k \quad \forall k$；</p><p>对于leaky ReLU：$\left[A_{\sigma}^{(\ell)}\right]_{k,1,\cdot}=\nu\boldsymbol{e}_{k},\left[A_{\sigma}^{(\ell)}\right]_{k,2,\cdot}=e_k \quad \forall k,v&gt;0$；</p><p>对于绝对值函数：$\left[A_{\sigma}^{(\ell)}\right]_{k,1,\cdot}=-\boldsymbol{e}_{k},\left[A_{\sigma}^{(\ell)}\right]_{k,2,\cdot}=e_{k}\quad\forall k$</p><p>其中 $e_{k}$ 代表 $\mathbb R^{D^{(l)}}$ 第k个正交基向量。</p><p><strong>命题三</strong>：任意一个满足分段仿射和凸的池化层都是一个MASO。</p><p>对于最大池化层， $R=\mathcal{R}_{k}$ (通常在所有输出维度上为常数).$\left[A_{\rho}^{(\ell)}\right]_{k,\cdot,\cdot}=\{e_{i},i\in\mathcal{R}_{k}\},\left[B_{\rho}^{(\ell)}\right]_{k,r}=0\forall k,r$。</p><p>平均池化层是一个退化的MASO(R=1), $\left[A_\rho^{(\ell)}\right]_{k,1,\cdot}=\frac1{(\mathcal{R}_k)}\sum_{i\in\mathcal{R}_k}e_i,\begin{bmatrix}B_{\rho}^{(\ell)}\end{bmatrix}_{k,1}=0\forall k.$</p><p><strong>命题四</strong>：由全连接/卷积算子任意组合构造的DN接上一个激活或池化算子的网络是一个MASO $S[A^{(\ell)},B^{(\ell)}]$,表示为：</p><script type="math/tex; mode=display">f^{(\ell)}(z^{(\ell-1)}(x))=A^{(\ell)}[x]z^{(\ell-1)}(x)+B^{(\ell)}[x].</script><p>因此，许多深度网络(DNs)都可以归结为MASO的复合形式。论文在附录中对CNN、ResNet、跳跃连接网络、全连接网络和RNN等进行了证明。</p><p><strong>定理一</strong>：由1至命题3中的任意全连接/卷积、激活和池化算子组成的深度网络（DN），是一个MASO的复合形式，等价于一个全局仿射样条算子。</p><p>需要注意的是，尽管定理1中所述的每个深度网络（DN）的层都是MASO，但多个层的复合形式不一定是MASO。事实上，MASO的复合仅在其所有组成算子（除了第一个算子）相对于它们各自的输出维度都是非减的情况下才仍然是MASO（Boyd＆Vandenberghe，2004）。有趣的是，ReLU和最大池化都是非减的，而leaky ReLU是严格递增的。导致复合层非凸性的罪魁祸首是全连接或卷积算子中的负项，这破坏了所需的非增性质。当这些罪魁祸首被排除时，DN就成为一个有趣的特例，因为它相对于其输入是凸的（Amos等人，2016），并且相对于其参数是多凸的（Xu＆Yin，2013）。<br>定理二：一个深度神经网络，它的第$2,\dots,L$层由具有非负权重（即$\boldsymbol{W}_{k,j}^{(\ell)}\geq0,\boldsymbol{C}_{k,j}^{(\ell)}\geq0;$）、非减、分段仿射和凸的全连接和卷积算子的任意组合构成，则这个网络是全局MASO，因此关于其每个输出维度也是全局凸的。</p><p>上述结果涉及使用凸的仿射算子的深度网络（DNs）。其他流行的非凸DN算子（例如sigmoid和arctan激活函数）可以被仿射样条算子任意接近逼近，但不能被MASO逼近。</p><p><strong>DNs是信号相关的仿射变换</strong>。上述结果的一个共同主题是，对于由命题1至命题3中的全连接/卷积、激活和池化算子构建的深度网络（DNs），算子/层的输出$z^{(l)}(x)$总是输入x的一个信号相关的仿射函数。应用到x的特定仿射映射取决于它在RD中哪个样条分区中。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 样条理论 </tag>
            
            <tag> spline theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feature Importance Estimation with Self-Attention Networks</title>
      <link href="/2022/03/15/feature-importance-estimation-with-self-attention-networks/"/>
      <url>/2022/03/15/feature-importance-estimation-with-self-attention-networks/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="《Feature-Importance-Estimation-with-Self-Attention-Networks》-研读报告"><a href="#《Feature-Importance-Estimation-with-Self-Attention-Networks》-研读报告" class="headerlink" title="《Feature Importance Estimation with Self-Attention Networks》 研读报告"></a>《Feature Importance Estimation with Self-Attention Networks》 研读报告</h1><blockquote><p>摘要：黑盒神经网络模型被广泛应用于工业界和科学研究，但目前还很难以去理解和翻译。最近，注意力机制的提出为神经语言模型的内部工作原理提供了insight。本文探讨了使用基于注意力的神经网络机制来估计特征重要性，作为解释从命题（表格）数据中学习的模型的手段。由提议的自注意力网络 (SAN) 架构评估的特征重要性估计值与已建立的基于 ReliefF、互信息和随机森林的估计值进行比较，这些估计值在实践中广泛用于模型解释。我们首次在 10 个真实和合成数据集上跨算法对特征重要性估计进行无标度比较，以研究所得特征重要性估计的异同，表明 SAN 识别出与其他方法相似的高等级特征。我们证明 SAN 识别特征交互，在某些情况下，这些交互产生比基线更好的预测性能，这表明注意力超出了几个关键特征的交互，并检测到与所考虑的学习任务相关的更大的特征子集。</p></blockquote><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>这篇文章提出了自注意力网络（Self-Attention Networks ，即SANS)的概念，并且探索了这种网络学习到的“表示”是否可以用于重要性估计。主要贡献如下：</p><ol><li>提出SAN，这是一种可以直接获得特征重要程度的神经网络。</li><li>对比ReliefF, Mutual Information 和Genie3等feature rank算法进行可拓展的经验评估。</li><li>针对特征重要度估计机型直接比较，突出考虑的算法输出之间的相似性</li><li>对SAN的属性进行理论研究，包括空间复杂度和时间复杂度。</li></ol><h3 id="Self-Attention-Networks"><a href="#Self-Attention-Networks" class="headerlink" title="Self-Attention Networks"></a>Self-Attention Networks</h3><p>本文实现了注意力机制的神经网络可以表示为：</p><script type="math/tex; mode=display">l_2=\sigma(W_2\cdot(a(W_{|F|}\cdot \Omega(X)+b_{l_1}))+b_{l_2})</script><p>图示如下：</p><p><img src="/2022/03/15/feature-importance-estimation-with-self-attention-networks/image-20220315203059280.png" alt="论文实现的神经网络结构"></p><p>其中：</p><script type="math/tex; mode=display">\Omega(x)=\frac{1}{k}\bigoplus_k[X\bigotimes softmax(W_{l_{att}}^kX+b_{l_{att}}^k)]</script><script type="math/tex; mode=display">a(X)= \mathrm{SELU}(x)=\lambda\begin{cases}x&\text{if}x>0\\\alpha(\exp(x)-1)&\text{if}x\leq0\end{cases}.</script><p>k表示注意力头的个数。$\oplus$表示Hadamard summation，$\otimes$表示Hadamard product。如下：</p><script type="math/tex; mode=display">\begin{bmatrix}a_{11}&a_{12}&\dots&a_{1n}\\a_{21}&a_{22}&\dots&a_{2n}\\\vdots&\vdots&\vdots&\vdots\\a_{m1}&a_{m2}&\dots&a_{mn}\end{bmatrix}\bigotimes\begin{bmatrix}b_{11}&b_{12}&\dots&b_{1n}\\b_{21}&b_{22}&\dots&b_{2n}\\\vdots&\vdots&\vdots&\vdots\\b_{m1}&b_{m2}&\dots&b_{mn}\end{bmatrix}=\begin{bmatrix}a_{11}×b_{11} & a_{12}×b_{12}&\dots&a_{1n}×b_{1n}\\a_{21}×b_{21}&a_{22}×b_{22}&\dots&a_{2n}×b_{2n}\\\vdots&\vdots&\vdots&\vdots\\a_{m1}×b_{m1}&a_{m2}×b_{m2}&\dots&a_{mn}×b_{mn}\end{bmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix}a_{11}&a_{12}&\dots&a_{1n}\\a_{21}&a_{22}&\dots&a_{2n}\\\vdots&\vdots&\vdots&\vdots\\a_{m1}&a_{m2}&\dots&a_{mn}\end{bmatrix}\bigoplus\begin{bmatrix}b_{11}&b_{12}&\dots&b_{1n}\\b_{21}&b_{22}&\dots&b_{2n}\\\vdots&\vdots&\vdots&\vdots\\b_{m1}&b_{m2}&\dots&b_{mn}\end{bmatrix}=\begin{bmatrix}a_{11}+b_{11} & a_{12}+b_{12}&\dots&a_{1n}+b_{1n}\\a_{21}+b_{21}&a_{22}+b_{22}&\dots&a_{2n}+b_{2n}\\\vdots&\vdots&\vdots&\vdots\\a_{m1}+b_{m1}&a_{m2}+b_{m2}&\dots&a_{mn}+b_{mn}\end{bmatrix}</script><p>这篇论文中使用的多头注意力与Transformer的多头注意力机制是不一样的。Transformer使用的多头注意力机制是将Q、K、V映射到更低的维度计算注意力，重复h次后将得到的所有注意力向量拼接到一起。而该论文使用的多头注意力是将所有的注意力向量相加做平均。</p><h4 id="Computing-feature-importance-with-SANs"><a href="#Computing-feature-importance-with-SANs" class="headerlink" title="Computing feature importance with SANs"></a>Computing feature importance with SANs</h4><p>下面展示如何利用上述结构获得特征的重要性权重。</p><ol><li><strong>Instance-level  aggregations (attention)</strong>：令$\{(x_i,y_i),1\le i \le n\}$为样本集合，令$SAN(x_i)$表示第i个样本对应的注意力权重。</li></ol><script type="math/tex; mode=display">SAN(x_i)=\frac{1}{k}\bigoplus_k[softmax(w_{l_{att}}^kx_i+b_{l_{att}}^k)]</script><p>获得注意力的第一个选择就是将每个样本的注意力做均值：</p><script type="math/tex; mode=display">R_I=\frac{1}{n}\sum_{i=1}^nSAN(x_i)</script><ol><li><strong>Counting only correctly predicted instances (attentionPositive)</strong>:第二种变体基于如下的假设：只考虑被正确预测的样本。</li></ol><script type="math/tex; mode=display">R_I^c=\frac{1}{n}\sum_{i=1}^nSAN(x_i)[\hat{y}_i=y_i]</script><ol><li><strong>Global attention layer (attentionGlobal)</strong>: 前面的两种方式通过累加注意力向量来获得全局的特征重要性。但是基于权重向量包含特征重要性信息的假设，可以在训练结束后直接获得全局的注意力权重。</li></ol><script type="math/tex; mode=display">R_G=\frac{1}{k}\bigoplus_k[softmax(diag(w_{l_{att}}^k))];w_{l_{att}}^k\in \mathbb{R}^{|F|×|F|}</script>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 特征提取 </tag>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>testpass</title>
      <link href="/2021/11/15/testpass/"/>
      <url>/2021/11/15/testpass/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div id="aplayer-MaVKTgpt" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-MaVKTgpt"),            narrow: false,            autoplay: false,            showlrc: false,            music: {              title: "test",              author: "test",              url: "test.mp3",              pic: "",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link href="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/"/>
      <url>/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Evaluating-Differentially-Private-Machine-Learning-in-Practice"><a href="#Evaluating-Differentially-Private-Machine-Learning-in-Practice" class="headerlink" title="Evaluating Differentially Private Machine Learning in Practice"></a>Evaluating Differentially Private Machine Learning in Practice</h1><p>Bargav Jayaraman and David Evans, University of Virginia  </p><blockquote><p>摘要：差分隐私可以用来计算一个机制泄露了多少隐私(用$\epsilon$表示)。当用于机器学习时，差分隐私的目标即为限制模型泄露的训练集中单个个体的隐私量。但是目前人们对于如何校准$\epsilon$还不是很了解。在机器学习中人们通常会设定一个很大的$\epsilon$值来获得更好的可用性，但对这种选择对隐私所造成的影响所知甚少。此外，在使用迭代学习程序的情况下，经常使用宽松的差分隐私定义，这似乎减少了所需的隐私预算，但人们对隐私性和可用性之间的平衡理解并不深刻。<strong>在这篇文章中</strong>，我们在逻辑回归和神经网络模型的实验中量化了这些选择对隐私的影响。我们的主要的发现是获取隐私是需要代价的——宽松的差分隐私定义减少了所需的噪声数量但同时增大了隐私泄露的风险。现有的差分隐私机器学习机制很少会对复杂学习任务做可接受的可用性与隐私性之间的平衡：降低准确率损失会降低隐私性，提供强隐私保障的会产生无用的模型。</p></blockquote><p><strong>主要内容：通过实验验证不同的$\epsilon$值和使用不同的宽松的差分隐私定义对模型可用性与隐私性的影响。</strong></p><h3 id="几种宽松的差分隐私定义"><a href="#几种宽松的差分隐私定义" class="headerlink" title="几种宽松的差分隐私定义"></a>几种宽松的差分隐私定义</h3><p>主要思想：多种差分隐私机制组合起来时其整体的privacy budget（暂且称为隐私代价）不一定为各个privacy budget之和。可以通过一些其他的规律来获得一个更紧的上界。</p><ol><li>advanced composition theorem ：考虑到隐私损失的期望，可以获得$\epsilon$的一个更紧的上界。</li><li>Concentrated Differential Privacy (CDP)  ：$\mathcal{D}_{subG}(\mathcal{M}(D)||\mathcal(D’))\le(\mu,\tau)$。任何$\epsilon-DP$算法都满足$(\epsilon \cdot(e^{\epsilon}-1)/2,\epsilon)-CDP$ ，反过来不一定满足。</li><li>Zero-Concentrated Differential Privacy (zCDP)  ：$\mathcal{D}_{\alpha}(\mathcal{M}(D)||\mathcal(D’))\le \xi + \rho a$。如果$\mathcal{M}$满足$\epsilon-DP$，则它满足$(\frac{1}{2}\epsilon^2)-zCDP$。如果它满足$\rho-zCDP$，则对任意$\delta&gt;0$它满足$(\rho+2\sqrt{\rho log(1/\delta),\delta})-DP$</li><li>Rényi Differential Privacy (RDP)  :$\mathcal{D}_{\alpha}(\mathcal{M}(D)||\mathcal{M}(D’))\le \epsilon$。如果$\mathcal{M}$满足$(\alpha,\epsilon)-RDP$，则对于任意$0&lt;\delta&lt;1,$满足$(\epsilon+\frac{log(1/\delta)}{\alpha-1},\delta)-DP$</li></ol><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211107162132017.png" alt="img"></p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><h4 id="实验设定"><a href="#实验设定" class="headerlink" title="实验设定"></a>实验设定</h4><ol><li>使用逻辑回归模型（凸优化）和神经网络模型（非凸）作为目标模型。</li><li>使用成员推断和属性推断作为攻击方式，从而获得模型的隐私损失。</li><li>在梯度上添加扰动。</li><li>使用具有两个隐藏层的神经网络作为推断（攻击）网络</li></ol><h4 id="攻击方式"><a href="#攻击方式" class="headerlink" title="攻击方式"></a>攻击方式</h4><p>成员推断使用Shokri（1）和Yeom（2）的方法。</p><ol><li>黑盒攻击。攻击者可以获得目标模型对输入的置信度（confidence score）。使用相同分布下采样的数据训练了多个影子模型。使用这些影子模型训练推断模型。推断模型的训练集来源于部分用来训练影子模型的训练数据和一些相同分布下随机采样的数据。推断模型的输入还包括影子模型对这些数据的置信度。</li><li>白盒攻击。假设攻击者可以访问训练集在目标模型上的平均损失。如果输入数据在目标模型上的损失值小于均值的话就认为这个输入在目标模型的训练集里。</li></ol><p>属性推断：使用Yeom的方法。与上2类似。暴力搜索隐私属性的所有可能值，选择与平均损失最接近的组合。</p><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>选择两个数据集</p><ol><li>CIFAR-100  （28×28 images）。使用PCA把维度压缩到50</li><li>Purchase-100  </li></ol><p>每个数据集中分别随机选10,000 个作为训练集，10000个作为测试集。剩下的用来训练影子模型和推断模型。</p><p>在进行属性推断攻击时，因为原数据集没有标注哪些属性为隐私属性，所以随机选取5个属性作为隐私属性。</p><h4 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h4><ol><li>accuracy loss：没有隐私保护的模型在测试集上的准确率（baseline） - 有隐私保护的模型的准确率<ol><li>privacy leakage：真阳率（True Positive Rate) - 假阳率（False Positive Rate)。如果是0的话代表没有隐私泄露。</li></ol></li></ol><h4 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h4><p>训练模型时使用$l_2$正则化。首先使用grid search训练一个非隐私模型，从而找到一个最大化测试集正确率的$\lambda$。</p><p>对于CIFAR-100 </p><ol><li>逻辑回归：$\lambda = 10^{-5}$</li><li>神经网络：$\lambda = 10^{-4}$</li></ol><p>对于Purchase-100  </p><ol><li>逻辑回归：$\lambda = 10^{-5}$</li><li>神经网络：$\lambda = 10^{-8}$</li></ol><p>然后用上面的设定训练隐私模型。$\epsilon$取值范围为$0.01-1000$，$\delta$固定为$10^{-5}$(假设训练集大小为n, $\delta\lt\frac{1}{n}$)</p><p>使用ADAM优化器，固定学习率为0.01.</p><p>batch size = 200</p><h4 id="Clipping"><a href="#Clipping" class="headerlink" title="Clipping"></a>Clipping</h4><p>使用Tensorflow Privacy框架实现了batch clipping 和 per-instance clipping。阈值$\mathcal{C}=1$。通过下图的比较结果发现Per-instance clipping放大了不同机制之间的差异。因此后面的实验中只使用这一种clipping方式。</p><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211107204703476.png" alt="image-20211107204703476"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h4><h5 id="CIFAR-100"><a href="#CIFAR-100" class="headerlink" title="CIFAR-100"></a>CIFAR-100</h5><p>baseline在训练集上的准确率是0.225，测试集上的准确率是0.155。中间的gap值为0.07。上图（b）显示了不同$\epsilon$值，不同差分隐私定义对准确率损失的影响。</p><p>下图（a) （b)展示成员推断攻击下的隐私损失，（c）展示了属性推断攻击下的隐私损失。</p><p>通过(a)看出对于Naive composition来说，当$\epsilon\le10$时隐私损失基本为0，当$\epsilon=1000$隐私泄露达到了$0.065\pm0.004$，同时RDP和zCDP的损失之达到了$0.08\pm0.004$。通过图(b)可以看出naive-composition在$\epsilon\le 10$时没有明显的隐私损失。但是当$\epsilon=1000$时损失快速上升到$0.093\pm0.002  $。</p><p>图中还显示了$\epsilon-differential\quad privacy$的理论损失上界：$e^{\epsilon}-1$   [Yeom et al]。</p><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211107211234195.png" alt="image-20211107211234195"></p><p>下面的图表展示了不同的差分隐私定义下泄露给敌手的训练集成员数量。</p><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211107213708468.png" alt="image-20211107213708468"></p><p>实验结论后面还有好多，与上面的大同小异，不一一展示了。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>获取隐私是有代价的（that there is no way to obtain privacy for free  ）</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/bargavj/EvaluatingDPML">https://github.com/bargavj/EvaluatingDPML</a>  </p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.usenix.org/conference/usenixsecurity19/presentation/jayaraman">原文链接</a></p><h3 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h3><ol><li><p>实验方法</p></li><li><p>在机器学习中应用差分隐私</p></li></ol><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211108121014599.png" alt="image-20211108121014599"></p><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211108121412999.png" alt="image-20211108121412999"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>An Attentive Survey of Attention Models(2021)研读报告</title>
      <link href="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/"/>
      <url>/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="An-Attentive-Survey-of-Attention-Models-2021-研读报告"><a href="#An-Attentive-Survey-of-Attention-Models-2021-研读报告" class="headerlink" title="An Attentive Survey of Attention Models(2021)研读报告"></a>An Attentive Survey of Attention Models(2021)研读报告</h1><blockquote><p>论文标题：关于注意力模型的细心调研<br>如今注意力模型已经成为了神经网络中一个非常重要的概念，而且在多种应用领域中得到了广泛的研究。这篇调研针对注意力机制的发展提供了一个结构化、综合的概述。我们将现有的注意力技术分类。我们回顾了一些具有注意力模型的神经结构，并且讨论一些注意力模型已经发挥显著作用的应用。我们还描述了注意力机制是如何用来提升神经网络的可解释性的。最后我们讨论了一些未来的注意力机制的研究方向。这篇调研可以针对注意力机制提供一个简洁的介绍，并在实践人员开发应用时提供引导。</p></blockquote><span id="more"></span><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><p>注意力模型（AM）最早用于机器翻译，并为神经网络模型带来很大的优势。AM作为大量NLP应用中的一个基本部件，它已经变得非常出名。注意力机制背后的直觉（intuition）可以用人类生物系统解释。举个例子，我们的视觉处理系统倾向于把注意点放在图像的某些部位，同时忽略其他不相关的信息。类似的，在一些关于语言、视觉、听觉的问题中，输入的某些部分比其它部分更加重要（如图像描述问题，相较于其他区域，图像的某一个特定区域可能对于生成描述句的下一个词更加重要）。注意力模型通过动态的调整模型的注意力从而让模型只注意对完成任务有用的部分输入。下面是[Yang et al. 2016] 做的关于注意力机制在情感分类问题中的应用的例子。AM学习到在这五句话中，第一句和第三句对情感分析更有帮助。</p><p><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_e1e9157e816c7a6c17f183442e37c95d.png" alt="img"></p><p>对注意力进行建模发展迅速的三个原因：</p><ol><li>是解决多任务最先进的模型。</li><li>为提升主任务性能提供了其它的优势。它被用于提升神将网络的可解释性（越来越多的人对影响人类生活的应用中的网络模型的公平性、问责制、透明度感兴趣）。</li><li>它们有助于克服递归神经网络RNN中的一些挑战，例如随着输入长度的增加性能下降，以及输入顺序不合理导致的计算效率低下。</li></ol><p>文章的组织：我们的工作致力于为注意力建模提供一个简短但是综合的研究。在第二节我们用一个简单的回归模型来为你提供关于注意力的初步印象。在第三节我们简短的介绍了[Bahdanau et al. 2015]提出的AM和其他的注意力函数（attention function）。第四节我们介绍了我们的分类结果。第五节和第六节讨论了使用AM的关键神经结构并展示了一些广泛使用注意力机制的应用。最后在第七节我们解释了注意力如何促进理解神经网络的可解释性，并在第八节介绍了未来的研究方向。</p><p>相关的调研：目前已经有一些专门领域的关于注意力机制的研究。如：</p><ol><li>[Wang and Tax 2016] on cv</li><li>[Lee et al. 2019] on graphs</li><li>[Galassi et al. 2020] on nlp</li></ol><h2 id="2-ATTENTION-BASICS"><a href="#2-ATTENTION-BASICS" class="headerlink" title="2 ATTENTION BASICS"></a>2 ATTENTION BASICS</h2><p>[Nadaraya 1964; Watson 1964]提出的回归模型可以帮助我们理解注意力机制。我们的数据样本有n个数据$\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}$。我们想知道当给定一个x时其对应的y的预测值$\hat{y}$。一个朴素的估计器对于任何输入都会输出样本中$y_i$的平均值。Naradaya-Watson提出了一个更好的方法：使用$y_i$的加权平均数。$y_i$的权值为$x,x_i$之间的相关度。公式如下：</p><script type="math/tex; mode=display">\hat{y} = \sum_{i=1}^{n}\alpha(x,x_i)y_i</script><p>$\alpha$是一个衡量$x,x_i$关联程度的函数。对于$\alpha$的一个普遍的选择是使用高斯核。Naradaya-Watson表示这种估计器有两个特性：</p><ol><li>一致性(consistency)：数据样本量越大结果越好。</li><li>朴素性(simplicity)：没有没用的参数。信息存储在样本数据中而不是在权重中。<br>往后推50年，深度模型中的注意力机制可以看作是上述公式的一种泛化，即权重函数是可以学习的。</li></ol><h2 id="3-ATTENTION-MODEL"><a href="#3-ATTENTION-MODEL" class="headerlink" title="3 ATTENTION MODEL"></a>3 ATTENTION MODEL</h2><p>AM的第一次应用被[Bahdanau et al. 2015]用来解决一个sequence-to-sequence modeling<br>task。s2s Model含有一个编码器(encoder)和一个解码器(decoder),他们的隐藏状态分别为$h_i,s_i$。编码器获得输入$\{x_1,x_2,\dots,x_T\}$并输出T个具有固定长度的向量$\{h_1,h_2,\dots,h_T\}$。将$h_T$输入解码器，解码器会输出$\{y_1,y_2,\dots,y_{T’}\}$token by token。<br><strong>上述模型有两个问题（挑战）：</strong></p><ol><li>将所有输入压缩成一个具有固定长度的向量会造成信息丢失。</li><li>无法对输入和输出序列之间的排列进行建模</li></ol><p>也就是说解码器在输出每个token时无法有选择的使用输入。</p><p><strong>核心思想</strong>：使用注意力权重来决定哪些输入对下一个输出更重要。</p><p><strong>注意力的使用方法</strong>：如下图(b)。在解码器的第j个时间步，模型结构中的注意力模块负责自动生成注意力权重$\alpha_{ij}$。用这个权重表示$s_{j-1},h_i$直间的关联度。接着这些权重会通过$c_j=\sum_{i=1}^{T}\alpha_{ij}h_i$生成contex vector c。c会输入到解码器的下一个时间步。通过生成c，解码器就可以获得整个输入信息同时只把”注意力”放在输入的某部分。这种方法提升了算法的性能和输出的质量。Table 1 是这种注意力机制的数学表达。<br><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_1455ea8a4e2ecf7e25bf47df00e2bac9.png" alt=""><br><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_7d99451e1b73905d753043d61336ddb0.png" alt=""></p><p><strong>学习注意力权重</strong>: 注意力权重的学习可以通过引入一个前馈神经网络来完成。这个前馈神经网络作为一个函数，它的的输入为$h_i,s_{j-1}$。这个函数叫做alignment funcion（table 1 中用a表示），它会对$h_i,s_{j-1}$的相关度进行评估（有的资料把这个函数叫做评分函数scoring function）并输出评分$e_{ij}$。distribution function（table 1中用p表示）会把$e_{ij}$转化为注意力权重。当a，p是可微的函数时，这整个基于注意力的encoder-decoder模型就会成为一个大的可微函数，并且可以一同训练。<br><strong>泛化的AM</strong>：AM可以看作是一种映射。这个映射会根据query q将keys K映射为权重$\alpha$。其中keys即为编码器的隐藏状态$\{h_1,h_2,\dots,h_T\}$，q即为$s_{j-1}$。注意力模型表示如下：</p><script type="math/tex; mode=display">A(q,K,V)=\sum_ip(a(k_i,q))*v_i</script><p>通常keys和values是一一对应的。在[Bahdanau et al. 2015]提出的模型中，$h_i = v_i = k_i$。通过上面的回归的例子来理解这个公式，输入x为query，$v_i$为$y_i$，$k_i$为$x_i$。</p><p><strong>Alignment functions（排列函数？）</strong>：<br>对于key和query在相同向量空间的：</p><ol><li>余弦相似度或者点积（dot product）</li><li>考虑到不同的表示长度，scaled dot product使用表示向量的长度来归一化点积。</li></ol><p>对于key和query在不同向量空间的：</p><ol><li>General alignment：引入可学习的转换矩阵将query映射到key的向量空间。</li><li>Activated general alignment：添加一个非线性的激活层（hyperbolic tangent, rectifier linear unit, or scaled exponential linear unit. ）</li><li>Biased general alignment ：不管query，通过添加偏置项（bias）直接学习一些key的全局重要性。</li><li>[Choromanski et al. 2021] 展示了key和query可以通过generalized kernel function匹配，而不是点积。<br>key，query联合表示的：</li><li>concat alignment：keys和queries拼接到一起形成联合表示。</li><li>Additive alignment：对key和query的贡献进行解耦。使得可以提前计算所有key的贡献而不用对每个query再计算一遍。降低了计算时间。</li><li>deep alignment 使用了多层神经网络。<br>针对特定使用场景的：</li><li>Location-based alignment：忽略keys的内容，只使用它的位置信息。</li><li>[Li et al. 2019a]提出当处理一组元素（如2-D patches for images 或者 1-D temporal<br>sequences），从属于该组的单个元素的表示中得出的特征（如平均数和标准差）可以作为alignment function的输入。</li></ol><p><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_617f45653ad3c44d2f61bd3ded2c020e.png" alt=""></p><p><strong>distribution function（分布函数）</strong>：</p><ol><li>dense distribution：softmax,logistic sigmoid。输出可以看作概率，但会降低模型的可解释性，而且会对不可能的输出分配概率（This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. ）。</li><li>sparse distribution：sparsemax和sparse entmax。通过只对少量可能的输出赋予概率从而产生sparse alignment。当大量元素之间是不相关时会非常有用。</li><li>[Tay et al. 2019]提出一种分布函数。计算两个项——$thanh(\frac{qk_i^T}{\sqrt{d_k}}),sigmoid(\frac{G(qk_i^T)}{\sqrt{d_k}})$的乘积来形成准注意力（qusi-attention）。其中第一项控制向量的加或者减，第二项可以看作是控制删除不相关元素的门函数。</li></ol><h2 id="4-TAXONOMY-OF-ATTENTION"><a href="#4-TAXONOMY-OF-ATTENTION" class="headerlink" title="4 TAXONOMY OF ATTENTION"></a>4 TAXONOMY OF ATTENTION</h2><p><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_7471d28d5ac5631a1e2d6b9048aab5ef.png" alt=""></p><h3 id="4-1-Number-of-sequences"><a href="#4-1-Number-of-sequences" class="headerlink" title="4.1 Number of sequences"></a>4.1 Number of sequences</h3><ol><li>distinctive attention：输入和输出是独立的两个序列。上面考虑的全是这种情况。大部分情况应用于翻译 [Bahdanau et al. 2015]、图像描述[Xu et al. 2015]和语音识别[Chan et al. 2016]。</li><li>co-attention：输入有多个序列，同时计算他们的注意力权重。[Lu et al. 2016]用它来做visual question answering。不仅提取图像中的重要信息a同样也要提取问题中的关键点b。a和b会互相影响。类似的[Yu et al. 2019] 也用它来做visual question answering task</li><li>self-attention(inner attention)。每个输入序列都可以学习自身中相关的元素。即key和query在同一个序列中。</li></ol><h3 id="4-2-Number-of-abstraction-levels"><a href="#4-2-Number-of-abstraction-levels" class="headerlink" title="4.2 Number of abstraction levels"></a>4.2 Number of abstraction levels</h3><ol><li>single-level：只对原始输入添加注意力权重。</li><li>multi-level：注意力模型的低层输出变为高层的输入（query）。再细分可以根据学习顺序分为自顶向下，自底向上两类。 [Yang et al. 2016]用“Hierarchical Attention Model”(HAM) 解决文档分类问题。这种多层模型可以从句子中找到关键词，从文档中找到关键句。上面提到的 [Lu et al. 2016]使用的co-attention也是多层模型（单词层，短语层，问题层），如下图。[Zhao and Zhang 2018]提出attention-via-attention，低层为字母高层为单词，并通过自顶向下的方式学习。<br><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_6149cd7ede3a72ba726fd358cba6b2f6.png" alt=""></li></ol><h3 id="4-3-Number-of-positions"><a href="#4-3-Number-of-positions" class="headerlink" title="4.3 Number of positions"></a>4.3 Number of positions</h3><ol><li>soft attention：编码器输出的所有隐藏状态都要参与注意力的计算。方便反向传播但是会造成二次方的损失（quadratic computational cost）。</li><li>hard attention：[Xu et al. 2015]提出通过范畴分布（multinoulli distribution）随机选取隐藏状态计算注意力权重。降低了计算损失但导致函数不可微，并且难以优化。Variational learning方法和 policy gradient 方法的提出可以帮助解决这些缺陷。</li><li>local and global：[Luong et al. 2015b] 为机器翻译提出两种注意力模型：local、global。global类似soft attention模型。local介于soft和hard之间。local的关键想法是在输入序列中找到一个”注意点”（attention point），在这个点（位置）周围选择一个创建一个窗口，在窗口中建立局部soft attention模型。这个点（位置）可以认为设定(monotonic alignment) 或者通过学习得到(predictive alignment)。</li></ol><h3 id="4-4-Number-of-representations"><a href="#4-4-Number-of-representations" class="headerlink" title="4.4 Number of representations"></a>4.4 Number of representations</h3><ol><li>Single-representational AM：只使用输入序列的一种特征表示法。这是大多数应用使用的方法。但是在某些情况下不能满足下游任务。</li><li>Multi-representational AM：通过多种特征表示来获取输入的不同方面的特征，然后使用注意力机制为不同的表示添加权重从而决定与这个任务最相关的表示法。[Kiela et al. 2018]、[Maharjan et al. 2018]、[Lin et al. 2017] 、 [Shen et al. 2018] 都做了类似的工作。</li></ol><h2 id="5-NETWORK-ARCHITECTURES-WITH-ATTENTION"><a href="#5-NETWORK-ARCHITECTURES-WITH-ATTENTION" class="headerlink" title="5 NETWORK ARCHITECTURES WITH ATTENTION"></a>5 NETWORK ARCHITECTURES WITH ATTENTION</h2><h3 id="5-1-Encoder-Decoder"><a href="#5-1-Encoder-Decoder" class="headerlink" title="5.1 Encoder-Decoder"></a>5.1 Encoder-Decoder</h3><p>最早在 [Bahdanau et al. 2015]中提出。编码器把输入压缩到一个固定长度的向量，解码器再对这个向量进行处理。这种方式把输入输出进行解耦，这使得杂交encoder-decoder成为了可能。比如编码器用CNN，解码器用LSTM。对许多模型任务例如图像，视频描述、Visual Question Answering和语音识别有非常大的帮助。<br>但是并不是所有问题都可以的输入输出都是序列化的。Pointer Network [Vinyals et al. 2015]有如下的两个不同：</p><ol><li>输出是离散的值，并且指向了输入序列的某个位置。</li><li>输出类别取决于输入的数量。<br>作者通过使用注意力权重来对每个时间步选择第i个输入符号的概率。这个方法可以用作离散优化问题（旅行商，排序）</li></ol><h3 id="5-2-Transformer"><a href="#5-2-Transformer" class="headerlink" title="5.2 Transformer"></a>5.2 Transformer</h3><p>循环结构会序列化的处理输入，这导致计算效率下降。Transformer [Vaswani et al. 2017].通过使用自注意力机制来获得输入输出的全局依赖。作者展示了使用Transformer，即便不使用循环结构也可以在机器翻译任务中获得超强的并行处理能力，更短的训练时间和更高的准确度。</p><h3 id="5-3-Memory-Networks"><a href="#5-3-Memory-Networks" class="headerlink" title="5.3 Memory Networks"></a>5.3 Memory Networks</h3><p>类似Question Answering的应用需要拥有从事实数据库中学习的能力。网络的输入是知识数据库（knowledge dagabase）和一个问题（query）。在数据库中有些事实是与问题相关的但有些不是。在这种情况下就需要注意力来选择相关的事实。</p><h3 id="5-4-Graph-Attention-Networks-GAT"><a href="#5-4-Graph-Attention-Networks-GAT" class="headerlink" title="5.4 Graph Attention Networks (GAT)"></a>5.4 Graph Attention Networks (GAT)</h3><p>战略性略过</p><h2 id="6-APPLICATIONS"><a href="#6-APPLICATIONS" class="headerlink" title="6 APPLICATIONS"></a>6 APPLICATIONS</h2><p><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_11ba64cfcd9ee772f67375ac7dbc8270.png" alt=""></p><h2 id="7-ATTENTION-FOR-INTERPRETABILITY"><a href="#7-ATTENTION-FOR-INTERPRETABILITY" class="headerlink" title="7 ATTENTION FOR INTERPRETABILITY"></a>7 ATTENTION FOR INTERPRETABILITY</h2><p>越来越多的人关注AI模型的可解释性，而且这正是神经网络特别是深度学习网络所欠缺的。注意力机制可以帮助我们提升AI模型的可解释性。存在的一种假设是注意力权重的大小与对应输入部分与当前时间步下的输出的相关度有关。通过可视化输入输出对可以发现这种规律。<br> [Rush et al. 2015]展示了在（文本）总结问题中，AM可以在输出时把注意力放在输入的相关的单词上（下图a）。 [He et al. 2018]表示注意力可以用来识别用户的兴趣（下图b）。[Xu et al. 2015]通过可视化展示了图片摘要问题中对输出有很大影响的图形区域。<br><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_5220608e9e49aeb36a7d305a78faa59a.png" alt=""><br>总结几种有趣的事实：</p><ol><li>[De-Arteaga et al. 2019] explored gender bias in occupation classification, and showed how the words getting more attention during classification task are often gendered.（没看懂，直接粘原文了）</li><li>[Yang et al. 2016]表示在情感分类问题中，”good”和”bad”这两个单词的重要性是与文本内容相关的。</li><li>[Chan et al. 2016]注意到在语音识别中，字符输出和音频信号之间的注意力可以正确识别音频信号中第一个字符的起始位置，并且对于具有声学相似性的单词，注意力权重是相似的。</li><li>[Kiela et al. 2018]发现multi-representational attention对 GloVe, FastText这两种词嵌入方式赋予了更高的权重。</li></ol><p>另一种有趣的应用： [Lee et al. 2017] and [Liu et al. 2018]提供了一种注意力可视化的工具</p><p>尽管许多人用注意力来提高AI模型的可解释性，但有些人也提出了相反的观点。 [Jain and Wallace 2019]提出注意力权重与特征重要性分析是不相关的。他们观察预测结果对注意力权重改变的敏感度，却发现通过使用随机排列和adversarial training并没有改变输出结果。[Serrano and Smith 2019] applied a different analysis based on intermediate representation erasure method and showed that attention weights are at best noisy predictors of relative importance of the specific regions of input sequence, and should not be treated as justifications for model’s decisions. （又没看懂………….）</p><h2 id="未来的研究方向"><a href="#未来的研究方向" class="headerlink" title="未来的研究方向"></a>未来的研究方向</h2><ol><li>Real-time Attention<br>类似实时翻译的应用需要在获得整个输入之前开始预测结果，这就需要Real-time Attention。<br>相关的成果：<ol><li>[Chiu and Raffel 2017] </li><li>[Ma et al. 2019] </li></ol></li><li><p>Stand-alone Attention<br>相关成果：</p><ol><li>[Ramachandran et al. 2019]</li><li>[Wang et al. 2020e]</li></ol></li><li><p>Model Distillation<br>相关成果：</p><ol><li>[Wang et al. 2020d] </li><li>[Mirzadeh et al. 2020]</li><li>[Touvron et al. 2020] </li></ol></li><li><p>Attention for Interpretability<br>相关成果</p><ol><li>[Mohankumar et al. 2020]</li></ol></li><li><p>Auto-learning Attention<br>相关成果：</p><ol><li>[Ma et al. 2020] </li></ol></li><li><p>Multi-instance Attention<br>相关成果：</p><ol><li>[Li et al. 2019a]</li></ol></li><li><p>Multi-agent Systems<br>相关成果：</p><ol><li>[Fujii et al. 2020; Li et al. 2020a] </li></ol></li><li><p>Scalability<br>相关成果：</p><ol><li>[Sukhbaatar et al. 2019b] </li><li>[Choromanski et al. 2021]</li><li>[Wang et al. 2020a]</li><li>[Li et al. 2020b]</li></ol></li></ol><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ol><li><a href="https://arxiv.org/pdf/1904.02874.pdf">Chaudhari S, Mithal V, Polatkan G, et al. An attentive survey of attention models[J]. ACM Transactions on Intelligent Systems and Technology (TIST), 2021, 12(5): 1-32.</a></li><li><a href="https://github.com/d2l-ai/d2l-zh">《动手学深度学习》</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/10/28/hello-world/"/>
      <url>/2021/10/28/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo new "My New Post"</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
