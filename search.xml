<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Reinforcement Learning-Based Black-Box Model Inversion Attacks 阅读笔记</title>
      <link href="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/"/>
      <url>/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Reinforcement-Learning-Based-Black-Box-Model-Inversion-Attacks"><a href="#Reinforcement-Learning-Based-Black-Box-Model-Inversion-Attacks" class="headerlink" title="Reinforcement Learning-Based Black-Box Model Inversion Attacks"></a>Reinforcement Learning-Based Black-Box Model Inversion Attacks</h1><h2 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h2><ul><li><strong>Type</strong>: ConferencePaper</li><li><strong>Title</strong>: Reinforcement Learning-Based Black-Box Model Inversion Attacks, </li><li><strong>Author</strong>: Han, Gyojin; Choi, Jaehyun; Lee, Haeil; Kim, Junmo, </li><li><strong>Year</strong>: 2023 ;</li><li><strong>Pages</strong>: 20504-20513</li><li><strong>Publisher</strong>: IEEE,</li><li><strong>Location</strong>: Vancouver, BC, Canada,</li><li><strong>DOI</strong>: 10.1109/CVPR52729.2023.01964</li></ul><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Model inversion attacks are a type of privacy attack that reconstructs private data used to train a machine learning model, solely by accessing the model. Recently, white-box model inversion attacks leveraging Generative Adversarial Networks (GANs) to distill knowledge from public datasets have been receiving great attention because of their excellent attack performance. On the other hand, current blackbox model inversion attacks that utilize GANs suffer from issues such as being unable to guarantee the completion of the attack process within a predetermined number of query accesses or achieve the same level of performance as whitebox attacks. To overcome these limitations, we propose a reinforcement learning-based black-box model inversion attack. We formulate the latent space search as a Markov Decision Process (MDP) problem and solve it with reinforcement learning. Our method utilizes the conﬁdence scores of the generated images to provide rewards to an agent. Finally, the private data can be reconstructed using the latent vectors found by the agent trained in the MDP. The experiment results on various datasets and models demonstrate that our attack successfully recovers the private information of the target model by achieving state-of-the-art attack performance. We emphasize the importance of studies on privacy-preserving machine learning by proposing a more advanced black-box model inversion attack.</p><p>模型逆向攻击是一种隐私攻击，通过仅访问模型，重新构建用于训练机器学习模型的私有数据。最近，利用生成对抗网络（GANs）从公共数据集中提取知识的白盒模型逆向攻击引起了广泛关注，因为其具有卓越的攻击性能。另一方面，当前利用GAN的黑盒模型逆向攻击存在一些问题，比如无法保证在预定数量的查询访问内完成攻击过程，或无法达到与白盒攻击相同水平的性能。为了克服这些限制，我们提出了一种基于强化学习的黑盒模型逆向攻击。我们将潜在空间搜索问题制定为马尔可夫决策过程（MDP）问题，并通过强化学习来解决它。我们的方法利用生成图像的置信度分数来为代理提供奖励。最终，通过在MDP中训练的代理找到的潜在向量，可以重构私有数据。在各种数据集和模型上的实验证明，我们的攻击成功地恢复了目标模型的私有信息，实现了最先进的攻击性能。我们强调通过提出更先进的黑盒模型逆向攻击，进行隐私保护机器学习研究的重要性。</p><hr><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>随着人工智能的迅速发展，深度学习应用正在各个领域不断涌现，如计算机视觉、医疗保健、自动驾驶和自然语言处理。随着需要使用私有数据来训练深度学习模型的案例数量增加，对包括敏感个人信息在内的私有数据泄漏的关注正在上升。特别是关于隐私攻击的研究[21]表明，恶意用户可以从训练过的模型中提取个人信息。其中一种对机器学习模型最具代表性的隐私攻击是模型逆向攻击，它通过仅访问模型就能重构目标模型的训练数据。</p><p>模型逆向攻击分为三类，即1）白盒攻击，2）黑盒攻击和3）仅标签攻击，这取决于目标模型的信息量。白盒攻击可以访问模型的所有参数。黑盒攻击可以访问由置信度分数组成的软推理结果，而仅标签攻击只能以硬标签形式访问推理结果。随着对深度学习模型隐私攻击的关注增加，加强在开发和部署深度学习模型时的隐私保护机制显得尤为重要。研究人员和实践者正在积极探索提高机器学习模型隐私性的方法，包括差分隐私、联邦学习和安全多方计算等技术。随着领域的不断发展，解决隐私挑战将对确保人工智能技术的负责和道德使用至关重要。</p><p>白盒模型逆向攻击[5, 25, 27]通过使用生成对抗网络（GANs）[10]成功地恢复了包括个人信息在内的高质量私有数据。首先，它们在独立的公共数据上训练GANs以学习私有数据的一般先验。然后，由于可以访问已训练白盒模型的参数，它们利用基于梯度的优化方法搜索并找到代表特定标签数据的潜在向量。然而，这些方法无法应用于保护模型参数的机器学习服务，如亚马逊的Rekognition [1]。要从这些服务中重构私有数据，需要进行关于黑盒和仅标签的模型逆向攻击的研究。</p><p>与白盒攻击不同，这些攻击需要能够探索GANs的潜在空间的方法，以便利用它们，因为无法进行基于梯度的优化。最近提出的深度学习网络的模型逆向（MIRROR）[2]使用遗传算法在从黑盒目标模型获取的置信度分数中搜索潜在空间。此外，边界驱逐模型逆向攻击（BREPMI）[14]通过使用基于决策的零阶优化算法进行潜在空间搜索，在仅标签的设置下取得了成功。</p><p>尽管存在这些尝试，但每种方法都存在显著问题。BREP-MI从第一个生成被分类为目标类别的图像的潜在向量开始潜在空间搜索的过程。这并不保证在通过随机采样找到第一个潜在向量之前需要多少次查询访问，并且在最坏的情况下，可能无法为某些目标类别启动搜索过程。对于MIRROR，尽管它使用了置信度分数进行攻击，但其表现较仅标签攻击BREP-MI更差。因此，我们提出了一种新方法，即基于强化学习的黑盒模型逆向攻击（RLB-MI），作为解决上述问题的方案。我们整合强化学习，以从置信度分数中获得有关潜在空间探索的有用信息。更具体地说，我们将在GAN中潜在空间的探索形式化为马尔可夫决策过程（MDP）的问题。然后，我们基于生成图像的置信度分数为代理提供奖励，并使用回放内存中的更新步骤使代理能够逼近包括潜在空间在内的环境。基于这些信息选择的代理操作比现有方法更有效地导航潜在向量。最终，我们可以通过从潜在向量中经过GAN重构私有数据。我们在各种数据集和模型上进行了实验。攻击性能与三类各种模型逆向攻击进行了比较。结果表明，所提出的攻击能够通过优于所有其他攻击的性能成功地恢复有关私有数据的有意义信息。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="模型逆向攻击"><a href="#模型逆向攻击" class="headerlink" title="模型逆向攻击"></a>模型逆向攻击</h3><p>模型逆向攻击是一种针对机器学习模型的隐私攻击，其目的是重构用于训练的数据。早期的研究主要关注白盒条件，即已训练的模型是完全可访问的。Fredrikson等人通过从低复杂性模型中提取敏感属性[8]和面部图像[7]，展示了模型逆向攻击的严重性。然而，早期的白盒模型逆向攻击由于无法从复杂模型中重构高维数据而存在明显的局限性。</p><p>为解决这些限制，许多白盒模型逆向攻击利用了生成模型。Zhang等人[27]提出了一种名为生成模型逆向攻击（GMI）的攻击，使用在公共数据上训练的GAN [10]。他们通过搜索GAN的潜在空间而不是图像空间来重构图像。GAN的流形逼近能力有效地缩小了攻击模型的搜索空间。知识丰富的分布模型逆向攻击（KED-MI）[5]通过使用一个具有反演特定功能的GAN来改进GMI，其中鉴别器执行多类推断。反演专用GAN的鉴别器可以区分真实数据和伪造数据，并且可以预测目标网络的输出。这使得反演专用GAN能够从公共数据中提取目标模型的有用知识。此外，KED-MI通过提取目标类别数据的分布，而不仅仅是目标类别的一个训练数据，可以创建多样化的图像。变分模型逆向攻击（VMI）[25]将模型逆向攻击形式化为包含深度标准化流和styleGAN [15]框架的变分推断问题。</p><p>不幸的是，白盒攻击并不适用于在训练模型的参数没有信息的情况下。因此，提出了仅需要访问软标签的黑盒模型逆向攻击。基于学习的模型逆向攻击（LBMI）[26]使用类似于自动编码器的结构来训练一个反演模型，该模型以与目标网络相反的方式行为。反演模型不仅需要访问攻击者构造的查询，还需要访问用户输入数据的置信度分数，该数据是反演的目标。Salem等人[22]提出了一种攻击，通过计算模型更新前后输出的差异来泄露用于更新的训练数据的信息。这两种方法都有一个局限，即它们需要攻击者通常难以访问的信息。深度学习网络的模型逆向（MIRROR）[2]表明可以利用GANs在黑盒模型逆向攻击中使用遗传算法。此外，提出了一种仅标签的模型逆向攻击，边界排斥模型逆向攻击（BREPMI）[14]。BREP-MI在反演过程中仅使用硬标签。BREP-MI从当前潜在向量为中心的球面上的点的标签估计梯度。使用估计的梯度更新潜在向量，使得GAN从潜在向量中重构目标类别的代表性图像。</p><h3 id="深度增强学习"><a href="#深度增强学习" class="headerlink" title="深度增强学习"></a>深度增强学习</h3><p>使用深度神经网络进行强化学习（RL）的研究开始引起关注，因为深度Q网络（DQN）[18]在Atari 2600游戏中展现出与人类专家相媲美的性能[4]。DQN通过使用深度神经网络来估计动作值函数，成功解决了具有高维状态输入（如原始像素）的任务。然而，DQN不能立即应用于具有高维、连续动作空间的问题，因为它通过找到最大化动作值函数值的动作来工作。为了解决具有高维、连续动作空间的问题，提出了深度确定性策略梯度（DDPG）[16]。DDPG是一种无模型和离策略算法，使用基于深度策略梯度（DPG）[23]的演员-评论家方法。通过将DQN的回放缓冲和目标网络的思想应用于演员-评论家方法，DDPG稳定了学习过程。即使在DDPG之后，许多深度强化学习方法已被提出以改进DDPG。为了克服过度估计偏差问题，提出了双延迟深度确定性策略梯度（TD3）[9]算法。TD3通过使用一些技巧，如剪切的双Q学习和延迟的策略更新，解决了这个问题。此外，提出了Soft Actor-Critic（SAC）[12]来处理复杂环境的问题。SAC通过将熵最大化项添加到标准最大奖励强化学习目标中，显著提高了探索性能和稳定性。</p><h2 id="提出的方法"><a href="#提出的方法" class="headerlink" title="提出的方法"></a>提出的方法</h2><p>在本节中，我们将介绍我们的方法，即基于强化学习的黑盒模型逆向攻击（RLB-MI）。RLB-MI的概述如图1所示。<br><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img1.png" alt="图 1"></p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p><strong>攻击者的目标</strong>：模型逆向的目标是从使用数据集$D_{pvt}$训练的模型$T$中恢复出类别$y$的样本。目标模型$T: x\to [0,1]^K$学习的是样本到类别的映射。用$K$表示$D_{pvt}$的大小，$d$表示输入的维度。</p><p><strong>攻击者的知识</strong>：由于我们的方法处理的是黑盒设置，攻击者只能访问由攻击者输入的数据和与数据相对应的软标签组织的查询。此外，攻击者了解目标模型的目的。正如先前的研究中所提到的[2, 5, 14, 25, 27]，提供的模型或服务的任务信息不仅是可用的，而且还可以从输出的类别中轻松推断出来。基于对任务的这种了解，攻击者可以访问相应任务的公共数据集。</p><p><strong>概述</strong>：给定在私有数据集$D_{pvt}$上训练的黑盒模型$T$，一个黑盒的模型逆向攻击目的就是恢复出这个私有的数据集。和最近的模型逆向攻击一致，我们也使用了一个公开数据集$D_{pub}$训练了一个Gan  $G$。攻击者只能知道一个模型输出的软标签，但对模型的参数和结构一无所知。因此，我们的方法就是通过搜索能够使模型输出相应类别的高置信度样本的隐向量。我们把隐向量的搜索过程形式化为马尔可夫过程（MDP），然后使用强化学习来解决该问题。更具体的，我们把MDP的状态空间定义为生成器$G$的隐空间，于是每一步$t$的状态$s_t$都可以用隐空间中的向量来表示。状态$s_t$经过动作 $a_t$后会变为状态$s_{t+1}$。任务的目标就是最大化目标模型的置信度向量。</p><h3 id="隐空间搜索中的马尔卡弗过程"><a href="#隐空间搜索中的马尔卡弗过程" class="headerlink" title="隐空间搜索中的马尔卡弗过程"></a>隐空间搜索中的马尔卡弗过程</h3><p>我们描述了用于潜在空间搜索的MDP的组成部分：状态，动作，状态转换和奖励。</p><p><strong>状态</strong>：MDP的状态空间就是生成器G的隐空间（输入空间）。对于每一个eposode，第一个状态$s_0$是一个$k$维的标准化随机向量：</p><script type="math/tex; mode=display">s_0 \sim \mathcal{N}_k(0,1),s_0 \in \mathbb{R}^k,</script><p>在每一步$t$，状态$s_t$会被动作$a_t$更新。</p><p><strong>动作</strong>：我们希望动作可以让随机产生的初始状态向着高奖励的最终状态前进。从宏观的视角来看，我们可以把这种问题理解为基于强化学习的路径查找问题。在传统的路径查找问题中，动作被定义为了$s_t$到$s_{t+1}$的位移$\triangle s$。然而，与寻路中的有界二维空间不同，潜在空间是一个不受限制的高维空间。在我们的阐述中，当将行为定义为位移向量时，由于与潜在空间相比存在较大的状态方差和狭窄的探索区域，强化学习代理可能无法收敛并达到局部最小值或完全失败。因此，我们将行为空间视为整个潜在空间。我们将一个潜在矢量形状的行为定义为一个引导矢量。如图1所示，根据定义，在相同的空间中选择行为。这使得能够广泛探索整个潜在空间，防止代理陷入局部最小值并保证代理的收敛。每一步$t$的动作都由强化代理$A:\mathbb{R}^k \to \mathbb{R}^k$来产生：</p><script type="math/tex; mode=display">a_t=A(s_t),a_t\in \mathbb{R}^k</script><p><strong>状态转移</strong>：我们通过在每一步将状态朝着行为移动来更新状态，使用一个差异因子 α 作为状态转移中当前状态的权重，以确定移动距离。在步骤 t 的状态转移如下：</p><script type="math/tex; mode=display">s_{t+1} = \alpha \cdot s_t + (1-\alpha) \cdot a_t</script><p>我们将 $\alpha$ 称为多样性因子的原因是，我们将 $\alpha$ 提出作为一个超参数，允许我们控制生成图像的多样性。正如Wang等人所提到的[25]，在重构图像时，模型逆向攻击存在准确性和多样性之间的权衡。我们可以通过 $\alpha$ 调整准确性和多样性之间的权衡。 $\alpha$ 越高，随机初始潜在向量对每个episode的影响越大，代理就越专注于生成具有高多样性的图像。例如，如果 $\alpha$ 的值为零，下一个状态将与当前步骤中的行为相同，因此代理只专注于生成目标分类器概率最高的一张图像。另一方面，随着 $\alpha$ 的增加，随机初始潜在向量的影响增大，代理被训练以找到具有高概率属于目标类别的各种图像。</p><p><strong>奖励</strong>：在通过行为更新状态后，代理从环境中接收一个奖励。生成器 $G$ 从更新后的潜在向量生成一张图像，通过使用目标网络 $T$ 进行推理，我们可以得到图像目标类别 $y$ 的置信度分数。由于行为指导了状态的移动方向，从行为生成的图像也应该接近目标类别空间。为了将状态和行为置于靠近目标类别空间的位置，我们需要在状态和行为具有较高置信度分数时提供较高的奖励。因此，我们构建一个奖励，其中包含状态分数和行为分数，这些分数是由每个矢量创建的图像的置信度分数的对数值计算而得。这些分数的计算如下：</p><script type="math/tex; mode=display">r_1 = log[T_y(G(s_{t+1}))]</script><script type="math/tex; mode=display">r_2 = log[T_y(G(a_{t}))]</script><p>其中$r_1$使状态分数，$r_2$是动作分数。此外，我们希望重构的图像具有目标类别的特征，使其与其他类别的图像有所区别。因此，我们提出了一个额外的项 $r_3$，对其他类别图像的高置信度分数进行惩罚。我们计算目标类别的置信度分数与其他类别的最大置信度分数之间的差异。与先前的分数一样，对计算值应用对数。由于对于小于或等于零的数字，对数是未定义的，因此取对数为减去的值和一个小正数 $\epsilon$ 中的较大者。项 $r3$ 的表达式如下：</p><script type="math/tex; mode=display">r_3=\log[\max\{\epsilon,T_y(G(s_{t+1}))-\max_{i\neq y}T_i(G(s_{t+1}))\}].</script><p>每一步的总奖励 $Rt$ 计算如下：</p><script type="math/tex; mode=display">\begin{aligned}R_t&=w_1\cdot r_1+w_2\cdot r_2+w_3\cdot r_3,\end{aligned}</script><p>其中$w_n$是分数$r_n$的权重。</p><h3 id="使用强化学习解决MDP"><a href="#使用强化学习解决MDP" class="headerlink" title="使用强化学习解决MDP"></a>使用强化学习解决MDP</h3><p>我们通过强化学习来解决被提出的潜在空间搜索问题，该问题被制定为马尔可夫决策过程（MDP）。由于所提出的由 G 和 T 组成的 MDP 的环境非常复杂，我们需要在复杂环境中具有鲁棒性的强化学习代理。此外，我们需要一个能够处理连续行为空间的代理，因为MDP中的行为空间被定义为 G 的潜在空间。因此，我们使用满足所有上述点的 Soft Actor-Critic (SAC) [12] 来解决MDP。我们训练一个SAC代理来从给定状态中选择适当的行为。在训练之后，通过将随机初始向量提供给训练过的代理作为初始状态，我们可以获得每个episode的重构图像。代理训练的整个过程如 算法 1 所示。</p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/al1.png" alt="算法 1"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p><strong>数据集</strong>：我们对使用代表性人脸数据集 CelebFaces 属性数据集（CelebA）[17]、FaceScrub 数据集[19] 和 PubFig83 数据集[20] 训练的目标分类器进行了攻击评估。我们将每个数据集分为一个用于训练目标分类器的私有数据集和一个用于训练生成模型的公共数据集。公共数据集与私有数据集之间没有类别交集，因此生成模型无法学习目标分类器的特定类别信息。</p><p>对于 CelebA，私有数据集包括 30,027 张属于 1,000 个身份的图像，而公共数据集包括从其余的 9,177 个身份中随机选择的 30,000 张图像，与先前的研究[5, 14]一样。对于 FaceScrub，总共 530 个身份中，随机选择的 200 个身份的所有图像用作私有数据集，剩余 330 个身份的所有图像用作公共数据集。对于 PubFig83，总共 83 个身份中，随机选择的 50 个身份的所有图像用作私有数据集，剩余 33 个身份的所有图像用作公共数据集。此外，我们使用 Flickr-Faces-HQ 数据集（FFHQ）[15] 作为公共数据集，考虑到公共数据集和私有数据集之间存在分布转移的情况。在这些实验中，从 FFHQ 随机选择的 30,000 张图像被用作公共数据集。所有人脸图像都经过居中裁剪，然后调整大小为 64 × 64。</p><p><strong>模型</strong>：为了进行公正比较，我们尝试对几种流行的网络结构进行攻击。与先前的研究[5, 14, 27]类似，我们在实验中使用了三种网络结构，分别是 VGG16 [24]、ResNet-152 [13] 和 Face.evoLVe [6]。</p><p><strong>评估指标</strong>：Zhang 等人[27]提出了可以定量评估模型逆向攻击的指标，与之前依赖于视觉检查的定性评估不同。我们简要描述评估指标，包括攻击准确率、K 最近邻距离（KNN Dist）和特征距离（Feat Dist）。</p><p>攻击准确率：为了评估重构图像的攻击准确率，我们在私有数据集上训练评估分类器。评估分类器必须与目标分类器不同，因为重构图像可能会过拟合目标分类器。我们使用了 Cheng 等人[6]提出的架构作为评估分类器。我们在 MS-Celeb-1M [11] 上对一个预训练的分类器进行微调，分别在 PubFig83、FaceScrub 和 CelebA 数据集上实现了 98%、99% 和 96% 的测试准确率。</p><p>KNN Dist：KNN Dist 是一种度量，用于测量重构图像的特征与目标标签图像中最近样本的特征之间的平均 L2 距离。特征是在评估分类器的全连接层之前获取的。</p><p>Feat Dist：Feat Dist 是一种度量，用于测量重构图像的特征与目标标签图像的特征中心的 L2 距离。特征是在评估分类器的全连接层之前获取的。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><strong>基线</strong>：我们将我们的攻击性能与三个类别中包括的代表性模型逆向攻击进行比较：白盒攻击、黑盒攻击和仅标签攻击。白盒攻击的基线是 GMI [27] 和 KED-MI [5]。GMI 是使用 GAN 的第一个模型逆向攻击，而 KED-MI 在白盒攻击中表现出最高的攻击性能。黑盒和仅标签的基线是 LB-MI [26]、MIRROR [2] 和 BREP-MI [14]。为了公平比较，除了 LB-MI 和 KED-MI 之外，攻击中使用了相同的 GAN，因为 LBMI 不使用 GAN，而 KED-MI 使用其自己的生成模型，名为反演专用 GAN。我们使用每个周期的代理生成图像，并选择对目标分类器具有最高置信度分数的图像。</p><p><strong>在各种模型上的性能评估</strong>：表1显示了RLB-MI和基线在使用CelebA训练的三个模型（VGG16、ResNet-152和Face.evoLVe）上的评估结果。每个目标模型的测试准确率分别为88%、91%和89%。尽管现有的黑盒模型逆向攻击LB-MI和MIRROR可以访问软标签，但它们的性能明显低于BREP-MI，这是一种仅标签的模型逆向攻击方法。然而，提出的黑盒攻击RLB-MI通过适当利用置信度分数的信息，远远超过了BREP-MI。此外，我们的攻击在ResNet-152和Face.evoLVe的情况下超过了最先进的白盒模型逆向攻击KED-MI，尽管无法访问梯度信息。即使在VGG16的情况下，通过RLB-MI重构的图像也捕捉到目标类的信息特征，具有与真实样本的K最近邻距离和特征距离最小。还可以看到目标分类器的更高预测性能导致攻击性能的提高。这个结果是合理的，因为性能更好的分类器包含关于训练数据特征的更准确和关键的信息。我们通过提供基线和我们的方法生成的真实样本和攻击图像，对定性评估结果进行展示，因为评估分类器可能不能完全代表人类判断。<br><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table1.png" alt="表1"></p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img2.png" alt="图2"></p><p><strong>在各种数据集上的性能评估</strong>：我们测量了在使用PubFig83、FaceScrub和CelebA训练的Face.evoLVe模型上进行攻击的性能。每个目标模型的测试准确率分别为96%、97%和89%。表2显示，RLB-MI的攻击准确率优于所有基线。尽管LB-MI显示出与其他方法竞争的特征距离，但它获得了非常低的攻击准确率。这是由于LB-MI反演模型学习来自公共数据集的一般先验能力的限制引起的。<br><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table2.png" alt="表2"></p><p><strong>公共数据集差异的影响</strong>：在大多数实际情境中，公共和私有数据集具有相同分布的机会很少，因此我们通过在与私有数据集不同分布的公共数据集上训练生成模型来进行实验。我们使用Flickr-Faces-HQ数据集（FFHQ）[15]训练的生成模型进行攻击评估。这些实验中使用的目标分类器是使用PubFig83、FaceScrub和CelebA进行训练的Face.evoLVe。在使用FFHQ作为公共数据集的实验中，模型逆向攻击显示出了降低的攻击性能。然而，即使在公共和私有数据集之间存在分布偏移的情况下，如表3所示，我们的攻击仍然实现了最先进的攻击性能。我们认为性能下降可能是因为每个数据集中对齐或裁剪面部的方式不同，每个数据集中包含的性别或年龄分布也不同。在拍照时，由光照条件或背景引起的图像分布差异可能也会影响攻击性能。</p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table3.png" alt="表3"></p><h3 id="对攻击配置进行实验"><a href="#对攻击配置进行实验" class="headerlink" title="对攻击配置进行实验"></a>对攻击配置进行实验</h3><p><strong>准确性和多样性之间的权衡</strong>：我们根据多样性因子$\alpha$的变化测量训练代理的攻击准确性和多样性。我们为各种$\alpha$值训练代理，并为特定身份的每个代理生成1,000张图像。为了分别评估生成图像的保真度和多样性，我们使用Density and Coverage (D&amp;C) [3]作为度量标准。在这些实验中使用的目标分类器是使用CelebA对Face.evoLVe进行训练的。图3显示了攻击准确性和多样性之间的权衡。随着$\alpha$的增加，攻击准确性减小，表示多样性的覆盖率增加。此外，从图3b可以看出，密度对$\alpha$的变化具有鲁棒性，这意味着代理生成的图像的保真度不受$\alpha$变化的影响。当$\alpha$为0.00和0.97时生成的图像在图4中可视化。</p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img3.png" alt="图3"></p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table4.png" alt="图4"></p><p><strong>攻击性能随最大轮数的变化趋势</strong>：为了观察攻击性能随最大轮数的变化，我们按照从1,000到40,000的步长报告攻击准确性。实验结果见图5。在图5a中，我们绘制了使用CelebA训练的各种目标模型的结果，而在图5b中，我们绘制了使用不同数据集训练的Face.evoLVe模型的结果。攻击性能每达到最大轮数就迅速增加，然后趋于稳定。即使更改目标分类器的结构，当数据集相同时，攻击准确性的趋势没有显著差异。此外，目标数据集的目标类别数越少，饱和点就越早出现，之后攻击准确性由于过拟合而下降。对于有50个目标类别的PubFig83，饱和点出现在25,000轮，而对于有200个目标类别的FaceScrub，饱和点出现在36,000轮。因此，根据目标数据集设置适当的最大轮数是很重要的。<br><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img5.png" alt="图5"></p><p><strong>对各种RL代理的实验</strong>：为了确定RL代理的效果，我们尝试使用DDPG [16]和TD3 [9]代替SAC进行潜在空间搜索。在这个实验中使用的目标分类器是使用CelebA训练的Face.evoLVe。表4中的结果显示，当在我们的攻击中使用SAC时，其性能明显高于DDPG和TD3。我们认为这些结果的原因在于SAC对嘈杂和复杂环境的鲁棒性。在我们的攻击中，每一轮都会重新给出一个高维随机潜在向量，而目标身份的置信度对这样的潜在向量变化非常敏感。因此，对于嘈杂和复杂环境具有鲁棒性的强化学习算法，如SAC，是可取的。此外，我们在图6中展示了每个RL代理的奖励变化。</p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/table4.png" alt="表4"></p><p><img src="/2024/01/18/reinforcement-learning-based-black-box-model-inversion-attacks-yue-du-bi-ji/img6.png" alt="图6"></p><h3 id="伦理的考虑"><a href="#伦理的考虑" class="headerlink" title="伦理的考虑"></a>伦理的考虑</h3><p>如果恶意用户滥用所提出的黑盒模型逆向攻击，可能会带来侵犯需要保护的个人信息等负面社会影响。然而，揭示当前系统的漏洞对于安全性的发展是不可或缺的。通过这项研究，我们提高了对机器学习隐私问题的关注，并敦促社区开发算法或系统以防范所提出的漏洞。我们相信，我们的工作将在安全性方面产生积极的影响，其益处将超过潜在的风险。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们提出了一种基于强化学习的新型黑盒模型逆向攻击，利用生成对抗网络（GAN）。我们将潜在空间的探索问题形式化为马尔可夫决策过程（MDP）问题，并训练一个强化学习代理来解决MDP，即使缺乏关于目标模型的信息，如权重和梯度。所提出的攻击解决了先前黑盒攻击存在的问题。此外，实验结果表明，我们的攻击成功地重构了目标模型的私有数据。我们的攻击不仅优于最先进的黑盒攻击，还优于所有其他方法，包括白盒和仅标签攻击。我们希望这项研究能够推动对黑盒模型逆向攻击和防御的研究。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> 模型逆向 </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CADE: Detecting and Explaining Concept Drift Samples for Security Applications 阅读笔记</title>
      <link href="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/"/>
      <url>/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="CADE-Detecting-and-Explaining-Concept-Drift-Samples-for-Security-Applications"><a href="#CADE-Detecting-and-Explaining-Concept-Drift-Samples-for-Security-Applications" class="headerlink" title="CADE: Detecting and Explaining Concept Drift Samples for Security Applications"></a>CADE: Detecting and Explaining Concept Drift Samples for Security Applications</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Concept drift poses a critical challenge to deploy machine learning models to solve practical security problems. Due to the dynamic behavior changes of attackers (and/or the benign counterparts), the testing data distribution is often shifting from the original training data over time, causing major failures to the deployed model.</p><p>To combat concept drift, we present a novel system CADE aiming to 1) detect drifting samples that deviate from existing classes, and 2) provide explanations to reason the detected drift. Unlike traditional approaches (that require a large number of new labels to determine concept drift statistically), we aim to identify individual drifting samples as they arrive. Recognizing the challenges introduced by the high-dimensional outlier space, we propose to map the data samples into a low-dimensional space and automatically learn a distance function to measure the dissimilarity between samples. Using contrastive learning, we can take full advantage of existing labels in the training dataset to learn how to compare and contrast pairs of samples. To reason the meaning of the detected drift, we develop a distance-based explanation method. We show that explaining “distance” is much more effective than traditional methods that focus on explaining a “decision boundary” in this problem context. We evaluate CADE with two case studies: Android malware classification and network intrusion detection. We further work with a security company to test CADE on its malware database. Our results show that CADE can effectively detect drifting samples and provide semantically meaningful explanations.</p><p>概念漂移对于部署机器学习模型以解决实际安全问题构成了重大挑战。由于攻击者（和/或良性对手）的动态行为变化，测试数据分布通常会随时间从原始训练数据中漂移，导致已部署模型的重大故障。</p><p>为了应对概念漂移，我们提出了一个新颖的系统 CADE，旨在实现以下两个目标：1）检测偏离现有类别的漂移样本，以及2）提供解释来推断检测到的漂移的原因。与传统方法不同（传统方法通常需要大量新标签来统计性地确定概念漂移），我们的目标是在漂移样本到达时识别它们。考虑到高维度离群值空间引入的挑战，我们提出将数据样本映射到低维空间，并自动学习一个距离函数来度量样本之间的差异。通过对比学习，我们可以充分利用训练数据集中的现有标签，以学习如何比较和对比样本对。为了推断检测到的漂移的含义，我们开发了一种基于距离的解释方法。我们证明，在这个问题背景下，解释“距离”要比传统方法（集中于解释“决策边界”）更有效。我们通过两个案例研究对 CADE 进行了评估：Android恶意软件分类和网络入侵检测。我们还与一家安全公司合作，在其恶意软件数据库上测试了 CADE。我们的结果表明，CADE 能够有效地检测漂移样本并提供有意义的解释。</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>在本文中，我们提出了一种新的方法，用于检测漂移样本，并配以一种解释检测决策的新方法。总体而言，我们构建了一个名为“CADE（Contrastive Autoencoder for Drifting Detection and Explanation）”的系统。关键挑战在于推导一个有效的距离函数来衡量样本的不相似度。与随意选择距离函数不同，我们利用对比学习的思想来从现有的训练数据中（基于现有标签）学习距离函数。给定原始分类器的训练数据（多个类别），我们将训练样本映射到低维潜在空间。映射函数通过对比样本来学习，以扩大不同类别样本之间的距离，同时减小相同类别样本之间的距离。我们展示了潜在空间中产生的距离函数能够有效地检测和排名漂移样本。<br>为了解释漂移样本，我们确定了一小组重要的特征，这些特征区分了这个样本与其最近的类别。一个关键观察是传统的（监督）解释方法效果不佳。洞察力在于监督解释方法需要漂移样本和现有类别都有足够的样本来估计它们的分布。然而，鉴于漂移样本位于训练分布之外的稀疏空间，这个要求很难满足。相反，我们发现基于距离变化的解释更有效，即导致漂移样本与其最近类别之间距离发生最大变化的特征。</p><p>贡献：</p><ol><li>我们提出 CADE 来补充现有的基于监督学习的安全应用，以应对概念漂移。我们引入了一种基于对比表示学习的有效方法，用于检测漂移样本。</li><li>我们阐明了监督解释方法在解释离群样本方面的局限性，并为这种情境引入了一种基于距离的解释方法。</li><li>我们对提出的方法进行了广泛的评估，涉及两个应用领域。我们与一家安全公司进行的初步测试表明，CADE是有效的。我们已经在此处发布了CADE的代码，以支持未来的研究。</li></ol><h2 id="Background-and-Problem-Scope"><a href="#Background-and-Problem-Scope" class="headerlink" title="Background and Problem Scope"></a>Background and Problem Scope</h2><h3 id="概念漂移"><a href="#概念漂移" class="headerlink" title="概念漂移"></a>概念漂移</h3><p>概念漂移指的是测试集分布会随着时间变化，导致真实的决策边界发生偏移。这通常会让模型的预测性能随着时间降低。<br>为了检测概念漂移，研究人员提出了各种技术，主要涉及收集新的数据集以对模型行为进行统计评估。在一些研究中，这些工作还需要进行数据标注的努力。在安全应用中，首先了解新攻击的存在并收集与之相关的数据是具有挑战性的。此外，标注数据是耗时且需要相当专业知识的工作。因此，假设大部分输入数据都能被充分标注是不切实际的。</p><h3 id="问题空间"><a href="#问题空间" class="headerlink" title="问题空间"></a>问题空间</h3><p>与使用充分检测和充分标注的数据来检测概念漂移不同，我们对单个样本进行研究，从而发现那些偏离原始样本的数据。这使得我们可以在这种漂移样本到来时检测出概念漂移并标注它们。一旦我们收集到了足够多的数据，就可以直接启动模型重训练。</p><p>在多分类问题中，存在着两种概念漂移：</p><ol><li>新类别的引入</li><li>类别内的变化<br>在这篇文章中，我们主要关注第一类漂移</li></ol><h3 id="可能的解决方案及限制"><a href="#可能的解决方案及限制" class="headerlink" title="可能的解决方案及限制"></a>可能的解决方案及限制</h3><ol><li>第一个方法是使用原始分类器的预测概率。更具体地说，一个监督分类器通常会输出一个预测概率（或置信度），作为预测标签的附带产品。例如，在深度神经网络中，通常使用softmax函数来生成预测概率，该概率表示给定样本属于每个现有类别的可能性（总和为1）。因此，低的预测概率可能表明输入样本与现有的训练数据不同。然而，我们认为在我们的问题背景下，预测概率不太可能有效。原因是该概率反映了相对于现有类别的适应度（例如，样本在类别A中的适应程度优于类别B）。如果样本来自一个全新的类别（既不属于类别A也不属于类别B），那么预测概率可能会极具误导性。许多先前的研究已经证明，来自新类别的测试样本可能会导致误导性的概率分配（例如，将一个错误的类别与高概率关联）。从根本上讲，预测概率仍然继承了分类器的“封闭世界假设”，因此不适合检测漂移样本。</li><li>与预测概率相比，一个更有前景的方向是直接评估样本与给定类别的适应度（按我的理解应该就是相似度）。这个想法是，我们不是评估样本在类别A中的适应程度优于类别B，而是评估该样本在类别A中与其他训练样本的适应程度。例如，可以使用自编码器基于重构误差来评估样本对给定分布的适应度。然而，作为一种无监督方法，当忽略标签时，自编码器很难学习到训练分布的准确表示（详见第4节）。在最近的一项工作中，Jordaney等人引入了一个名为Transcend的系统。它将“非一致性度量”定义为适应度评估。Transcend使用可信度p值来量化测试样本x与属于同一类别的训练样本的相似程度。p是该类别中至少与同一类别的其他样本不相似的样本比例。虽然这个度量可以确定漂移样本，但这样的系统在很大程度上依赖于对“不相似性”的良好定义。正如我们将在第4节中展示的那样，任意的不相似性度量（特别是在数据维度高的情况下）可能导致性能不佳。</li></ol><h2 id="Designing-CADE"><a href="#Designing-CADE" class="headerlink" title="Designing CADE"></a>Designing CADE</h2><p>我们提出了一个叫做CADE的漂移样本检测和解释系统。我们首先描述了设计背后的直觉和见解，然后是每个组件的技术细节。</p><h3 id="设计直觉"><a href="#设计直觉" class="headerlink" title="设计直觉"></a>设计直觉</h3><p>如图一所示，我们的系统由两部分组成：</p><ol><li>检测模块：检测漂移样本</li><li>解释模块：帮助研究人员理解此次漂移意味着什么</li></ol><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231011145903.png" alt="img"></p><p>通过开始的分析，我们发现这两个任务有一个相同的挑战：漂移样本位于稀疏的离群空间，这让开发有意义的距离函数变得非常困难。</p><p>首先，检测漂移样本需要学习一个良好的距离函数，以度量“漂移样本”与现有分布的差异。然而，异常值空间具有无限大且稀疏的特点。对于高维数据，由于“维数诅咒” ，距离的概念开始失去效力。其次，解释的目标是识别一个重要特征的小子集，最有效地区分漂移样本和训练数据。因此，我们还需要一个有效的距离函数来度量这些差异。</p><p>在接下来的部分中，我们设计了一个漂移检测模块和一个解释模块，共同解决这些挑战。在高层次上，我们首先使用对比学习来学习训练数据的压缩表示。对比学习的一个关键优势是它可以利用现有标签，相对于无监督方法如自动编码器和主成分分析（PCA），实现了更好的性能。这使我们能够从训练数据中学习距离函数以检测漂移样本（第3.2节）。至于解释模块，我们将描述一种基于距离的解释公式，以解决前面提到的挑战（第3.3节）。</p><h3 id="漂移样本检测"><a href="#漂移样本检测" class="headerlink" title="漂移样本检测"></a>漂移样本检测</h3><p>漂移检测模型通过检测输入样本实现离群点的检测。</p><h4 id="对比学习实现特征隐表示"><a href="#对比学习实现特征隐表示" class="headerlink" title="对比学习实现特征隐表示"></a>对比学习实现特征隐表示</h4><p>我们探讨了对比学习的思想，以学习训练数据的良好表示。对比学习利用训练数据中的现有标签来学习一个有效的距离函数，以度量不同样本之间的相似性（或对比）。与监督分类器不同，对比学习的目标不是将样本分类到已知类别，而是学习如何比较两个样本。</p><p>如图二所示，对比学习的目的是将输入映射到低维的隐空间。经过映射后，同一类的样本距离更小，不同类的样本距离更大。因此，隐空间中的距离度量可以反映出样本对的差异。任何与所有现有类别表现出较大距离的新样本都有可能是漂移样本。</p><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231011175350.png" alt="img"></p><p>为了实现这个想法，我们使用了一个带有对比损失的自动编码器。自编码器是学习给定输入分布的压缩表示(具有降低的维度)的有用工具。形式上，让$x\in\mathbb{R}^{1\times 1}$是训练集中的一个样本。我们训练一个包含编码器$f$和解码器$h$的自动编码器。$f,h$的参数分别为$\Theta,\Phi$。损失函数如下：</p><script type="math/tex; mode=display">\min_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathbb{E}_{\boldsymbol{x}}\|\boldsymbol{x}-\hat{\boldsymbol{x}}\|_2^2+\lambda\mathbb{E}_{\boldsymbol{x}_i,\boldsymbol{x}_j}\left[(1-y_{ij})d_{ij}^2+y_{ij}(\boldsymbol{m}-d_{ij})_+^2\right]</script><p>该损失函数包含两项：</p><ol><li>重建损失$\mathbb{E}_{\boldsymbol{x}}|\boldsymbol{x}-\hat{\boldsymbol{x}}|_2^2$。即自动编码器对原样本进行重建后得到的样本与原样本距离的二范数。在这个自动编码器中，编码器$f$将$x$映射到一个低维向量$z=f(x,\Theta)$。自动编码器保证这个向量可以以较小的重建损失重建这个样本。</li><li>对比损失$\lambda\mathbb{E}_{\boldsymbol{x}_i,\boldsymbol{x}_j}\left[(1-y_{ij})d_{ij}^2+y_{ij}(\boldsymbol{m}-d_{ij})_+^2\right]$。$(x_i,x_j)$是一个样本对。$y_{i,j}$表示两个样本之间的关系。如果两个样本来自不同的类别，则值为1，否则为0。$(.)_+$是$max(0,.)$的缩写。$d_{i,j}$是样本隐表示$z_i=(x_i;\Theta),z_j=f(x_j;\Theta)$之间的欧几里得距离。$z\in \mathbb{R}^{d\times 1}$。使用该损失，模型会最小化统一类别的样本距离，同时最大化不同类别之间的样本距离（这个距离最大被限制到m）。</li></ol><p>经过对比学习后，$f$可以把相同类别的样本映射为一个较紧的样本簇。在隐空间中使用距离函数就可以把漂移样本检测出来。</p><h4 id="基于MAD的漂移样本检测"><a href="#基于MAD的漂移样本检测" class="headerlink" title="基于MAD的漂移样本检测"></a>基于MAD的漂移样本检测</h4><p>在训练好自动编码器之后，我们就可以使用它来检测漂移样本了。给定$K$个测试样本$\{x_t^{(k)}\}（k=1,\dots,K)$,我们寻求一种方法可以找到这些样本中的漂移样本。检测方法如算法1所示。</p><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231011191936.png" alt="img"></p><p>假设训练集有$N$个类别，每个类别有$n_i$个训练样本。我们首先把所有的训练样本映射到隐空间中(2-4行)。对于每个类别$i$，我们可以计算出一个中心$c_i$（就是直接取样本均值）。对于待检测的测试样本$x_t^{(k)}$，我们同样需要把它映射到隐空间中得到$z_t^{(k)}$（第14行）。接着我们计算测试样本和每个中心之间的欧几里得距离$d_i^{(k)}$。我们基于这个距离判断测试样本是否为离群点。在这我们不使用离测试样本最近的样本点的原因是，训练集中的离群点会影响判断。</p><p>不同的类别有不同的紧凑程度，故对于不同的类别需要使用不同的阈值。我们没有为每个类别手动设定阈值，而是使用了一种叫做“绝对中位差（Median Absolute Deviation，MAD）“的方法。计算出某一类的中心后，需要计算该类每个样本与中心的距离。距离中位数记为$\tilde{d}_i$。则：</p><script type="math/tex; mode=display">MAD_i = b*median(|d_i^{(j)}-\tilde{d}_i|),j=1,\dots,n_i</script><p>基于$MAD_i$，我们就可以决定$d_i^{(k)}$是不是所有类别的离群点。MAD的优势是每个类别都有一个独立的距离阈值，而且这个阈值和每一类的分布相关。举例来说，如果一个类别更分散，则这个阈值会更大。</p><p>需要注意的是，当一个类没有足够的样本时，MAD可能会受到影响，因为它的中值可能容易受噪声影响。在我们的设计中，对比学习可以帮助缓解这个问题，因为每个类被映射到潜在空间中的一个紧凑区域，这有助于稳定中位数。</p><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><p>正如图一所示，研究人员或许需要对漂移样本进行进一步的研究来解释漂移的含义。在研究时间有限的情况下，对每个漂移样本进行排序是非常有必要的。我们使用了一个非常朴素的方法进行排序，即每个样本与最近类别中心的距离（这个距离由27行计算得到）。这让我们可以优先研究距离中心最远的样本。</p><blockquote><p>排序是否也要考虑不同类别之间的离散程度呢？按照我的理解，本文应该是在各个中心的周围选最远的漂移样本。也就是说n个类别就会有n个最远的漂移样本。</p></blockquote><h3 id="解释漂移样本"><a href="#解释漂移样本" class="headerlink" title="解释漂移样本"></a>解释漂移样本</h3><blockquote><p>前面论文中说本文关注的是两种概念漂移中的第一种，即本文中的概念漂移指的是出现了新的类别，而不是类内分布发生变化。但这里却尝试解释漂移样本为啥从某一个类别中漂出来了。按理说新类别中的样本应该和其它类别没啥关系。</p></blockquote><p>解释模型的目的是找到致使样本产生漂移的最重要的特征（重要指的是对漂移现象重要）。给定一个漂移样本$x_t$和离它最近的类别$y_t$，我们需要找出来一个使得$x_t$成为$y_t$离群点的特征的子集。为了实现这个目标，直觉上我们可以把问题转化为监督学习模型的可解释性。举个例子，我们可以把漂移检测器当作是一个分类器（二分类，漂移或者没漂），然后利用现有的对于分类器的解释手段来解释漂移现象。然而，由于利群空间的高度稀疏性，我们发现想让一个漂移样本跨国决策边界是很难的，从而导致解释失败。受此启发，我们针对漂移检测设计了一种新的解释方法。这种解释方法解释了漂移样本和类内样本之间的距离，而不是决策边界。下面，我们首先分析直觉上的想法，然后再解释我们的方法。</p><p>直觉上的方法：略</p><h4 id="我们的方法：基于距离的解释"><a href="#我们的方法：基于距离的解释" class="headerlink" title="我们的方法：基于距离的解释"></a>我们的方法：基于距离的解释</h4><p>与有监督分类器不同，漂移检测器的决策基于样本与类别中心的距离。于是我们的目标就是从原始特征中找到使概念漂移发生的（特征）子集。在这种设定下，我们不需要让$x_t$跨越决策边界。我们只需要在原始样本上添加扰动，并观察隐空间中样本的变化。</p><blockquote><p>在这种设定下，选取出来的特征似乎和编码器的映射方式有很大的关系。</p></blockquote><p>为了实现这种想法，我们需要设计一种扰动机制。大多数现存的机制都是为图像数据设计的。而我们需要一种既可以作用于连续数据，也可以作用于离散数据的扰动机制。为了满足这种要求，我们通过把$x_t$的特征修改为$x_{yt}^{(c)}$的对应特征从而实现扰动。$x_{yt}^{(c)}$为距离类别c最近的样本点。因此，我们的目的就是寻找哪些样本修改之后会对隐空间对应的样本点造成最大的影响。将特征修改为$x_{yt}^{(c)}$的特征也保证了样本点会大致向着类别中心靠近。</p><p>我们用$m\in \mathbb{R}^{q\times 1}$表示重要特征。$m$是一个掩码，值为1表示此处的特征被替换。 每一个$m_i$都可以以概率$p_i$从伯努利分布中采样。下面我们的问题就转化为了求解$p_{1:q}$使下面的式子最小：</p><script type="math/tex; mode=display">\begin{aligned}&\mathbb{E}_{\boldsymbol{m}\sim Q(\boldsymbol{p})}\|\hat{\boldsymbol{z}}_t-\boldsymbol{c}_{y_t}\|_2+\lambda_1R(\boldsymbol{m},\boldsymbol{b}), \\&\hat{z}_t=f(x_t\odot(1-m\odot b)+x_{y_t}^{(c)}\odot(m\odot b)), \\&R(m,b)=\|m\odot b\|_1+\|m\odot b\|_2,\quad Q(p)=\prod_{i=1}^qp(m_i|p_i)\end{aligned}</script><p>$\tilde{z}_t$是扰动样本的隐空间向量。$R(m,b)$控制了$m$中非0元素的个数。为了加快速度，用$b$表示一个pre-filter，如果$(x_t)_i=(x_{yt}^{(c)})_i$ ，则$(b)_i=0$。也就是不对 漂移样本和中心附近的样本相同的分量 做优化。</p><p>注意到伯努利分布是离散的，我们不能直接用梯度下降的方式解决这个问题，所以我们使用了伯努利分布的连续近似分布来进行求解。</p><h2 id="评估：漂移检测"><a href="#评估：漂移检测" class="headerlink" title="评估：漂移检测"></a>评估：漂移检测</h2><p>我们使用两个数据集进行评估</p><ol><li>Drebin：Android 恶意软件检测数据集。我们从中选取了8类样本，每一类样本至少有100条记录（总共3317个样本）。在实验之前，随机选取一类作为漂移样本，漂移样本只存在于测试集中。我们的目标是在测试阶段正确的识别出漂移样本。训练集和测试集按照时间顺序8-2分。通过特征筛选，最后得到了1340条特征。为了使实验结果更具有普适性，我们迭代的选择每一个类别作为漂移类并重复实验。</li></ol><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012165437.png" alt="img"></p><ol><li>网络入侵检测系统：CICIDS-2018。为了加速实验，我们选择了10%的流量作为实验数据集。其它数据集分割及实验方式和上述一致。</li></ol><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012165437.png" alt="img"></p><p>使用精准率、召回率、F1-score等作为评价指标。使用两种方法作为baseline：</p><ol><li>Vanilla autoencoder。用它来进行数据降维，使用本文的方法做漂移检测。用来说明对比学习的优势。</li><li>Transcend。Transcend定义了一个”非一致性测度”来量化传入样本与预测类别的吻合程度，并计算可信度p值来判断传入样本是否为漂移样本。</li></ol><h3 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h3><p>漂移样本检测性能。我们首先使用一个实验设置来解释我们的评估过程。以Drebin数据集为例。假设我们将Iconosys家族作为测试集中以前未见的家族。在训练检测模型（不包含任何Iconosys样本）之后，我们使用该模型来检测和排名漂移样本。为了评估排名列表的质量，我们模拟了一位分析师从列表顶部检查样本的过程。</p><p>图4a显示，当我们检查更多的漂移样本（最多150个样本）时，精确度保持在较高水平（超过0.97），而召回率逐渐达到100%。结合精确度和召回率，最高的F1得分为0.98。在150个样本之后，精确度将下降，因为剩余集合中没有更多未见类别的样本了。这证实了排名列表的高质量，意味着几乎所有来自未见家族的样本都排在了最前面。</p><p>与之相比，Transcend和Vanilla AE的排名列表并不令人满意。对于Transcend（图4b），前150个样本的精确度和召回率都很低，表明排名靠前的样本并不来自未见家族。在检查了150个样本之后，我们开始看到更多来自未见家族的样本。在检查了350个样本之后，Transcend已经涵盖了大部分来自未见家族的样本（即召回率接近1.0），但精确度仅为0.46。这意味着分析师检查的样本中超过一半与问题无关。最佳的F1得分为0.63。如图4c所示，Vanilla AE的性能更差。即使在检查了600个样本之后，召回率仅略高于0.8。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012182526.png" alt="img"></p><p>表3证实了CADE能够准确检测漂移样本，并且优于两个基准模型。在Drebin数据集上，CADE的平均F1得分为0.96，而基准模型的F1得分分别为0.80和0.72。对于IDS2018数据集，可以得出类似的结论。此外，CADE的标准差要小得多，表明在不同的实验设置中具有更一致的性能。最后，我们展示了CADE具有较低的归一化检查工作量，这证实了排名的高质量。<br>请注意，Transcend基准模型在某些情况下确实表现良好。例如，在IDS2018数据集中将DoS-Hulk设置为未见家族时，其F1得分为99.69%（与我们的系统类似）。然而，问题在于Transcend在不同的设置中表现不稳定，这在表3中的高标准差中得到了体现。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012183835.png" alt="img"></p><p>对对比学习的影响。为了了解性能提升的来源，我们研究了对比学习的影响。首先，我们在图7中呈现了Drebin数据集的训练样本和来自选择的未见家族（FakeDoc）的测试样本的t-SNE图。t-SNE  是一种非线性降维技术，可以将数据样本投影到二维图中进行可视化。为了可视化我们的数据样本，我们将样本从原始空间（1,340维）映射到二维空间（图7a）。同时，我们还将样本从潜空间（7维）映射到二维空间进行比较（图7b和图7c）。我们可以观察到，CADE的潜空间中的样本形成了更紧密的聚类，这使得更容易将现有样本与未见家族区分开来。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012182204.png" alt="img"></p><p>为了从统计的角度观察不同的实验设置，我们绘制了图8。与之前类似，我们迭代地将一个家族作为Drebin数据集中的未见家族。然后，我们测量测试样本到其在原始特征空间中最近质心的距离（图8a），以及CADE生成的潜空间中的距离（图8b）。IDS2018数据集的结果得出了相同的结论，为了简洁起见，省略了在此展示。我们展示了漂移样本和非漂移样本在原始空间中更难分离。经过对比学习后，潜空间中的分离更加明显。原因在于对比学习学习到了一个适合的距离函数，可以将不同类别的样本拉得更远，从而更容易检测到未见家族。</p><p><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012184117.png" alt="img"></p><p>案例研究：CADE的局限性。在大多数设置中，CADE的表现良好。然而，我们发现在某些情况下，CADE的性能受到影响。例如，当将FakeInstaller作为未见家族时，我们的检测精确度仅为82%，而召回率达到100%。我们注意到，许多来自GingerMaster和Plankton家族的测试样本被检测为漂移样本。经过仔细检查，我们发现，当将FakeInstaller作为未见家族时，为了保持整体的80:20的训练-测试比例，在GingerMaster和Plankton家族尚没有足够的训练样本时，我们需要在这个时间点拆分数据集。因此，许多来自GingerMaster和Plankton家族的测试样本与这两个家族中少量训练样本（基于潜空间距离）非常不同。外部证据也表明，这两个家族有许多变种[5, 70]。虽然这些恶意软件变种不属于新的家族（根据我们的定义是误报），但它们对于研究理解同一家族内的恶意软件变异也具有价值。</p><h2 id="评估：解释漂移样本"><a href="#评估：解释漂移样本" class="headerlink" title="评估：解释漂移样本"></a>评估：解释漂移样本</h2><p>为了评估解释模块，在每个数据集中随机选择一个类（例如，在Drebin数据集中选择FakeDoc，在IDS2018数据集中选择Infiltration）作为漂移样本。其他设置的结果得出相同的结论，为了简洁起见，省略了这些结果。在这个设置下，我们为检测到的漂移样本生成解释，并对解释结果进行定量和定性评估。</p><p>使用三种方法作为baseline：</p><ol><li>random。随机选择特征作为重要特征</li><li>基于边界。如前文所说</li><li>无监督的解释方法COIN。COIN构建了一组本地的LinearSVM分类器，用于将一个个体离群值与其在分布邻域样本中进行分离。由于LinearSVM分类器是自解释的，它们可以指出对离群值分类起重要作用的特征。为了进行公平比较，我们选择了与我们的方法相同数量的顶部特征作为基线方法。这些基线方法的实现和超参数可以在附录B中找到。请注意，我们没有选择现有的黑盒解释方法（例如，LIME 和SHAP）作为我们的比较基线。这是因为白盒方法通常比黑盒方法表现更好，这要归功于它们对原始模型的访问权限。</li></ol><p>评估指标：定量的情况下，我们直接评估选择特征带来的距离变化。给定一个测试样本$x_t$和解释方法，我们可以得到一个特征子集$m_t$。我们通过这个指标来量化这个解释结果的精确性：$d_{xt}’=||f(x_t\odot(1-m_t)+x_{yt}^{(c)}\odot m_t)-c_{yt}||_2$。这个值表示了隐空间中扰动样本和它最近的样本中心之间的距离。如果这个特征真的是重要的，那么扰动样本和最近的样本之间的距离会变小。</p><p>除了这个$d_{xt}’$指标之外，我们还使用传统的度量指标（第5.2节）来检查能够穿越决策边界的扰动样本的比例。</p><h3 id="精确性评价结果"><a href="#精确性评价结果" class="headerlink" title="精确性评价结果"></a>精确性评价结果</h3><p>特征对距离的影响。表4显示了所有漂移样本的$d_{xt}’$的均值和标准差（即扰动样本到最近质心之间的距离）。我们有四个关键观察结果。首先，基于随机选择的特征扰动漂移样本几乎不会影响潜空间距离（比较行2和行3）。其次，基于边界的解释方法可以在两个数据集上降低26%–47%的距离（比较行2和行4）。这表明这种策略具有一定的有效性。然而，绝对距离值仍然很高。第三，COIN减小了IDS2018数据集中的潜空间距离（比较行2和行5），但在Drebin数据集中平均距离却有所增加。实质上，COIN是一种专门的基于边界的方法，它使用一组LinearSVM分类器来逼近决策边界。我们发现COIN在高维空间上效果不好，并且很难将漂移样本拉过边界（将在第5.3节讨论）。最后，我们在CADE中的解释模块具有最低的距离度量均值和标准差。距离与原始距离相比显著降低（即在Drebin上降低了98.8%，在IDS2018上降低了79.9%，比较行2和行6）。特别是，CADE在边界解释方法方面表现出色。由于我们的方法克服了样本稀疏性和不平衡的问题，它能够准确指出对距离（影响漂移检测决策）有更大影响的有效特征。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012194227.png" alt="img"></p><p>选择的特征数量。总体而言，选择的特征数量较少，这使得手动解释成为可能。如前所述，我们将所有方法配置为选择相同数量的重要特征（与CADE相同）。对于Drebin数据集，平均选择的特征数量为44.7，标准差为6.2。这在1000多个特征中只占很小一部分（3%）。类似地，IDS2018数据集的平均选择特征数量为16.2，约占所有特征的20%。</p><h3 id="跨越决策边界"><a href="#跨越决策边界" class="headerlink" title="跨越决策边界"></a>跨越决策边界</h3><p>上述评估结果证实了所选特征对距离度量的影响，而这恰恰是CADE旨在优化的内容。为了提供另一个视角，我们进一步研究了所选特征对穿越决策边界的影响。具体而言，我们计算了成功穿越决策边界的扰动样本的比例。如表6所示，我们确认在漂移检测的上下文中，大多数情况下穿越边界是困难的。特别是，CADE能够将97.64%的扰动样本推动穿越Drebin数据集的检测边界，但只有1.41%的样本能够穿越IDS2018数据集的边界。相比之下，基线方法在原始特征空间中很少能够成功扰动漂移样本使其穿越边界。通过放宽这个条件并专注于距离变化，我们的方法在确定重要特征方面更加有效。</p><h3 id="样例学习"><a href="#样例学习" class="headerlink" title="样例学习"></a>样例学习</h3><p>为了证明我们的方法确实捕捉到了有意义的特征，我们展示了一些案例研究。在表5中，我们展示了Drebin数据集的一个案例研究。我们选择了FakeDoc是未见过的家族，并随机选择一个漂移样本来运行解释模块。在1000多个特征中，我们的解释模块确定了42个重要特征，其中27个特征的值为“1”（表示该样本包含这些特征）。如表5所示，最接近的家族是GingerMaster。</p><p>我们手动检查这些特征，以确定它们是否具有正确的语义含义。虽然很难获取“地面真相”的解释，但我们收集了有关FakeDoc恶意软件和GingerMaster的外部分析报告[68, 70]。根据这些报告，与GingerMaster相比，FakeDoc恶意软件的一个关键区别是它通常通过短信订阅高级服务并向受害用户收费。如表5所示，许多选定的特征与读取、写入和发送短信的权限和API调用有关。我们突出显示了与短信相关功能匹配的这些特征。其他相关特征也被突出显示。例如，“RESTART_PACKAGES”权限允许恶意软件终止后台进程（例如显示传入短信的进程），以避免提醒用户。权限“DISABLE_KEYGUARD”允许恶意软件在不解锁屏幕的情况下发送高级短信消息。“WRITE_SETTINGS”有助于偷偷写入系统设置以发送短信。”<a href="https://ws.tapjoyads.com/">https://ws.tapjoyads.com/</a>“ 是FakeDoc通常使用的广告库。再次强调，这小部分特征是从1000多个特征中选择出来的。我们得出结论，这些特征高度指示了该样本与最近的已知家族有何不同</p><h2 id="评估：类内漂移"><a href="#评估：类内漂移" class="headerlink" title="评估：类内漂移"></a>评估：类内漂移</h2><p>到目前为止，我们的评估主要集中在一种概念漂移类型（类型A），即漂移样本来自以前未见过的家族。接下来，我们探索如何调整我们的解决方案以应对另一种概念漂移类型（类型B），即漂移样本来自现有类别。我们在一个二元分类设置下进行了一项简要的实验。</p><p>更具体地说，我们首先使用Drebin数据集训练了一个二元SVM分类器，用于将恶意样本与良性样本进行分类。该分类器在Drebin数据集上表现出很高的准确性，训练时的F1得分为0.99。我们想测试该分类器在另一个名为Marvin 的Android恶意软件数据集上的表现。相对于Drebin数据集（2010年至2012年），Marvin数据集略微更新（2010年至2014年）。我们首先移除Marvin数据集中与Drebin重叠的样本，以确保Marvin样本是真正的先前未见过的。这样，Marvin数据集中剩下了9,592个良性样本和9,179个恶意样本。</p><p>在这个实验中，我们将Marvin数据集随机分成验证集和测试集（50:50）。对于这两个集合，我们保持恶意样本和良性样本的平衡比例。我们将原始的分类器（在Drebin数据上训练）应用于Marvin的测试集。我们发现由于可能存在恶意类别和/或良性类别内的评估问题，测试准确率不再很高（F1得分为0.70）。</p><p>为了解决类内漂移的问题，我们在Marvin的验证集上应用CADE和Transcend来识别少量的漂移样本（它们可以是良性或恶意的）。我们通过使用它们的“真实标签”来模拟对这些样本进行标记，然后将这些带有标签的漂移样本添加回Drebin的训练数据中，重新训练二元分类器。最后，我们在Marvin的测试集上测试重新训练的分类器。</p><p>根据表7的数据，我们发现CADE仍然明显优于Transcend。例如，通过添加仅150个漂移样本（占Marvin验证集的1.7%）进行重新训练，CADE将二元分类器的F1得分提升至0.92。而对于Transcend来说，相同数量的样本只能将F1得分提升至0.74。此外，我们还发现CADE的速度更快：CADE的运行时间为1.2小时（而Transcend需要10小时）。这个实验证实了CADE可以适应处理二元恶意软件分类器的类内演化问题。<br><img src="/2023/10/12/cade-detecting-and-explaining-concept-drift-samples-for-security-applications-yue-du-bi-ji/20231012204614.png" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 概念漂移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码执行日志记录</title>
      <link href="/2023/10/05/dai-ma-zhi-xing-ri-zhi-ji-lu/"/>
      <url>/2023/10/05/dai-ma-zhi-xing-ri-zhi-ji-lu/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>忘了在哪个开源项目里看到了一个执行日志系统。它会在每次运行时备份代码。对于我这种随时会忘掉实验结果和实验设置的人来说，这种东西应该会很有用……吧。凭印象和自己的喜好复现了一下，具体如下。</p><p>首先，项目应该保持如下的结构。code路径下存放所有的代码，任何数据和中间运行结果都<strong>不</strong>应该放在这里。log目录下存放所有的日志记录。其它的目录根据自己喜好设置。<br><img src="/2023/10/05/dai-ma-zhi-xing-ri-zhi-ji-lu/path.png" alt="代码路径"></p><p>在code/util路径下，新建一个code_logger.py。粘贴如下内容。该日志模块下集成了代码备份、参数保存、日志输出与保存的功能。</p><pre class=" language-lang-python"><code class="language-lang-python">import osfrom datetime import datetimeimport shutilimport jsonimport loggingclass CodeLogger:    def __init__(self, comment=None, args=None):        self.run_time = datetime.now()        self.time_fmt = "%Y-%m-%d-%H-%M-%S"        # self.time_fmt = "%Y%m%d%H%M%S"        self.comment = comment        self.log_dir = self.run_time.strftime(self.time_fmt)        if comment is not None:            self.log_dir = f"{self.log_dir}-{comment}"        self.log_dir = os.path.join("../log", self.log_dir)        self.args = args        # create log dir if not exists        if not os.path.exists(self.log_dir):            os.mkdir(self.log_dir)        # back up codes        shutil.copytree("../code/", os.path.join(self.log_dir, "code/"))        if self.args is not None:            self.save_args(args)        self.logger = self.init_logger()    def save_args(self, args):        with open(os.path.join(self.log_dir, "args.json"), "w") as f:            json.dump(args.__dict__, f)    def init_logger(self):        logger = logging.getLogger(self.comment)        logger.setLevel(logging.INFO)        log_format = logging.Formatter(            "%(asctime)s %(levelname)-8s %(message)s")        filename = os.path.join(self.log_dir, "run.log")        log_handler = logging.FileHandler(filename, mode="w")        log_handler.setLevel(logging.INFO)        log_handler.setFormatter(log_format)        logger.addHandler(log_handler)        console_logger = logging.StreamHandler()        console_logger.setLevel(logging.INFO)        console_logger.setFormatter(log_format)        logger.addHandler(console_logger)        return logger</code></pre><p>在使用该模块时，需要先import该模块。演示如下：</p><pre class=" language-lang-python"><code class="language-lang-python">from util.models import get_architecturefrom util.code_logger import CodeLoggerimport argparseparser = argparse.ArgumentParser()parser.add_argument("--model", type=str, default="resnet")parser.add_argument("--dataset", type=str, default="cifar10")parser.add_argument("--epochs", type=int, default=150)parser.add_argument("--batch_size", type=int, default=256)parser.add_argument("--lr", type=float, default=0.01)parser.add_argument("--verbose", action="store_true")args = parser.parse_args()model_name = args.modeldataset = args.datasetepochs = args.epochsbatch_size = args.batch_sizelr = args.lrverbose = args.verbosecl = CodeLogger(comment=f"train model {model_name}_{dataset}", args=args)''' 上面是日志模块初始化，初始化会做三件事：1. 在log文件夹下新建一个文件夹，文件夹名称为“当前时间-comment内容”。2. 把code路径下所有文件保存到新建的文件夹中3. 把运行参数args存放到新建文件夹的args.json中'''cl.logger.info(f"test output") # 控制台输出，同时会把输出内容保存到新建文件夹下的run.log中</code></pre><p>如果再发生没有记录实验结果，或者忘掉实验设置时，就可以通过查找log目录下的代码备份或者参数备份来一键找回了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 日志系统 </tag>
            
            <tag> 执行日志 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023安全顶会AI安全相关汇总</title>
      <link href="/2023/09/06/2023-an-quan-ding-hui-ai-an-quan-xiang-guan-hui-zong/"/>
      <url>/2023/09/06/2023-an-quan-ding-hui-ai-an-quan-xiang-guan-hui-zong/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="NDSS"><a href="#NDSS" class="headerlink" title="NDSS"></a>NDSS</h2><h3 id="对抗样本防御"><a href="#对抗样本防御" class="headerlink" title="对抗样本防御"></a>对抗样本防御</h3><ol><li><a href="https://www.ndss-symposium.org/ndss-paper/adversarial-robustness-for-tabular-data-through-cost-and-utility-awareness/">Adversarial Robustness for Tabular Data through Cost and Utility Awareness</a></li><li><a href="https://www.ndss-symposium.org/ndss-paper/bars-local-robustness-certification-for-deep-learning-based-traffic-analysis-systems/">BARS: Local Robustness Certification for Deep Learning based Traffic Analysis Systems</a>    流量相关</li></ol><h3 id="后门攻击"><a href="#后门攻击" class="headerlink" title="后门攻击"></a>后门攻击</h3><ol><li><a href="https://www.ndss-symposium.org/ndss-paper/backdoor-attacks-against-dataset-distillation/">Backdoor Attacks Against Dataset Distillation</a></li></ol><h2 id="S-amp-P"><a href="#S-amp-P" class="headerlink" title="S&amp;P"></a>S&amp;P</h2><h3 id="AI伦理"><a href="#AI伦理" class="headerlink" title="AI伦理"></a>AI伦理</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179333">How technical do you get? I’m an English teacher”: Teaching and Learning Cybersecurity and AI Ethics in High School</a></li></ol><h3 id="AI-amp-差分隐私"><a href="#AI-amp-差分隐私" class="headerlink" title="AI&amp;差分隐私"></a>AI&amp;差分隐私</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179409">A Theory to Instruct Differentially-Private Learning via Clipping Bias Reduction</a></li><li><a href="https://ieeexplore.ieee.org/document/10179466">Continual Observation under User-level Differential Privacy</a></li><li><a href="https://ieeexplore.ieee.org/document/10179389">Locally Differentially Private Frequency Estimation Based on Convolution Framework</a></li><li><a href="Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering">Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering</a></li></ol><h3 id="对抗样本攻击"><a href="#对抗样本攻击" class="headerlink" title="对抗样本攻击"></a>对抗样本攻击</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179473">AI-Guardian: Defeating Adversarial Attacks using Backdoors</a></li><li><a href="https://ieeexplore.ieee.org/document/10179303">SoK: Certified Robustness for Deep Neural Networks</a></li></ol><h3 id="后门攻击-1"><a href="#后门攻击-1" class="headerlink" title="后门攻击"></a>后门攻击</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179375">Redeem Myself: Purifying Backdoors in Deep Learning Models using Self Attention Distillation</a></li><li><a href="https://ieeexplore.ieee.org/document/10179308">Disguising Attacks with Explanation-Aware Backdoors</a></li></ol><h3 id="推理攻击"><a href="#推理攻击" class="headerlink" title="推理攻击"></a>推理攻击</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179334">SNAP: Efficient Extraction of Private Properties with Poisoning</a></li><li><a href="https://ieeexplore.ieee.org/document/10179463">Accuracy-Privacy Trade-off in Deep Ensemble: A Membership Inference Perspective</a></li><li><a href="https://ieeexplore.ieee.org/document/10179281">SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</a></li></ol><h3 id="模型萃取攻击"><a href="#模型萃取攻击" class="headerlink" title="模型萃取攻击"></a>模型萃取攻击</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179406">D-DAE: Defense-Penetrating Model Extraction Attacks</a></li></ol><h3 id="机器学习可解释性"><a href="#机器学习可解释性" class="headerlink" title="机器学习可解释性"></a>机器学习可解释性</h3><ol><li><a href="https://ieeexplore.ieee.org/document/10179321">Everybody’s Got ML, Tell Me What Else You Have: Practitioners’ Perception of ML-Based Security Tools and Explanations</a></li></ol><h2 id="USENIX"><a href="#USENIX" class="headerlink" title="USENIX"></a>USENIX</h2><h3 id="对抗样本攻击-1"><a href="#对抗样本攻击-1" class="headerlink" title="对抗样本攻击"></a>对抗样本攻击</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/wu-xinghui">KENKU: Towards Efficient and Stealthy Black-box Adversarial Attacks against ASR Systems</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/bethany">Towards Targeted Obfuscation of Adversarial Unsafe Images using Reconstruction and Counterfactual Super Region Attribution Explainability</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhu">TPatch: A Triggered Physical Adversarial Patch</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-shibo">CAPatch: Physical Adversarial Patch against Image Captioning Systems</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/tao">Hard-label Black-box Universal Adversarial Patch Attack</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/sheatsley">The Space of Adversarial Strategies</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/liu-aishan">X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yu-zhiyuan-smack">SMACK: Semantically Meaningful Adversarial Audio Attack</a> audio</li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/eykholt">URET: Universal Robustness Evaluation Toolkit (for Evasion)</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yuan-yuanyuan-certification">Precise and Generalized Robustness Certification for Neural Networks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-jiawei">DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing</a></li></ol><h3 id="成员推理攻击"><a href="#成员推理攻击" class="headerlink" title="成员推理攻击"></a>成员推理攻击</h3><h3 id="后门攻击-2"><a href="#后门攻击-2" class="headerlink" title="后门攻击"></a>后门攻击</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/qi">Towards A Proactive ML Approach for Detecting Backdoor Poison Samples</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-zhuo-pelican">PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis</a> 二进制代码分析自动发现后门</li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/lv">A Data-free Backdoor Injection Approach in Neural Networks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/tian">Sparsity Brings Vulnerabilities: Exploring New Metrics in Backdoor Attacks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/wei-chengan">Aliasing Backdoor Attacks on Pre-trained Models</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/pan">ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/bai">VILLAIN: Backdoor Attacks Against Vertical Split Learning</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/fu-chong">FreeEagle: Detecting Complex Neural Trojans in Data-Free Cases</a></li></ol><h3 id="投毒攻击"><a href="#投毒攻击" class="headerlink" title="投毒攻击"></a>投毒攻击</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zeng">Meta-Sift: How to Sift Out a Clean Subset in the Presence of Data Poisoning?</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/li-xiaoguang">Fine-grained Poisoning Attack to Local Differential Privacy Protocols for Mean and Variance Estimation</a></li></ol><h3 id="比特翻转-bit-flip-攻击"><a href="#比特翻转-bit-flip-攻击" class="headerlink" title="比特翻转(bit-flip)攻击"></a>比特翻转(bit-flip)攻击</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/wang-jialai">Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/liu-qi">NeuroPots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks</a></li></ol><h3 id="差分隐私"><a href="#差分隐私" class="headerlink" title="差分隐私"></a>差分隐私</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/nanayakkara">What Are the Chances? Explaining the Epsilon Parameter in Differential Privacy</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/nasr">Tight Auditing of Differentially Private Machine Learning</a></li></ol><h3 id="模型水印"><a href="#模型水印" class="headerlink" title="模型水印"></a>模型水印</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yan">Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation</a></li></ol><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><ol><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/niu">CodexLeaks: Privacy Leaks from Code Generation Language Models in GitHub Copilot</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/christou">IvySyn: Automated Vulnerability Discovery in Deep Learning Frameworks</a></li><li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/mink">“Security is not my field, I’m a stats guy”: A Qualitative Root Cause Analysis of Barriers to Adversarial Machine Learning Defenses in Industry</a>  对抗训练相关</li></ol><h2 id="CCS"><a href="#CCS" class="headerlink" title="CCS"></a>CCS</h2><h3 id="后门攻击-3"><a href="#后门攻击-3" class="headerlink" title="后门攻击"></a>后门攻击</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616617">Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information</a></li></ol><h3 id="模型窃取"><a href="#模型窃取" class="headerlink" title="模型窃取"></a>模型窃取</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616652">Stealing the Decoding Algorithms of Language Models</a>语言模型的解码算法、超参数窃取</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616653">Stolen Risks of Models with Security Properties</a> 强化学习模型隐私风险验证</li></ol><h3 id="差分隐私-amp-机器学习"><a href="#差分隐私-amp-机器学习" class="headerlink" title="差分隐私&amp;机器学习"></a>差分隐私&amp;机器学习</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616593">DPMLBench: Holistic Evaluation of Differentially Private Machine Learning</a></li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623142">Geometry of Sensitivity: Twice Sampling and Hybrid Clipping in Differential Privacy with Optimal Gaussian Noise and Application to Deep Learning</a></li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623165">Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation</a> 图神经网络</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616592">DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass</a> 语言模型</li></ol><h3 id="其它-1"><a href="#其它-1" class="headerlink" title="其它"></a>其它</h3><ol><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623116">Stateful Defenses for Machine Learning Models Are Not Yet Secure Against Black-box Attacks</a> 黑盒攻击增强策略</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623069">Prediction Privacy in Distributed Multi-Exit Neural Networks: Vulnerabilities and Solutions</a></li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623173">Devil in Disguise: Breaching Graph Neural Networks Privacy through Infiltration</a> 对图神经网络的攻击</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623189">Evading Watermark based Detection of AI-Generated Content</a> 生成模型水印检测规避</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616681">Interactive Proofs For Differentially Private Counting</a> 交互式差分隐私证明（可能没有AI相关的内容）</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623076">SalsaPicante: A Machine Learning Attack on LWE with Binary Secrets</a> 用机器学习攻击量子密码系统</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623117">Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting</a> 攻击恶意软件检测模型</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623130">“Get in Researchers; We’re Measuring Reproducibility”: A Reproducibility Study of Machine Learning Papers in Tier 1 Security Conferences</a> 论文可复现性检查</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3616588">DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models</a> 生成图像检测</li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623177">Attack Some while Protecting Others: Selective Attack Strategies for Attacking and Protecting Multiple Concepts</a> </li><li><a href="https://dl.acm.org/doi/10.1145/3576915.3623093">Unforgeability in Stochastic Gradient Descent</a> SGD执行的可伪造性</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 顶会 </tag>
            
            <tag> 汇总 </tag>
            
            <tag> AI安全 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SplineCam 阅读笔记</title>
      <link href="/2023/08/07/splinecam/"/>
      <url>/2023/08/07/splinecam/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="SplineCam-Exact-Visualization-and-Characterization-of-Deep-Network-Geometry-and-Decision-Boundaries"><a href="#SplineCam-Exact-Visualization-and-Characterization-of-Deep-Network-Geometry-and-Decision-Boundaries" class="headerlink" title="SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries"></a>SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries</h1><h2 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h2><ul><li><strong>Type</strong>: Journal Article</li><li><strong>Title</strong>: SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries, </li><li><strong>Author</strong>: Humayun, Ahmed Imtiaz; Balestriero, Randall; Balakrishnan, Guha; Baraniuk, Richard G, </li></ul><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Current Deep Network (DN) visualization and interpretability methods rely heavily on data space visualizations such as scoring which dimensions of the data are responsible for their associated prediction or generating new data features or samples that best match a given DN unit or representation. In this paper, we go one step further by developing the first provably exact method for computing the geometry of a DN’s mapping – including its decision boundary – over a specified region of the data space. By leveraging the theory of Continuous Piece-Wise Linear (CPWL) spline DNs, SplineCam exactly computes a DN’s geometry without resorting to approximations such as sampling or architecture simplification. SplineCam applies to any DN architecture based on CPWL activation nonlinearities, including (leaky) ReLU, absolute value, maxout, and maxpooling and can also be applied to regression DNs such as implicit neural representations. Beyond decision boundary visualization and characterization, SplineCam enables one to compare architectures, measure generalizability, and sample from the decision boundary on or off the data manifold. Project website: bit.ly/splinecam.</p><p>当前深度网络(DN)的可视化和解释方法主要依赖于数据空间的可视化，例如评估哪些数据维度对其相关预测负责，或生成与给定的DN单元或表示最匹配的新数据特征或样本。在本文中，我们进一步发展了第一个可证明的方法，用于计算在数据空间的特定区域内，DN的映射几何结构，包括其决策边界。通过利用连续分段线性(CPWL)样条DN的理论，SplineCam可以精确计算DN的几何结构，而无需采用采样或架构简化等近似方法。SplineCam适用于基于CPWL激活非线性的任何DN架构，包括（leaky）ReLU、绝对值、maxout和最大池化，并且还可以应用于回归DN，例如隐式神经表示。除了决策边界的可视化和特征描述外，SplineCam还可以用于比较架构、测量泛化能力，并从数据流形上或离数据流形上的决策边界中进行抽样。项目网站：bit.ly/splinecam。</p><hr><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>在本文中，我们专注于使用连续分段线性(CPWL)激活函数$\sigma$的深度网络(DNs)，例如(leaky-)ReLU、绝对值和最大池化。在这种设置下，整个深度网络本身成为一个CPWL算子，即在其定义域的划分区域内，其映射是仿射的。之前的研究专注于估计这类CPWL DNs的划分，并将实证结果与可解释性相结合。例如，Raghu等人[34]表明划分密度提供了DN表现能力的度量，Hanin等人[13]将DN划分密度与学习函数的复杂性联系起来，Jordan等人[20]近似DN的划分以提供稳健性证书，Zhang等人[43]解释了与DN划分相关的dropout影响，Balestriero等人[3]提出改进批归一化以进一步适应数据几何的DN划分，Humayun等人[17, 18]提出通过近似DN划分来控制预训练生成网络的输出分布，Chen等人[25]提出了一种基于划分统计的神经网络架构搜索方法。尽管这些方法都取得了成功，但它们都依赖于DN划分的近似。</p><p>我们提出了SplineCam，一种无需采样的方法来精确计算DN的划分。我们的方法在输入空间的二维域上计算划分，可以轻松适应DN的宽度和深度，可以处理卷积层和跳跃连接，并且可以扩展到发现大量的区域，与之前的方法相比具有更高的可扩展性。我们的方法还允许基于划分统计对输入空间进行局部特征描述，并且能够有效地采样任意多个样本，这些样本可以证明位于DN的决策边界上 - 为可视化和解释提供了新的途径。我们总结我们的贡献如下：</p><ul><li>我们开发了一种可扩展的枚举方法，它可以在给定有界的DN输入空间的二维域的情况下，计算DN的输入空间划分（也称为线性区域）和决策边界。</li><li>我们开发了SplineCam，利用我们新的枚举方法直接可视化DN的输入空间划分，计算划分统计量，并从决策边界进行采样。</li><li>我们进行了定量分析，证明了SplineCam对于表征DN、比较架构选择和训练方式的能力。我们展示了SplineCam在不同深度网络架构和训练策略下的性能，并通过具体指标和对比实验来量化其优势。这些结果证明了SplineCam作为一种强大的工具，可以帮助深度学习研究人员更好地理解和分析深度网络的性质和性能。<h2 id="The-Exact-Geometry-and-Decision-Boundary-of-Continuous-Piece-Wise-Linear-Deep-Networks"><a href="#The-Exact-Geometry-and-Decision-Boundary-of-Continuous-Piece-Wise-Linear-Deep-Networks" class="headerlink" title="The Exact Geometry and Decision Boundary of Continuous Piece-Wise Linear Deep Networks"></a>The Exact Geometry and Decision Boundary of Continuous Piece-Wise Linear Deep Networks</h2></li></ul><h3 id="Deep-Networks-as-Continuous-Piece-Wise-Linear-Operators"><a href="#Deep-Networks-as-Continuous-Piece-Wise-Linear-Operators" class="headerlink" title="Deep Networks as Continuous Piece-Wise Linear Operators"></a>Deep Networks as Continuous Piece-Wise Linear Operators</h3><p>非线性函数最基本的函数形式之一来自于多项式，尤其是样条运算符。一般来说，样条是一种映射，在输入空间分区Ω的每个区域ω上具有局部度为D的多项式，并且有额外的约束条件，即这些多项式的前D-1阶导数在整个定义域上连续，也就是在从一个区域到其任何相邻区域时施加了平滑性约束。更正式地说，在深度网络的背景下，我们将特别关注仿射样条，即具有D = 1且仅约束在整个定义域上保持连续性的样条运算符。</p><p><strong>引理一</strong>：深度网络的第1层到$l$层的复合,记为$S^l$，其输出空间为$\mathbb R^l$，可以表示为：</p><script type="math/tex; mode=display">S^{\ell}(x)=\sum_{\omega\in\Omega}\left(A_{\omega}^{\ell}x+b_{\omega}^{\ell}\right)\mathbb{1}_{\{x\in\omega\}},</script><p>其中</p><script type="math/tex; mode=display">\begin{aligned}&A_{u}^{\ell}&& =\prod_{i=1}^{\ell}\operatorname{diag}\left(\boldsymbol{q}_{\omega}^{i}\right)\boldsymbol{W}^{i},\\&b_{\omega}^{\ell}=&& \mathrm{diag}\left(\boldsymbol{q}_{\omega}^{\ell}\right)\boldsymbol{b}^{\ell}+\sum_{i=1}^{\ell-1}\Bigg(\prod_{j=i+1}^{\ell}\mathrm{diag}\left(\boldsymbol{q}_{\omega}^{j}\right)\boldsymbol{W}^{j}\Bigg)\mathrm{diag}\left(\boldsymbol{q}_{\omega}^{i}\right)\boldsymbol{b}^{i}. \end{aligned}</script><p>其中，$q_\omega^l$是激活函数$\sigma$在预激活$W^lz^{l-1}+b^l$处的逐点导数。根据Balestriero等人的定理，$q_\omega^l$对于任意一个区域$\omega \in\Omega$是唯一的。<br>这种对DN的表达方式之前已经被用于使理论研究适用于实际DN而无需任何简化，同时利用了关于样条理论的丰富文献，例如逼近理论、最优控制、统计学及相关领域。通过这种方式，可以更好地理解和分析DN的性质，并将理论研究与实际应用相结合。</p><h3 id="Exact-Computation-of-Their-Partition-and-Decision-Boundary"><a href="#Exact-Computation-of-Their-Partition-and-Decision-Boundary" class="headerlink" title="Exact Computation of Their Partition and Decision Boundary"></a>Exact Computation of Their Partition and Decision Boundary</h3><p>假设$w_i^l$和$b_i^l$分别$W^l,b^l$的第i行。通过下面的引理2可以把第$l$层输入空间$\mathbb R^{l-1}$的一个超平面$h^l_i$反向映射到输入空间$R^S$中。$h_i^l$的定义如下：</p><script type="math/tex; mode=display">h_i^\ell\triangleq\{z\in\mathbb{R}^{\ell-1}:\langle w_i^\ell,z\rangle+b_i^\ell=0\}.</script><blockquote><p>为啥把一层的超平面定义成这样呢？我觉得是因为CPWL每一层的激活函数都在0处分界，故这个超平面就是这一层的分界平面。</p></blockquote><p><strong>引理2</strong>：给定一个超平面$h^l_i\in \mathbb R ^{l-1}$，这个超平面可以被映射到源域$\omega\in \Omega\in \mathbb R^S$中，as：</p><script type="math/tex; mode=display">proj_\omega(h_i^\ell)=\{\boldsymbol{x}\in\mathbb{R}^S:\langle\boldsymbol{w}_i^\ell,\boldsymbol{A}_\omega^{\ell-1}\boldsymbol{x}+\boldsymbol{b}_\omega^{\ell-1}\rangle+\boldsymbol{b}_i^\ell=0\}.</script><blockquote><p>其实把前$l-1$层整体看作是一个CPWL，把CPWL的表达式代入h的定义就可以了。</p></blockquote><p><strong>定理1</strong>：假设$S$是一个二分类的DN，因此其输出是一个单一的神经元。在$\mathbb R^{L-1}$空间中，它的决策边界是超平面$h_1^L$。$\mathbb R^S$中的决策平面是$\bigcup_{\omega\in \Omega}\{proj_\omega(h_1^L)\cap\omega\}$ 。</p><p>尽管如此，为了方便可视化，我们还是想在一个二维多面体$P \in R^S$上计算$\Omega$。</p><p><strong>SplineCam</strong>:记第$1 \to l$层组成的输入空间中的划分为$\Omega_l$。使用算法1，SplineCam首先利用超平面$h_i^1$把$P$分割为$\Omega_1$。接着，对于$\forall \omega\in \Omega_1$，使用引理1和引理2获得$proj_w(h_i^2)$。接着在每个区域$\omega$上使用算法1来获得下一步的划分$\Omega_2$。以此类推可以得到$\Omega_L$。</p><blockquote><p>算法1利用给定的边界和线来计算图中的点和边。算法2 利用算法1的结果计算图中的环，即$\omega$。</p></blockquote><p><img src="/2023/08/07/splinecam/a_1.png" alt="算法1"></p><p><img src="/2023/08/07/splinecam/a_2.png" alt="算法2"></p><h2 id="Visualizing-and-Understanding-Implicit-Neural-Representations"><a href="#Visualizing-and-Understanding-Implicit-Neural-Representations" class="headerlink" title="Visualizing and Understanding Implicit Neural Representations"></a>Visualizing and Understanding Implicit Neural Representations</h2><p>下面开始探讨深度网络的几何学，首先从隐式神经表示（INRs）入手。INRs通常是多层感知器（MLPs），经过训练能够将一维、二维或三维信号坐标映射为对应坐标处信号的强度。在3D视图合成和逆问题等应用中，它们被广泛使用。由于INRs具有低输入维度和基本函数的可解释性，因此它们是定性验证SplineCam的良好背景。目前，对于现有INR实践缺乏理论理解[42]。例如，虽然ReLU MLP主要用于NeRF[26]等最流行的INR应用，但当前的实践已经转向使用周期性激活来编码输入坐标，并随后使用ReLU MLP。在本节中，我们将研究周期性编码的影响，并可视化INR学习的区域几何结构。</p><h3 id="Decision-Boundary-of-Signed-Distance-Functions"><a href="#Decision-Boundary-of-Signed-Distance-Functions" class="headerlink" title="Decision Boundary of Signed Distance Functions"></a>Decision Boundary of Signed Distance Functions</h3><p>有符号距离函数（SDF）是一个隐式的连续表面或边界表示；SDF的输出是输入距离所表示边界的有符号距离。因此，SDF的零级集表示函数的表面或边界。将INR训练成有符号距离函数本质上是一个回归任务，模型通过拟合地面真实距离场来隐式地学习连续边界。我们训练了一个2D和一个3D的SDF，并使用我们的方法可视化了解析的零级集（类似于决策边界），并在图1和图5中提供了函数学习的样条划分。</p><blockquote><p>SDF 的概念可以参考这个<a href="https://zhuanlan.zhihu.com/p/536530019">知乎专栏</a>。属于2d或者3d模型的隐式存储方式。这篇论文使用神经网络来做SDF，具体来说就是使用神经网络记住空间中每个点与模型表面的距离。如果这个点在模型的内部，则这个距离为负数，如果点在外部，则这个距离为正数。如果点正好的模型的表面，则距离为0。</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 样条理论 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> SplineCam </tag>
            
            <tag> 可解释性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Spline Theory of Deep Learning 阅读笔记</title>
      <link href="/2023/07/22/a-spline-theory-of-deep-learning/"/>
      <url>/2023/07/22/a-spline-theory-of-deep-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="A-Spline-Theory-of-Deep-Learning"><a href="#A-Spline-Theory-of-Deep-Learning" class="headerlink" title="A Spline Theory of Deep Learning"></a>A Spline Theory of Deep Learning</h1><h2 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h2><ul><li><strong>Type</strong>: ConferencePaper</li><li><strong>Title</strong>: A Spline Theory of Deep Learning, </li><li><strong>Author</strong>: Balestriero, Randall; baraniuk,, </li><li><strong>Year</strong>: 2018 ;<ul><li><strong>Pages</strong>: 374-383</li><li><strong>Publisher</strong>: PMLR,</li></ul></li></ul><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.</p><p>我们通过样条函数和运算符在深度网络（DNs）和逼近理论之间建立了严谨的桥梁。我们的关键结果是，大部分的深度网络可以被写成最大仿射样条运算符（MASOs）的组合形式，这为我们观察和分析其内部工作提供了强大的途径。例如，对于给定的输入信号，MASO DN的输出可以被表示为输入的简单仿射变换。这意味着DN构建了一组依赖于信号的、类别特定的模板，通过简单的内积与信号进行比较；我们探索了与经典的通过匹配滤波器进行最优分类理论的联系以及数据记忆的影响。进一步地，我们提出了一个简单的惩罚项，可以添加到任何DN学习算法的损失函数中，强制模板彼此正交；这将显著提高分类性能，并减少过拟合，而无需改变DN的架构。输入信号空间的样条分区为我们研究DN如何以分层方式组织信号提供了一条新的几何途径。作为一个应用，我们开发并验证了一种用于信号的新的距离度量，用于量化它们分区编码之间的差异。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>深度学习极大地提升了我们解决各种困难的机器学习和信号处理问题的能力。当今的机器学习领域被深度（神经）网络（DNs）所主导，它们由大量简单参数化的线性和非线性变换组成。最近普遍的情况是将深度网络作为黑盒子插入应用程序中，在大量训练数据上进行训练，然后在性能上显著超越传统方法。</p><p>尽管取得了实证方面的进展，但深度学习能够如此出色地工作的确切机制仍然相对不太清楚，给整个领域增添了一丝神秘感。目前对建立严格的数学框架的努力大致可分为五个阵营：(i) 探测和测量深度网络以可视化其内部工作机制 (Zeiler &amp; Fergus, 2014)；(ii) 分析深度网络的性质，如表达能力 (Cohen et al., 2016)、损失曲面几何 (Lu &amp; Kawaguchi, 2017; Soudry &amp; Hoffer, 2017)、干扰管理 (Soatto &amp; Chiuso, 2016)、稀疏化 (Papyan et al., 2017) 和泛化能力；(iii) 新的数学框架，与深度网络有一些（但不是全部）共同特征 (Bruna &amp; Mallat, 2013)；(iv) 可以导出特定深度网络的概率生成模型 (Arora et al., 2013; Patel et al., 2016)；以及 (v) 信息理论界限 (Tishby &amp; Zaslavsky, 2015)。</p><p>在本文中，我们通过样条函数和运算符在深度网络（DNs）和逼近理论之间建立了严谨的桥梁。我们证明了一大类DNs，包括卷积神经网络（CNNs）（LeCun，1998）、残差网络（ResNets）（He等，2016；Targ等，2016）、跳跃连接网络（Srivastava等，2015）、全连接网络（Pal＆Mitra，1992）、循环神经网络（RNNs）（Graves，2013）等，可以被写成样条运算符的形式。特别是当这些网络采用当前的标准实践分段仿射凸非线性（例如ReLU，最大池化等）时，它们可以被写成最大仿射样条运算符（MASOs）的组合形式（Magnani＆Boyd，2009；Hannah＆Dunson，2013）。我们在这里重点关注这样的非线性函数，但请注意，我们的框架也适用于非分段仿射非线性函数，通过标准的逼近论证可以实现。</p><p>最大仿射样条连接为使用逼近理论和函数分析工具来观察和分析深度网络内部工作提供了一个强大的途径。以下是我们的主要贡献的总结：</p><p>a) 我们证明了大部分深度网络可以被写成最大仿射样条运算符（MASOs）的组合形式，由此可立即得出结论：在给定输入信号的条件下，深度网络的输出是输入的简单仿射变换。在第4节中，我们通过推导卷积神经网络（CNN）的输入/输出映射的闭式表达式来进行说明。<br>b) 仿射映射公式使我们能够将MASO深度网络解释为构建了一组依赖于信号的、类别特定的模板，通过简单的内积与信号进行比较。在第5节中，我们将深度网络与经典的通过匹配滤波器进行最优分类理论直接相关联，并提供了关于数据记忆效应的见解（Zhang等，2016）。<br>c) 我们提出了一个简单的惩罚项，可以添加到任何深度网络学习算法的损失函数中，以强制模板彼此正交。在第6节中，我们展示了这将显著提高在标准测试数据集（如CIFAR100）上的分类性能，并减少过拟合，而无需对深度网络的架构进行任何改变。<br>d) MASO所引发的输入空间的分区将深度网络与矢量量化（VQ）和K均值聚类理论联系起来，为研究深度网络如何以分层方式对信号进行聚类和组织提供了一条新的几何途径。第7节研究了MASO分区的性质。<br>e) 利用事实：如果两个信号位于相同的MASO分区区域中，那么深度网络将他们视为相似的。我们在第7.3节中开发了一种新的信号距离，用于衡量它们分区编码之间的差异。该距离可以通过反向传播轻松计算。</p><p>补充材料（SM）中的一些附录包含了数学设置和证明。关于这些内容的大幅扩展及众多新结果的详细说明可在（Balestriero＆Baraniuk，2018）中找到。</p><h2 id="Background-Problem-Statement"><a href="#Background-Problem-Statement" class="headerlink" title="Background / Problem Statement"></a>Background / Problem Statement</h2><h3 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h3><p>深度神经网络是一种算子，该算子将输入的信号$x\in \mathbb R^D$映射到预测值$\hat{y} \in \mathbb R^C$ ，即$f_{\Theta}:\mathbb R^D \to \mathbb R^C$。所有的神经网络可以写作是L个中间映射的组合：</p><script type="math/tex; mode=display">f_{\Theta}=(f_{\theta^{(1)}}^{(L)}\circ\cdots\circ f_{\theta^{(1)}}^{(1)}),</script><p>其中$\Theta=\left\{\theta^{(1)},\ldots,\theta^{(L)}\right\}$是每一层的网络参数集合。整体上来说，这种映射的组合是非线性且不可交换的。</p><h3 id="样条算子基础"><a href="#样条算子基础" class="headerlink" title="样条算子基础"></a>样条算子基础</h3><p>逼近论是研究如何以及多好的函数可以最好地用更简单的函数来逼近的理论。这篇文章主要讨论仿射样条（线性样条），即每一段都用线性函数去拟合。</p><blockquote><p>举例：样条函数<br>在逼近理论中，样条函数是一种使用分段的多项式函数来逼近复杂函数的工具。比如下图的$y=x^2$曲线，我们可以把x轴分割为三段，每一段分别用AG、AB、BH三条直线去拟合。下面所示的样条函数有两类需要优化的参数：1. 分段位置  2. 每一段分别用什么函数去拟合。<br><img src="/2023/07/22/a-spline-theory-of-deep-learning/20230730155321.png" alt="img"></p></blockquote><p><strong>多元仿射样条</strong>(Multivariate Affine Splines)：考虑对域$\mathbb R^D$的一个分割$\Omega=\{\omega_1,\dots,\omega_R\}$，和一系列的本地映射$\Phi=\{\phi_{1},\ldots,\phi_{R}\}$。本地映射和子域一一对应，并将子域中的点$x\in \omega_r$映射到$\mathbb R$。形式上，$\phi_r(\boldsymbol{x}):=\langle[\alpha]_{r,\cdot},\boldsymbol{x}\rangle+[\beta]_r$。其中$\alpha\in \mathbb R^{R\times D},\beta \in \mathbb R^R$。$[\alpha]_r$表示由$\alpha$的第r行组成的列向量。在此设定下，多元仿射样条的定义如下:</p><script type="math/tex; mode=display">\begin{gathered}s[\alpha,\beta,\Omega](x) =\sum_{r=1}^R\left(\langle[\alpha]_{r,\cdot},x\rangle+[\beta]_r\right)\mathbf{1}(x\in\omega_r) \\=:\langle\alpha[x],\boldsymbol{x}\rangle+\beta[\boldsymbol{x}], \end{gathered}</script><p>$\mathbf{1}(x\in \omega_r)$是一个指示函数，当满足括号里的条件时其值为1，否则为0。这样定义的样条是分段仿射且分段凸的。只有在R=1时，其为全局仿射和全局凸。我们称这种情况下的多元放射样条为退化样条。</p><p><strong>最大仿射样条函数</strong>(Max-Affine Spline Functions)：用仿射函数进行逼近的主要挑战是我们需要同时优化样条参数$\alpha,\beta$和输入域的分割$\Omega$。但是，如果我们限制仿射样条为全局凸的，则这个仿射样条可以被写为最大仿射样条：</p><script type="math/tex; mode=display">s[\alpha,\beta,\Omega](\boldsymbol{x})=\max_{r=1,\ldots,R}\langle[\alpha]_r,.,\boldsymbol{x}\rangle+[\beta]_r.</script><p>这种样条的一个非常有用的特点是它完全由它的参数$\alpha,\beta$决定，而不需要指定划分$\Omega$。</p><blockquote><p>举例：最大仿射样条<br>还是上面的图。如果我们对仿射样条加以限制，使其为全局凸的，假设直线AG为$y=a_1x+b_1$，直线AB为$y=a_2x+b_2$，直线BH为$y=a_3x+b_3$ ，则该仿射样条可以写作：$y=MAX\{y=a_1x+b_1,y=a_2x+b_2,y=a_3x+b_3\}$。这种情况下，对参数$a_i,b_i$的改变同时会影响到分割$\Omega$。故只需要对$\alpha,\beta$进行优化。</p></blockquote><p><strong>最大仿射样条算子</strong>(Max-Affine Spline Operators)：是最大仿射样条函数的一种拓展，他会产生多元的输出。这种算子由K个最大仿射样条函数的拼接得到。一个由参数$A\in\mathbb R^{K\times R\times D}, B\in\mathbb R^{K\times R}$定义的MASO可以表示为：</p><script type="math/tex; mode=display">\begin{aligned}S[A,B](x)& =\begin{bmatrix}\max_{r=1,...,R}\langle[A]_{1,r,.},\boldsymbol{x}\rangle+[B]_{1,r}\\\vdots\\\max_{r=1,...,R}\langle[A]_{K,r,.},\boldsymbol{x}\rangle+[B]_{K,r}\end{bmatrix}  \\&=:A[\boldsymbol{x}]\boldsymbol{x}+B[\boldsymbol{x}].\end{aligned}</script><p>最大仿射样条函数和算子关于每个输出维度始终是分段仿射和全局凸的(因此也是连续的)。反之，任何分段仿射和全局凸函数/算子都可以写成一个极大仿射样条。此外，利用标准的逼近论点，很容易证明一个MASO可以任意逼近任意在每个输出维数上是凸的(非线性)算子。 </p><h3 id="深度神经网络是仿射算子的组合"><a href="#深度神经网络是仿射算子的组合" class="headerlink" title="深度神经网络是仿射算子的组合"></a>深度神经网络是仿射算子的组合</h3><p>虽然MASO仅适用于逼近凸函数/算子，但我们现在展示了几乎所有现今的深度网络(DNs)都可以被写成MASO的复合形式，每一层都对应一个MASO。这样的复合在一般情况下是非凸的，因此可以逼近更广泛的函数/算子类别。有趣的是，在某些广泛条件下，这种复合仍然是分段仿射样条算子，从而为深度网络(DNs)提供了各种洞察力。</p><h4 id="深度神经网络的算子是MASO"><a href="#深度神经网络的算子是MASO" class="headerlink" title="深度神经网络的算子是MASO"></a>深度神经网络的算子是MASO</h4><p><strong>命题一</strong>：任意一个全连接算子 $f^{(l)}_W$是仿射算子，因此也是一个退化的MASO $S\Big[A_{\boldsymbol{W}}^{(\ell)},B_{\boldsymbol{W}}^{(\ell)}\Big],R=1.[A_{\boldsymbol{W}}^{(\ell)}]_{k,1,}.=\left[W^{(\ell)}\right]_{k},[B_{\boldsymbol{W}}^{(\ell)}]_{k,1}={\left[b_{\boldsymbol{W}}^{(\ell)}\right]}_{k}$。卷积层同理。</p><p><strong>命题二</strong>：任意一个满足分段仿射且凸的激活函数是一个MASO  $S\Big[A_\sigma^{(\ell)},B_\sigma^{(\ell)}\Big],R=2$。$\left[B_{\sigma}^{(\ell)}\right]_{k,1}=\left[B_{\sigma}^{(\ell)}\right]_{k,2}=0 \quad \forall k$.</p><p>对于ReLU：$\left[A_{\sigma}^{(\ell)}\right]_{k,1,\cdot}=0,\left[A_{\sigma}^{(\ell)}\right]_{k,2,\cdot}=e_k \quad \forall k$；</p><p>对于leaky ReLU：$\left[A_{\sigma}^{(\ell)}\right]_{k,1,\cdot}=\nu\boldsymbol{e}_{k},\left[A_{\sigma}^{(\ell)}\right]_{k,2,\cdot}=e_k \quad \forall k,v&gt;0$；</p><p>对于绝对值函数：$\left[A_{\sigma}^{(\ell)}\right]_{k,1,\cdot}=-\boldsymbol{e}_{k},\left[A_{\sigma}^{(\ell)}\right]_{k,2,\cdot}=e_{k}\quad\forall k$</p><p>其中 $e_{k}$ 代表 $\mathbb R^{D^{(l)}}$ 第k个正交基向量。</p><p><strong>命题三</strong>：任意一个满足分段仿射和凸的池化层都是一个MASO。</p><p>对于最大池化层， $R=\mathcal{R}_{k}$ (通常在所有输出维度上为常数).$\left[A_{\rho}^{(\ell)}\right]_{k,\cdot,\cdot}=\{e_{i},i\in\mathcal{R}_{k}\},\left[B_{\rho}^{(\ell)}\right]_{k,r}=0\forall k,r$。</p><p>平均池化层是一个退化的MASO(R=1), $\left[A_\rho^{(\ell)}\right]_{k,1,\cdot}=\frac1{(\mathcal{R}_k)}\sum_{i\in\mathcal{R}_k}e_i,\begin{bmatrix}B_{\rho}^{(\ell)}\end{bmatrix}_{k,1}=0\forall k.$</p><p><strong>命题四</strong>：由全连接/卷积算子任意组合构造的DN接上一个激活或池化算子的网络是一个MASO $S[A^{(\ell)},B^{(\ell)}]$,表示为：</p><script type="math/tex; mode=display">f^{(\ell)}(z^{(\ell-1)}(x))=A^{(\ell)}[x]z^{(\ell-1)}(x)+B^{(\ell)}[x].</script><p>因此，许多深度网络(DNs)都可以归结为MASO的复合形式。论文在附录中对CNN、ResNet、跳跃连接网络、全连接网络和RNN等进行了证明。</p><p><strong>定理一</strong>：由1至命题3中的任意全连接/卷积、激活和池化算子组成的深度网络（DN），是一个MASO的复合形式，等价于一个全局仿射样条算子。</p><p>需要注意的是，尽管定理1中所述的每个深度网络（DN）的层都是MASO，但多个层的复合形式不一定是MASO。事实上，MASO的复合仅在其所有组成算子（除了第一个算子）相对于它们各自的输出维度都是非减的情况下才仍然是MASO（Boyd＆Vandenberghe，2004）。有趣的是，ReLU和最大池化都是非减的，而leaky ReLU是严格递增的。导致复合层非凸性的罪魁祸首是全连接或卷积算子中的负项，这破坏了所需的非增性质。当这些罪魁祸首被排除时，DN就成为一个有趣的特例，因为它相对于其输入是凸的（Amos等人，2016），并且相对于其参数是多凸的（Xu＆Yin，2013）。<br>定理二：一个深度神经网络，它的第$2,\dots,L$层由具有非负权重（即$\boldsymbol{W}_{k,j}^{(\ell)}\geq0,\boldsymbol{C}_{k,j}^{(\ell)}\geq0;$）、非减、分段仿射和凸的全连接和卷积算子的任意组合构成，则这个网络是全局MASO，因此关于其每个输出维度也是全局凸的。</p><p>上述结果涉及使用凸的仿射算子的深度网络（DNs）。其他流行的非凸DN算子（例如sigmoid和arctan激活函数）可以被仿射样条算子任意接近逼近，但不能被MASO逼近。</p><p><strong>DNs是信号相关的仿射变换</strong>。上述结果的一个共同主题是，对于由命题1至命题3中的全连接/卷积、激活和池化算子构建的深度网络（DNs），算子/层的输出$z^{(l)}(x)$总是输入x的一个信号相关的仿射函数。应用到x的特定仿射映射取决于它在RD中哪个样条分区中。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 样条理论 </tag>
            
            <tag> spline theory </tag>
            
            <tag> 论文笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feature Importance Estimation with Self-Attention Networks</title>
      <link href="/2022/03/15/feature-importance-estimation-with-self-attention-networks/"/>
      <url>/2022/03/15/feature-importance-estimation-with-self-attention-networks/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="《Feature-Importance-Estimation-with-Self-Attention-Networks》-研读报告"><a href="#《Feature-Importance-Estimation-with-Self-Attention-Networks》-研读报告" class="headerlink" title="《Feature Importance Estimation with Self-Attention Networks》 研读报告"></a>《Feature Importance Estimation with Self-Attention Networks》 研读报告</h1><blockquote><p>摘要：黑盒神经网络模型被广泛应用于工业界和科学研究，但目前还很难以去理解和翻译。最近，注意力机制的提出为神经语言模型的内部工作原理提供了insight。本文探讨了使用基于注意力的神经网络机制来估计特征重要性，作为解释从命题（表格）数据中学习的模型的手段。由提议的自注意力网络 (SAN) 架构评估的特征重要性估计值与已建立的基于 ReliefF、互信息和随机森林的估计值进行比较，这些估计值在实践中广泛用于模型解释。我们首次在 10 个真实和合成数据集上跨算法对特征重要性估计进行无标度比较，以研究所得特征重要性估计的异同，表明 SAN 识别出与其他方法相似的高等级特征。我们证明 SAN 识别特征交互，在某些情况下，这些交互产生比基线更好的预测性能，这表明注意力超出了几个关键特征的交互，并检测到与所考虑的学习任务相关的更大的特征子集。</p></blockquote><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>这篇文章提出了自注意力网络（Self-Attention Networks ，即SANS)的概念，并且探索了这种网络学习到的“表示”是否可以用于重要性估计。主要贡献如下：</p><ol><li>提出SAN，这是一种可以直接获得特征重要程度的神经网络。</li><li>对比ReliefF, Mutual Information 和Genie3等feature rank算法进行可拓展的经验评估。</li><li>针对特征重要度估计机型直接比较，突出考虑的算法输出之间的相似性</li><li>对SAN的属性进行理论研究，包括空间复杂度和时间复杂度。</li></ol><h3 id="Self-Attention-Networks"><a href="#Self-Attention-Networks" class="headerlink" title="Self-Attention Networks"></a>Self-Attention Networks</h3><p>本文实现了注意力机制的神经网络可以表示为：</p><script type="math/tex; mode=display">l_2=\sigma(W_2\cdot(a(W_{|F|}\cdot \Omega(X)+b_{l_1}))+b_{l_2})</script><p>图示如下：</p><p><img src="/2022/03/15/feature-importance-estimation-with-self-attention-networks/image-20220315203059280.png" alt="论文实现的神经网络结构"></p><p>其中：</p><script type="math/tex; mode=display">\Omega(x)=\frac{1}{k}\bigoplus_k[X\bigotimes softmax(W_{l_{att}}^kX+b_{l_{att}}^k)]</script><script type="math/tex; mode=display">a(X)= \mathrm{SELU}(x)=\lambda\begin{cases}x&\text{if}x>0\\\alpha(\exp(x)-1)&\text{if}x\leq0\end{cases}.</script><p>k表示注意力头的个数。$\oplus$表示Hadamard summation，$\otimes$表示Hadamard product。如下：</p><script type="math/tex; mode=display">\begin{bmatrix}a_{11}&a_{12}&\dots&a_{1n}\\a_{21}&a_{22}&\dots&a_{2n}\\\vdots&\vdots&\vdots&\vdots\\a_{m1}&a_{m2}&\dots&a_{mn}\end{bmatrix}\bigotimes\begin{bmatrix}b_{11}&b_{12}&\dots&b_{1n}\\b_{21}&b_{22}&\dots&b_{2n}\\\vdots&\vdots&\vdots&\vdots\\b_{m1}&b_{m2}&\dots&b_{mn}\end{bmatrix}=\begin{bmatrix}a_{11}×b_{11} & a_{12}×b_{12}&\dots&a_{1n}×b_{1n}\\a_{21}×b_{21}&a_{22}×b_{22}&\dots&a_{2n}×b_{2n}\\\vdots&\vdots&\vdots&\vdots\\a_{m1}×b_{m1}&a_{m2}×b_{m2}&\dots&a_{mn}×b_{mn}\end{bmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix}a_{11}&a_{12}&\dots&a_{1n}\\a_{21}&a_{22}&\dots&a_{2n}\\\vdots&\vdots&\vdots&\vdots\\a_{m1}&a_{m2}&\dots&a_{mn}\end{bmatrix}\bigoplus\begin{bmatrix}b_{11}&b_{12}&\dots&b_{1n}\\b_{21}&b_{22}&\dots&b_{2n}\\\vdots&\vdots&\vdots&\vdots\\b_{m1}&b_{m2}&\dots&b_{mn}\end{bmatrix}=\begin{bmatrix}a_{11}+b_{11} & a_{12}+b_{12}&\dots&a_{1n}+b_{1n}\\a_{21}+b_{21}&a_{22}+b_{22}&\dots&a_{2n}+b_{2n}\\\vdots&\vdots&\vdots&\vdots\\a_{m1}+b_{m1}&a_{m2}+b_{m2}&\dots&a_{mn}+b_{mn}\end{bmatrix}</script><p>这篇论文中使用的多头注意力与Transformer的多头注意力机制是不一样的。Transformer使用的多头注意力机制是将Q、K、V映射到更低的维度计算注意力，重复h次后将得到的所有注意力向量拼接到一起。而该论文使用的多头注意力是将所有的注意力向量相加做平均。</p><h4 id="Computing-feature-importance-with-SANs"><a href="#Computing-feature-importance-with-SANs" class="headerlink" title="Computing feature importance with SANs"></a>Computing feature importance with SANs</h4><p>下面展示如何利用上述结构获得特征的重要性权重。</p><ol><li><strong>Instance-level  aggregations (attention)</strong>：令$\{(x_i,y_i),1\le i \le n\}$为样本集合，令$SAN(x_i)$表示第i个样本对应的注意力权重。</li></ol><script type="math/tex; mode=display">SAN(x_i)=\frac{1}{k}\bigoplus_k[softmax(w_{l_{att}}^kx_i+b_{l_{att}}^k)]</script><p>获得注意力的第一个选择就是将每个样本的注意力做均值：</p><script type="math/tex; mode=display">R_I=\frac{1}{n}\sum_{i=1}^nSAN(x_i)</script><ol><li><strong>Counting only correctly predicted instances (attentionPositive)</strong>:第二种变体基于如下的假设：只考虑被正确预测的样本。</li></ol><script type="math/tex; mode=display">R_I^c=\frac{1}{n}\sum_{i=1}^nSAN(x_i)[\hat{y}_i=y_i]</script><ol><li><strong>Global attention layer (attentionGlobal)</strong>: 前面的两种方式通过累加注意力向量来获得全局的特征重要性。但是基于权重向量包含特征重要性信息的假设，可以在训练结束后直接获得全局的注意力权重。</li></ol><script type="math/tex; mode=display">R_G=\frac{1}{k}\bigoplus_k[softmax(diag(w_{l_{att}}^k))];w_{l_{att}}^k\in \mathbb{R}^{|F|×|F|}</script>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 特征提取 </tag>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>testpass</title>
      <link href="/2021/11/15/testpass/"/>
      <url>/2021/11/15/testpass/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div id="aplayer-ZTMKubzl" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-ZTMKubzl"),            narrow: false,            autoplay: false,            showlrc: false,            music: {              title: "test",              author: "test",              url: "test.mp3",              pic: "",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link href="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/"/>
      <url>/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Evaluating-Differentially-Private-Machine-Learning-in-Practice"><a href="#Evaluating-Differentially-Private-Machine-Learning-in-Practice" class="headerlink" title="Evaluating Differentially Private Machine Learning in Practice"></a>Evaluating Differentially Private Machine Learning in Practice</h1><p>Bargav Jayaraman and David Evans, University of Virginia  </p><blockquote><p>摘要：差分隐私可以用来计算一个机制泄露了多少隐私(用$\epsilon$表示)。当用于机器学习时，差分隐私的目标即为限制模型泄露的训练集中单个个体的隐私量。但是目前人们对于如何校准$\epsilon$还不是很了解。在机器学习中人们通常会设定一个很大的$\epsilon$值来获得更好的可用性，但对这种选择对隐私所造成的影响所知甚少。此外，在使用迭代学习程序的情况下，经常使用宽松的差分隐私定义，这似乎减少了所需的隐私预算，但人们对隐私性和可用性之间的平衡理解并不深刻。<strong>在这篇文章中</strong>，我们在逻辑回归和神经网络模型的实验中量化了这些选择对隐私的影响。我们的主要的发现是获取隐私是需要代价的——宽松的差分隐私定义减少了所需的噪声数量但同时增大了隐私泄露的风险。现有的差分隐私机器学习机制很少会对复杂学习任务做可接受的可用性与隐私性之间的平衡：降低准确率损失会降低隐私性，提供强隐私保障的会产生无用的模型。</p></blockquote><p><strong>主要内容：通过实验验证不同的$\epsilon$值和使用不同的宽松的差分隐私定义对模型可用性与隐私性的影响。</strong></p><h3 id="几种宽松的差分隐私定义"><a href="#几种宽松的差分隐私定义" class="headerlink" title="几种宽松的差分隐私定义"></a>几种宽松的差分隐私定义</h3><p>主要思想：多种差分隐私机制组合起来时其整体的privacy budget（暂且称为隐私代价）不一定为各个privacy budget之和。可以通过一些其他的规律来获得一个更紧的上界。</p><ol><li>advanced composition theorem ：考虑到隐私损失的期望，可以获得$\epsilon$的一个更紧的上界。</li><li>Concentrated Differential Privacy (CDP)  ：$\mathcal{D}_{subG}(\mathcal{M}(D)||\mathcal(D’))\le(\mu,\tau)$。任何$\epsilon-DP$算法都满足$(\epsilon \cdot(e^{\epsilon}-1)/2,\epsilon)-CDP$ ，反过来不一定满足。</li><li>Zero-Concentrated Differential Privacy (zCDP)  ：$\mathcal{D}_{\alpha}(\mathcal{M}(D)||\mathcal(D’))\le \xi + \rho a$。如果$\mathcal{M}$满足$\epsilon-DP$，则它满足$(\frac{1}{2}\epsilon^2)-zCDP$。如果它满足$\rho-zCDP$，则对任意$\delta&gt;0$它满足$(\rho+2\sqrt{\rho log(1/\delta),\delta})-DP$</li><li>Rényi Differential Privacy (RDP)  :$\mathcal{D}_{\alpha}(\mathcal{M}(D)||\mathcal{M}(D’))\le \epsilon$。如果$\mathcal{M}$满足$(\alpha,\epsilon)-RDP$，则对于任意$0&lt;\delta&lt;1,$满足$(\epsilon+\frac{log(1/\delta)}{\alpha-1},\delta)-DP$</li></ol><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211107162132017.png" alt="img"></p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><h4 id="实验设定"><a href="#实验设定" class="headerlink" title="实验设定"></a>实验设定</h4><ol><li>使用逻辑回归模型（凸优化）和神经网络模型（非凸）作为目标模型。</li><li>使用成员推断和属性推断作为攻击方式，从而获得模型的隐私损失。</li><li>在梯度上添加扰动。</li><li>使用具有两个隐藏层的神经网络作为推断（攻击）网络</li></ol><h4 id="攻击方式"><a href="#攻击方式" class="headerlink" title="攻击方式"></a>攻击方式</h4><p>成员推断使用Shokri（1）和Yeom（2）的方法。</p><ol><li>黑盒攻击。攻击者可以获得目标模型对输入的置信度（confidence score）。使用相同分布下采样的数据训练了多个影子模型。使用这些影子模型训练推断模型。推断模型的训练集来源于部分用来训练影子模型的训练数据和一些相同分布下随机采样的数据。推断模型的输入还包括影子模型对这些数据的置信度。</li><li>白盒攻击。假设攻击者可以访问训练集在目标模型上的平均损失。如果输入数据在目标模型上的损失值小于均值的话就认为这个输入在目标模型的训练集里。</li></ol><p>属性推断：使用Yeom的方法。与上2类似。暴力搜索隐私属性的所有可能值，选择与平均损失最接近的组合。</p><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>选择两个数据集</p><ol><li>CIFAR-100  （28×28 images）。使用PCA把维度压缩到50</li><li>Purchase-100  </li></ol><p>每个数据集中分别随机选10,000 个作为训练集，10000个作为测试集。剩下的用来训练影子模型和推断模型。</p><p>在进行属性推断攻击时，因为原数据集没有标注哪些属性为隐私属性，所以随机选取5个属性作为隐私属性。</p><h4 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h4><ol><li>accuracy loss：没有隐私保护的模型在测试集上的准确率（baseline） - 有隐私保护的模型的准确率<ol><li>privacy leakage：真阳率（True Positive Rate) - 假阳率（False Positive Rate)。如果是0的话代表没有隐私泄露。</li></ol></li></ol><h4 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h4><p>训练模型时使用$l_2$正则化。首先使用grid search训练一个非隐私模型，从而找到一个最大化测试集正确率的$\lambda$。</p><p>对于CIFAR-100 </p><ol><li>逻辑回归：$\lambda = 10^{-5}$</li><li>神经网络：$\lambda = 10^{-4}$</li></ol><p>对于Purchase-100  </p><ol><li>逻辑回归：$\lambda = 10^{-5}$</li><li>神经网络：$\lambda = 10^{-8}$</li></ol><p>然后用上面的设定训练隐私模型。$\epsilon$取值范围为$0.01-1000$，$\delta$固定为$10^{-5}$(假设训练集大小为n, $\delta\lt\frac{1}{n}$)</p><p>使用ADAM优化器，固定学习率为0.01.</p><p>batch size = 200</p><h4 id="Clipping"><a href="#Clipping" class="headerlink" title="Clipping"></a>Clipping</h4><p>使用Tensorflow Privacy框架实现了batch clipping 和 per-instance clipping。阈值$\mathcal{C}=1$。通过下图的比较结果发现Per-instance clipping放大了不同机制之间的差异。因此后面的实验中只使用这一种clipping方式。</p><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211107204703476.png" alt="image-20211107204703476"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h4><h5 id="CIFAR-100"><a href="#CIFAR-100" class="headerlink" title="CIFAR-100"></a>CIFAR-100</h5><p>baseline在训练集上的准确率是0.225，测试集上的准确率是0.155。中间的gap值为0.07。上图（b）显示了不同$\epsilon$值，不同差分隐私定义对准确率损失的影响。</p><p>下图（a) （b)展示成员推断攻击下的隐私损失，（c）展示了属性推断攻击下的隐私损失。</p><p>通过(a)看出对于Naive composition来说，当$\epsilon\le10$时隐私损失基本为0，当$\epsilon=1000$隐私泄露达到了$0.065\pm0.004$，同时RDP和zCDP的损失之达到了$0.08\pm0.004$。通过图(b)可以看出naive-composition在$\epsilon\le 10$时没有明显的隐私损失。但是当$\epsilon=1000$时损失快速上升到$0.093\pm0.002  $。</p><p>图中还显示了$\epsilon-differential\quad privacy$的理论损失上界：$e^{\epsilon}-1$   [Yeom et al]。</p><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211107211234195.png" alt="image-20211107211234195"></p><p>下面的图表展示了不同的差分隐私定义下泄露给敌手的训练集成员数量。</p><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211107213708468.png" alt="image-20211107213708468"></p><p>实验结论后面还有好多，与上面的大同小异，不一一展示了。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>获取隐私是有代价的（that there is no way to obtain privacy for free  ）</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/bargavj/EvaluatingDPML">https://github.com/bargavj/EvaluatingDPML</a>  </p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.usenix.org/conference/usenixsecurity19/presentation/jayaraman">原文链接</a></p><h3 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h3><ol><li><p>实验方法</p></li><li><p>在机器学习中应用差分隐私</p></li></ol><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211108121014599.png" alt="image-20211108121014599"></p><p><img src="/2021/11/08/evaluating-differentially-private-machine-learning-in-practice/image-20211108121412999.png" alt="image-20211108121412999"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>An Attentive Survey of Attention Models(2021)研读报告</title>
      <link href="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/"/>
      <url>/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="An-Attentive-Survey-of-Attention-Models-2021-研读报告"><a href="#An-Attentive-Survey-of-Attention-Models-2021-研读报告" class="headerlink" title="An Attentive Survey of Attention Models(2021)研读报告"></a>An Attentive Survey of Attention Models(2021)研读报告</h1><blockquote><p>论文标题：关于注意力模型的细心调研<br>如今注意力模型已经成为了神经网络中一个非常重要的概念，而且在多种应用领域中得到了广泛的研究。这篇调研针对注意力机制的发展提供了一个结构化、综合的概述。我们将现有的注意力技术分类。我们回顾了一些具有注意力模型的神经结构，并且讨论一些注意力模型已经发挥显著作用的应用。我们还描述了注意力机制是如何用来提升神经网络的可解释性的。最后我们讨论了一些未来的注意力机制的研究方向。这篇调研可以针对注意力机制提供一个简洁的介绍，并在实践人员开发应用时提供引导。</p></blockquote><span id="more"></span><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><p>注意力模型（AM）最早用于机器翻译，并为神经网络模型带来很大的优势。AM作为大量NLP应用中的一个基本部件，它已经变得非常出名。注意力机制背后的直觉（intuition）可以用人类生物系统解释。举个例子，我们的视觉处理系统倾向于把注意点放在图像的某些部位，同时忽略其他不相关的信息。类似的，在一些关于语言、视觉、听觉的问题中，输入的某些部分比其它部分更加重要（如图像描述问题，相较于其他区域，图像的某一个特定区域可能对于生成描述句的下一个词更加重要）。注意力模型通过动态的调整模型的注意力从而让模型只注意对完成任务有用的部分输入。下面是[Yang et al. 2016] 做的关于注意力机制在情感分类问题中的应用的例子。AM学习到在这五句话中，第一句和第三句对情感分析更有帮助。</p><p><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_e1e9157e816c7a6c17f183442e37c95d.png" alt="img"></p><p>对注意力进行建模发展迅速的三个原因：</p><ol><li>是解决多任务最先进的模型。</li><li>为提升主任务性能提供了其它的优势。它被用于提升神将网络的可解释性（越来越多的人对影响人类生活的应用中的网络模型的公平性、问责制、透明度感兴趣）。</li><li>它们有助于克服递归神经网络RNN中的一些挑战，例如随着输入长度的增加性能下降，以及输入顺序不合理导致的计算效率低下。</li></ol><p>文章的组织：我们的工作致力于为注意力建模提供一个简短但是综合的研究。在第二节我们用一个简单的回归模型来为你提供关于注意力的初步印象。在第三节我们简短的介绍了[Bahdanau et al. 2015]提出的AM和其他的注意力函数（attention function）。第四节我们介绍了我们的分类结果。第五节和第六节讨论了使用AM的关键神经结构并展示了一些广泛使用注意力机制的应用。最后在第七节我们解释了注意力如何促进理解神经网络的可解释性，并在第八节介绍了未来的研究方向。</p><p>相关的调研：目前已经有一些专门领域的关于注意力机制的研究。如：</p><ol><li>[Wang and Tax 2016] on cv</li><li>[Lee et al. 2019] on graphs</li><li>[Galassi et al. 2020] on nlp</li></ol><h2 id="2-ATTENTION-BASICS"><a href="#2-ATTENTION-BASICS" class="headerlink" title="2 ATTENTION BASICS"></a>2 ATTENTION BASICS</h2><p>[Nadaraya 1964; Watson 1964]提出的回归模型可以帮助我们理解注意力机制。我们的数据样本有n个数据$\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}$。我们想知道当给定一个x时其对应的y的预测值$\hat{y}$。一个朴素的估计器对于任何输入都会输出样本中$y_i$的平均值。Naradaya-Watson提出了一个更好的方法：使用$y_i$的加权平均数。$y_i$的权值为$x,x_i$之间的相关度。公式如下：</p><script type="math/tex; mode=display">\hat{y} = \sum_{i=1}^{n}\alpha(x,x_i)y_i</script><p>$\alpha$是一个衡量$x,x_i$关联程度的函数。对于$\alpha$的一个普遍的选择是使用高斯核。Naradaya-Watson表示这种估计器有两个特性：</p><ol><li>一致性(consistency)：数据样本量越大结果越好。</li><li>朴素性(simplicity)：没有没用的参数。信息存储在样本数据中而不是在权重中。<br>往后推50年，深度模型中的注意力机制可以看作是上述公式的一种泛化，即权重函数是可以学习的。</li></ol><h2 id="3-ATTENTION-MODEL"><a href="#3-ATTENTION-MODEL" class="headerlink" title="3 ATTENTION MODEL"></a>3 ATTENTION MODEL</h2><p>AM的第一次应用被[Bahdanau et al. 2015]用来解决一个sequence-to-sequence modeling<br>task。s2s Model含有一个编码器(encoder)和一个解码器(decoder),他们的隐藏状态分别为$h_i,s_i$。编码器获得输入$\{x_1,x_2,\dots,x_T\}$并输出T个具有固定长度的向量$\{h_1,h_2,\dots,h_T\}$。将$h_T$输入解码器，解码器会输出$\{y_1,y_2,\dots,y_{T’}\}$token by token。<br><strong>上述模型有两个问题（挑战）：</strong></p><ol><li>将所有输入压缩成一个具有固定长度的向量会造成信息丢失。</li><li>无法对输入和输出序列之间的排列进行建模</li></ol><p>也就是说解码器在输出每个token时无法有选择的使用输入。</p><p><strong>核心思想</strong>：使用注意力权重来决定哪些输入对下一个输出更重要。</p><p><strong>注意力的使用方法</strong>：如下图(b)。在解码器的第j个时间步，模型结构中的注意力模块负责自动生成注意力权重$\alpha_{ij}$。用这个权重表示$s_{j-1},h_i$直间的关联度。接着这些权重会通过$c_j=\sum_{i=1}^{T}\alpha_{ij}h_i$生成contex vector c。c会输入到解码器的下一个时间步。通过生成c，解码器就可以获得整个输入信息同时只把”注意力”放在输入的某部分。这种方法提升了算法的性能和输出的质量。Table 1 是这种注意力机制的数学表达。<br><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_1455ea8a4e2ecf7e25bf47df00e2bac9.png" alt=""><br><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_7d99451e1b73905d753043d61336ddb0.png" alt=""></p><p><strong>学习注意力权重</strong>: 注意力权重的学习可以通过引入一个前馈神经网络来完成。这个前馈神经网络作为一个函数，它的的输入为$h_i,s_{j-1}$。这个函数叫做alignment funcion（table 1 中用a表示），它会对$h_i,s_{j-1}$的相关度进行评估（有的资料把这个函数叫做评分函数scoring function）并输出评分$e_{ij}$。distribution function（table 1中用p表示）会把$e_{ij}$转化为注意力权重。当a，p是可微的函数时，这整个基于注意力的encoder-decoder模型就会成为一个大的可微函数，并且可以一同训练。<br><strong>泛化的AM</strong>：AM可以看作是一种映射。这个映射会根据query q将keys K映射为权重$\alpha$。其中keys即为编码器的隐藏状态$\{h_1,h_2,\dots,h_T\}$，q即为$s_{j-1}$。注意力模型表示如下：</p><script type="math/tex; mode=display">A(q,K,V)=\sum_ip(a(k_i,q))*v_i</script><p>通常keys和values是一一对应的。在[Bahdanau et al. 2015]提出的模型中，$h_i = v_i = k_i$。通过上面的回归的例子来理解这个公式，输入x为query，$v_i$为$y_i$，$k_i$为$x_i$。</p><p><strong>Alignment functions（排列函数？）</strong>：<br>对于key和query在相同向量空间的：</p><ol><li>余弦相似度或者点积（dot product）</li><li>考虑到不同的表示长度，scaled dot product使用表示向量的长度来归一化点积。</li></ol><p>对于key和query在不同向量空间的：</p><ol><li>General alignment：引入可学习的转换矩阵将query映射到key的向量空间。</li><li>Activated general alignment：添加一个非线性的激活层（hyperbolic tangent, rectifier linear unit, or scaled exponential linear unit. ）</li><li>Biased general alignment ：不管query，通过添加偏置项（bias）直接学习一些key的全局重要性。</li><li>[Choromanski et al. 2021] 展示了key和query可以通过generalized kernel function匹配，而不是点积。<br>key，query联合表示的：</li><li>concat alignment：keys和queries拼接到一起形成联合表示。</li><li>Additive alignment：对key和query的贡献进行解耦。使得可以提前计算所有key的贡献而不用对每个query再计算一遍。降低了计算时间。</li><li>deep alignment 使用了多层神经网络。<br>针对特定使用场景的：</li><li>Location-based alignment：忽略keys的内容，只使用它的位置信息。</li><li>[Li et al. 2019a]提出当处理一组元素（如2-D patches for images 或者 1-D temporal<br>sequences），从属于该组的单个元素的表示中得出的特征（如平均数和标准差）可以作为alignment function的输入。</li></ol><p><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_617f45653ad3c44d2f61bd3ded2c020e.png" alt=""></p><p><strong>distribution function（分布函数）</strong>：</p><ol><li>dense distribution：softmax,logistic sigmoid。输出可以看作概率，但会降低模型的可解释性，而且会对不可能的输出分配概率（This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. ）。</li><li>sparse distribution：sparsemax和sparse entmax。通过只对少量可能的输出赋予概率从而产生sparse alignment。当大量元素之间是不相关时会非常有用。</li><li>[Tay et al. 2019]提出一种分布函数。计算两个项——$thanh(\frac{qk_i^T}{\sqrt{d_k}}),sigmoid(\frac{G(qk_i^T)}{\sqrt{d_k}})$的乘积来形成准注意力（qusi-attention）。其中第一项控制向量的加或者减，第二项可以看作是控制删除不相关元素的门函数。</li></ol><h2 id="4-TAXONOMY-OF-ATTENTION"><a href="#4-TAXONOMY-OF-ATTENTION" class="headerlink" title="4 TAXONOMY OF ATTENTION"></a>4 TAXONOMY OF ATTENTION</h2><p><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_7471d28d5ac5631a1e2d6b9048aab5ef.png" alt=""></p><h3 id="4-1-Number-of-sequences"><a href="#4-1-Number-of-sequences" class="headerlink" title="4.1 Number of sequences"></a>4.1 Number of sequences</h3><ol><li>distinctive attention：输入和输出是独立的两个序列。上面考虑的全是这种情况。大部分情况应用于翻译 [Bahdanau et al. 2015]、图像描述[Xu et al. 2015]和语音识别[Chan et al. 2016]。</li><li>co-attention：输入有多个序列，同时计算他们的注意力权重。[Lu et al. 2016]用它来做visual question answering。不仅提取图像中的重要信息a同样也要提取问题中的关键点b。a和b会互相影响。类似的[Yu et al. 2019] 也用它来做visual question answering task</li><li>self-attention(inner attention)。每个输入序列都可以学习自身中相关的元素。即key和query在同一个序列中。</li></ol><h3 id="4-2-Number-of-abstraction-levels"><a href="#4-2-Number-of-abstraction-levels" class="headerlink" title="4.2 Number of abstraction levels"></a>4.2 Number of abstraction levels</h3><ol><li>single-level：只对原始输入添加注意力权重。</li><li>multi-level：注意力模型的低层输出变为高层的输入（query）。再细分可以根据学习顺序分为自顶向下，自底向上两类。 [Yang et al. 2016]用“Hierarchical Attention Model”(HAM) 解决文档分类问题。这种多层模型可以从句子中找到关键词，从文档中找到关键句。上面提到的 [Lu et al. 2016]使用的co-attention也是多层模型（单词层，短语层，问题层），如下图。[Zhao and Zhang 2018]提出attention-via-attention，低层为字母高层为单词，并通过自顶向下的方式学习。<br><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_6149cd7ede3a72ba726fd358cba6b2f6.png" alt=""></li></ol><h3 id="4-3-Number-of-positions"><a href="#4-3-Number-of-positions" class="headerlink" title="4.3 Number of positions"></a>4.3 Number of positions</h3><ol><li>soft attention：编码器输出的所有隐藏状态都要参与注意力的计算。方便反向传播但是会造成二次方的损失（quadratic computational cost）。</li><li>hard attention：[Xu et al. 2015]提出通过范畴分布（multinoulli distribution）随机选取隐藏状态计算注意力权重。降低了计算损失但导致函数不可微，并且难以优化。Variational learning方法和 policy gradient 方法的提出可以帮助解决这些缺陷。</li><li>local and global：[Luong et al. 2015b] 为机器翻译提出两种注意力模型：local、global。global类似soft attention模型。local介于soft和hard之间。local的关键想法是在输入序列中找到一个”注意点”（attention point），在这个点（位置）周围选择一个创建一个窗口，在窗口中建立局部soft attention模型。这个点（位置）可以认为设定(monotonic alignment) 或者通过学习得到(predictive alignment)。</li></ol><h3 id="4-4-Number-of-representations"><a href="#4-4-Number-of-representations" class="headerlink" title="4.4 Number of representations"></a>4.4 Number of representations</h3><ol><li>Single-representational AM：只使用输入序列的一种特征表示法。这是大多数应用使用的方法。但是在某些情况下不能满足下游任务。</li><li>Multi-representational AM：通过多种特征表示来获取输入的不同方面的特征，然后使用注意力机制为不同的表示添加权重从而决定与这个任务最相关的表示法。[Kiela et al. 2018]、[Maharjan et al. 2018]、[Lin et al. 2017] 、 [Shen et al. 2018] 都做了类似的工作。</li></ol><h2 id="5-NETWORK-ARCHITECTURES-WITH-ATTENTION"><a href="#5-NETWORK-ARCHITECTURES-WITH-ATTENTION" class="headerlink" title="5 NETWORK ARCHITECTURES WITH ATTENTION"></a>5 NETWORK ARCHITECTURES WITH ATTENTION</h2><h3 id="5-1-Encoder-Decoder"><a href="#5-1-Encoder-Decoder" class="headerlink" title="5.1 Encoder-Decoder"></a>5.1 Encoder-Decoder</h3><p>最早在 [Bahdanau et al. 2015]中提出。编码器把输入压缩到一个固定长度的向量，解码器再对这个向量进行处理。这种方式把输入输出进行解耦，这使得杂交encoder-decoder成为了可能。比如编码器用CNN，解码器用LSTM。对许多模型任务例如图像，视频描述、Visual Question Answering和语音识别有非常大的帮助。<br>但是并不是所有问题都可以的输入输出都是序列化的。Pointer Network [Vinyals et al. 2015]有如下的两个不同：</p><ol><li>输出是离散的值，并且指向了输入序列的某个位置。</li><li>输出类别取决于输入的数量。<br>作者通过使用注意力权重来对每个时间步选择第i个输入符号的概率。这个方法可以用作离散优化问题（旅行商，排序）</li></ol><h3 id="5-2-Transformer"><a href="#5-2-Transformer" class="headerlink" title="5.2 Transformer"></a>5.2 Transformer</h3><p>循环结构会序列化的处理输入，这导致计算效率下降。Transformer [Vaswani et al. 2017].通过使用自注意力机制来获得输入输出的全局依赖。作者展示了使用Transformer，即便不使用循环结构也可以在机器翻译任务中获得超强的并行处理能力，更短的训练时间和更高的准确度。</p><h3 id="5-3-Memory-Networks"><a href="#5-3-Memory-Networks" class="headerlink" title="5.3 Memory Networks"></a>5.3 Memory Networks</h3><p>类似Question Answering的应用需要拥有从事实数据库中学习的能力。网络的输入是知识数据库（knowledge dagabase）和一个问题（query）。在数据库中有些事实是与问题相关的但有些不是。在这种情况下就需要注意力来选择相关的事实。</p><h3 id="5-4-Graph-Attention-Networks-GAT"><a href="#5-4-Graph-Attention-Networks-GAT" class="headerlink" title="5.4 Graph Attention Networks (GAT)"></a>5.4 Graph Attention Networks (GAT)</h3><p>战略性略过</p><h2 id="6-APPLICATIONS"><a href="#6-APPLICATIONS" class="headerlink" title="6 APPLICATIONS"></a>6 APPLICATIONS</h2><p><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_11ba64cfcd9ee772f67375ac7dbc8270.png" alt=""></p><h2 id="7-ATTENTION-FOR-INTERPRETABILITY"><a href="#7-ATTENTION-FOR-INTERPRETABILITY" class="headerlink" title="7 ATTENTION FOR INTERPRETABILITY"></a>7 ATTENTION FOR INTERPRETABILITY</h2><p>越来越多的人关注AI模型的可解释性，而且这正是神经网络特别是深度学习网络所欠缺的。注意力机制可以帮助我们提升AI模型的可解释性。存在的一种假设是注意力权重的大小与对应输入部分与当前时间步下的输出的相关度有关。通过可视化输入输出对可以发现这种规律。<br> [Rush et al. 2015]展示了在（文本）总结问题中，AM可以在输出时把注意力放在输入的相关的单词上（下图a）。 [He et al. 2018]表示注意力可以用来识别用户的兴趣（下图b）。[Xu et al. 2015]通过可视化展示了图片摘要问题中对输出有很大影响的图形区域。<br><img src="/2021/10/29/an-attentive-survey-of-attention-models-2021-yan-du-bao-gao/upload_5220608e9e49aeb36a7d305a78faa59a.png" alt=""><br>总结几种有趣的事实：</p><ol><li>[De-Arteaga et al. 2019] explored gender bias in occupation classification, and showed how the words getting more attention during classification task are often gendered.（没看懂，直接粘原文了）</li><li>[Yang et al. 2016]表示在情感分类问题中，”good”和”bad”这两个单词的重要性是与文本内容相关的。</li><li>[Chan et al. 2016]注意到在语音识别中，字符输出和音频信号之间的注意力可以正确识别音频信号中第一个字符的起始位置，并且对于具有声学相似性的单词，注意力权重是相似的。</li><li>[Kiela et al. 2018]发现multi-representational attention对 GloVe, FastText这两种词嵌入方式赋予了更高的权重。</li></ol><p>另一种有趣的应用： [Lee et al. 2017] and [Liu et al. 2018]提供了一种注意力可视化的工具</p><p>尽管许多人用注意力来提高AI模型的可解释性，但有些人也提出了相反的观点。 [Jain and Wallace 2019]提出注意力权重与特征重要性分析是不相关的。他们观察预测结果对注意力权重改变的敏感度，却发现通过使用随机排列和adversarial training并没有改变输出结果。[Serrano and Smith 2019] applied a different analysis based on intermediate representation erasure method and showed that attention weights are at best noisy predictors of relative importance of the specific regions of input sequence, and should not be treated as justifications for model’s decisions. （又没看懂………….）</p><h2 id="未来的研究方向"><a href="#未来的研究方向" class="headerlink" title="未来的研究方向"></a>未来的研究方向</h2><ol><li>Real-time Attention<br>类似实时翻译的应用需要在获得整个输入之前开始预测结果，这就需要Real-time Attention。<br>相关的成果：<ol><li>[Chiu and Raffel 2017] </li><li>[Ma et al. 2019] </li></ol></li><li><p>Stand-alone Attention<br>相关成果：</p><ol><li>[Ramachandran et al. 2019]</li><li>[Wang et al. 2020e]</li></ol></li><li><p>Model Distillation<br>相关成果：</p><ol><li>[Wang et al. 2020d] </li><li>[Mirzadeh et al. 2020]</li><li>[Touvron et al. 2020] </li></ol></li><li><p>Attention for Interpretability<br>相关成果</p><ol><li>[Mohankumar et al. 2020]</li></ol></li><li><p>Auto-learning Attention<br>相关成果：</p><ol><li>[Ma et al. 2020] </li></ol></li><li><p>Multi-instance Attention<br>相关成果：</p><ol><li>[Li et al. 2019a]</li></ol></li><li><p>Multi-agent Systems<br>相关成果：</p><ol><li>[Fujii et al. 2020; Li et al. 2020a] </li></ol></li><li><p>Scalability<br>相关成果：</p><ol><li>[Sukhbaatar et al. 2019b] </li><li>[Choromanski et al. 2021]</li><li>[Wang et al. 2020a]</li><li>[Li et al. 2020b]</li></ol></li></ol><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ol><li><a href="https://arxiv.org/pdf/1904.02874.pdf">Chaudhari S, Mithal V, Polatkan G, et al. An attentive survey of attention models[J]. ACM Transactions on Intelligent Systems and Technology (TIST), 2021, 12(5): 1-32.</a></li><li><a href="https://github.com/d2l-ai/d2l-zh">《动手学深度学习》</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/10/28/hello-world/"/>
      <url>/2021/10/28/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo new "My New Post"</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
