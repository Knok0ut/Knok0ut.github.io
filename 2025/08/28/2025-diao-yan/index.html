<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="2025调研, 酸辣果汁">
    <meta name="description" content="
AI声明：筛选、整理过程存在AI辅助，谨慎使用

USENIX 2024错误注入和鲁棒性
DNN-GP: Diagnosing and Mitigating Model’s Faults Using Latent Concepts.
Ye">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>2025调研 | 酸辣果汁</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">酸辣果汁</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">酸辣果汁</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/WangXurun" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/WangXurun" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/10.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">2025调研</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/">
                                <span class="chip bg-color">论文调研</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-29
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>AI声明：筛选、整理过程存在AI辅助，谨慎使用</p>
</blockquote>
<h2 id="USENIX-2024"><a href="#USENIX-2024" class="headerlink" title="USENIX 2024"></a>USENIX 2024</h2><h3 id="错误注入和鲁棒性"><a href="#错误注入和鲁棒性" class="headerlink" title="错误注入和鲁棒性"></a>错误注入和鲁棒性</h3><ul>
<li>DNN-GP: Diagnosing and Mitigating Model’s Faults Using Latent Concepts.</li>
<li>Yes, One-Bit-Flip Matters! Universal DNN Model Inference Depletion with Runtime Code Fault Injection.</li>
<li>Tossing in the Dark: Practical Bit-Flipping on Gray-box Deep Neural Networks for Runtime Trojan Injection.</li>
<li>Forget and Rewire: Enhancing the Resilience of Transformer-based Models against Bit-Flip Attacks.<h3 id="大模型攻击与防御"><a href="#大模型攻击与防御" class="headerlink" title="大模型攻击与防御"></a>大模型攻击与防御</h3></li>
<li>An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection. </li>
<li>REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models. 水印</li>
<li>Formalizing and Benchmarking Prompt Injection Attacks and Defenses.</li>
<li>Instruction Backdoor Attacks Against Customized LLMs.<h3 id="安全ML"><a href="#安全ML" class="headerlink" title="安全ML"></a>安全ML</h3></li>
<li>AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE.</li>
<li>Fast and Private Inference of Deep Neural Networks by Co-designing Activation Functions.</li>
<li>OblivGNN: Oblivious Inference on Transductive and Inductive Graph Neural Network.</li>
<li>MD-ML: Super Fast Privacy-Preserving Machine Learning for Malicious Security with a Dishonest Majority.</li>
<li>Accelerating Secure Collaborative Machine Learning with Protocol-Aware RDMA.<h3 id="隐私推理"><a href="#隐私推理" class="headerlink" title="隐私推理"></a>隐私推理</h3></li>
<li>A Linear Reconstruction Approach for Attribute Inference Attacks against Synthetic Data.</li>
<li>Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models.</li>
<li>MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training.</li>
<li>Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks.</li>
<li>Property Existence Inference against Generative Models.</li>
<li>How Does a Deep Learning Model Architecture Impact Its Privacy? A Comprehensive Study of Privacy Attacks on CNNs and Transformers.</li>
<li>Reconstructing training data from document understanding models.</li>
<li>Privacy Side Channels in Machine Learning Systems.</li>
<li>FaceObfuscator: Defending Deep Learning-based Privacy Attacks with Gradient Descent-resistant Features in Face Recognition.<h3 id="后门"><a href="#后门" class="headerlink" title="后门"></a>后门</h3></li>
<li>Neural Network Semantic Backdoor Detection and Mitigation: A Causality-Based Approach.</li>
<li>On the Difficulty of Defending Contrastive Learning against Backdoor Attacks.</li>
<li>Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models.</li>
<li>Xplain: Analyzing Invisible Correlations in Model Explanation.</li>
<li>Verify your Labels! Trustworthy Predictions and Datasets via Confidence Scores.<h3 id="Digital-Adversarial-Attacks"><a href="#Digital-Adversarial-Attacks" class="headerlink" title="Digital Adversarial Attacks"></a>Digital Adversarial Attacks</h3></li>
<li>More Simplicity for Trainers, More Opportunity for Attackers: Black-Box Attacks on Speaker Recognition Systems by Inferring Feature Extractor.</li>
<li>Transferability of White-box Perturbations: Query-Efficient Adversarial Attacks against Commercial DNN Services.</li>
<li>Adversarial Illusions in Multi-Modal Embeddings.</li>
<li>It Doesn’t Look Like Anything to Me: Using Diffusion Model to Subvert Visual Phishing Detectors.</li>
<li>Invisibility Cloak: Proactive Defense Against Visual Game Cheating.<h3 id="对抗攻防"><a href="#对抗攻防" class="headerlink" title="对抗攻防"></a>对抗攻防</h3></li>
<li>Correction-based Defense Against Adversarial Video Attacks via Discretization-Enhanced Video Compressive Sensing.</li>
<li>Rethinking the Invisible Protection against Unauthorized Image Usage in Stable Diffusion.</li>
<li>Splitting the Difference on Adversarial Training.</li>
<li>Machine Learning needs Better Randomness Standards: Randomised Smoothing and PRNG-based attacks.</li>
<li>PatchCURE: Improving Certifiable Robustness, Model Utility, and Computation Efficiency of Adversarial Patch Defenses.<h3 id="评估和最好的实践"><a href="#评估和最好的实践" class="headerlink" title="评估和最好的实践"></a>评估和最好的实践</h3></li>
<li>SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models<h3 id="后门-1"><a href="#后门-1" class="headerlink" title="后门"></a>后门</h3></li>
<li>UBA-Inf: Unlearning Activated Backdoor Attack with Influence-Driven Camouflage<h3 id="越狱"><a href="#越狱" class="headerlink" title="越狱"></a>越狱</h3></li>
<li>LLM-Fuzzer: Scaling Assessment of Large Language Model Jailbreaks.</li>
<li>Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models.</li>
<li>Malla: Demystifying Real-world Large Language Model Integrated Malicious Services.</li>
<li>Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction.<h3 id="模型萃取和水印"><a href="#模型萃取和水印" class="headerlink" title="模型萃取和水印"></a>模型萃取和水印</h3></li>
<li>SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice.</li>
<li>Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through Data-Free Model Extraction Attacks?</li>
<li>ClearStamp: A Human-Visible and Robust Model-Ownership Proof based on Transposed Model Training.</li>
<li>DeepEclipse: How to Break White-Box DNN-Watermarking Schemes.</li>
<li>ModelGuard: Information-Theoretic Defense Against Model Extraction Attacks.</li>
</ul>
<h3 id="大模型滥用"><a href="#大模型滥用" class="headerlink" title="大模型滥用"></a>大模型滥用</h3><ul>
<li>Moderating Illicit Online Image Promotion for Unsafe User Generated Content Games Using Large Vision-Language Models.</li>
<li>Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text.</li>
<li>Prompt Stealing Attacks Against Text-to-Image Generation Models.</li>
<li>Quantifying Privacy Risks of Prompts in Visual Prompt Learning.<h3 id="安全分析"><a href="#安全分析" class="headerlink" title="安全分析"></a>安全分析</h3></li>
<li>Hijacking Attacks against Neural Network by Analyzing Training Data.</li>
<li>False Claims against Model Ownership Resolution.</li>
<li>Landscape More Secure Than Portrait? Zooming Into the Directionality of Digital Images With Security Implications.</li>
<li>Information Flow Control in Machine Learning through Modular Model Architecture.<h3 id="物理对抗攻击"><a href="#物理对抗攻击" class="headerlink" title="物理对抗攻击"></a>物理对抗攻击</h3></li>
<li>Devil in the Room: Triggering Audio Backdoors in the Physical World.</li>
<li>FraudWhistler: A Resilient, Robust and Plug-and-play Adversarial Example Detection Method for Speaker Recognition.</li>
<li>pi-Jack: Physical-World Adversarial Attack on Monocular Depth Estimation with Perspective Hijacking.</li>
<li>AE-Morpher: Improve Physical Robustness of Adversarial Objects against LiDAR-based Detectors via Object Reconstruction.</li>
</ul>
<h3 id="用户研究"><a href="#用户研究" class="headerlink" title="用户研究"></a>用户研究</h3><ul>
<li>“I Don’t Know If We’re Doing Good. I Don’t Know If We’re Doing Bad”: Investigating How Practitioners Scope, Motivate, and Conduct Privacy Work When Developing AI Products.</li>
<li>Towards More Practical Threat Models in Artificial Intelligence Security.</li>
</ul>
<h2 id="USENIX-2025-cycle1"><a href="#USENIX-2025-cycle1" class="headerlink" title="USENIX 2025 cycle1"></a>USENIX 2025 cycle1</h2><h3 id="攻击-Attacks"><a href="#攻击-Attacks" class="headerlink" title="攻击 (Attacks)"></a>攻击 (Attacks)</h3><h4 id="越狱与提示工程-Jailbreaking-amp-Prompt-Engineering"><a href="#越狱与提示工程-Jailbreaking-amp-Prompt-Engineering" class="headerlink" title="越狱与提示工程 (Jailbreaking &amp; Prompt Engineering)"></a>越狱与提示工程 (Jailbreaking &amp; Prompt Engineering)</h4><ul>
<li>PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models</li>
<li>PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs</li>
<li>On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts</li>
<li>Exposing the Guardrails: Reverse-Engineering and Jailbreaking Safety Filters in DALL·E Text-to-Image Pipelines</li>
<li>Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack</li>
<li>Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents</li>
<li>Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink</li>
<li>Low-Cost and Comprehensive Non-textual Input Fuzzing with LLM-Synthesized Input Generators</li>
</ul>
<h4 id="数据投毒与后门-Data-Poisoning-amp-Backdoors"><a href="#数据投毒与后门-Data-Poisoning-amp-Backdoors" class="headerlink" title="数据投毒与后门 (Data Poisoning &amp; Backdoors)"></a>数据投毒与后门 (Data Poisoning &amp; Backdoors)</h4><ul>
<li>PoiSAFL: Scalable Poisoning Attack Framework to Byzantine-resilient Semi-asynchronous Federated Learning</li>
<li>Persistent Backdoor Attacks in Continual Learning</li>
<li>From Purity to Peril: Backdooring Merged Models From “Harmless” Benign Components</li>
</ul>
<h4 id="成员与属性推理-Membership-amp-Attribute-Inference"><a href="#成员与属性推理-Membership-amp-Attribute-Inference" class="headerlink" title="成员与属性推理 (Membership &amp; Attribute Inference)"></a>成员与属性推理 (Membership &amp; Attribute Inference)</h4><ul>
<li>Enhanced Label-Only Membership Inference Attacks with Fewer Queries</li>
<li>Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses</li>
<li>Membership Inference Attacks Against Vision-Language Models</li>
<li>Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models</li>
</ul>
<h4 id="对抗性与物理攻击-Adversarial-amp-Physical-Attacks"><a href="#对抗性与物理攻击-Adversarial-amp-Physical-Attacks" class="headerlink" title="对抗性与物理攻击 (Adversarial &amp; Physical Attacks)"></a>对抗性与物理攻击 (Adversarial &amp; Physical Attacks)</h4><ul>
<li>Fighting Fire with Fire: Continuous Attack for Adversarial Android Malware Detection</li>
<li>Atkscopes: Multiresolution Adversarial Perturbation as a Unified Attack on Perceptual Hashing and Beyond</li>
<li>Invisible but Detected: Physical Adversarial Shadow Attack and Defense on LiDAR Object Detection</li>
</ul>
<h4 id="系统与硬件漏洞利用-System-amp-Hardware-Exploits"><a href="#系统与硬件漏洞利用-System-amp-Hardware-Exploits" class="headerlink" title="系统与硬件漏洞利用 (System &amp; Hardware Exploits)"></a>系统与硬件漏洞利用 (System &amp; Hardware Exploits)</h4><ul>
<li>NeuroScope: Reverse Engineering Deep Neural Network on Edge Devices using Dynamic Analysis</li>
<li>BarraCUDA: Edge GPUs do Leak DNN Weights</li>
<li>Not so Refreshing: Attacking GPUs using RFM Rowhammer Mitigation</li>
<li>Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI</li>
<li>Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning</li>
<li>When Translators Refuse to Translate: A Novel Attack to Speech Translation Systems</li>
<li>Chimera: Creating Digitally Signed Fake Photos by Fooling Image Recapture and Deepfake Detectors</li>
</ul>
<hr>
<h3 id="防御-Defenses"><a href="#防御-Defenses" class="headerlink" title="防御 (Defenses)"></a>防御 (Defenses)</h3><h4 id="隐私保护与安全计算-Privacy-amp-Secure-Computation"><a href="#隐私保护与安全计算-Privacy-amp-Secure-Computation" class="headerlink" title="隐私保护与安全计算 (Privacy &amp; Secure Computation)"></a>隐私保护与安全计算 (Privacy &amp; Secure Computation)</h4><ul>
<li>DP-BREM: Differentially-Private and Byzantine-Robust Federated Learning with Client Momentum</li>
<li>LOHEN: Layer-wise Optimizations for Neural Network Inferences over Encrypted Data with High Performance or Accuracy</li>
<li>Task-Oriented Training Data Privacy Protection for Cloud-based Model Training</li>
<li>Arbitrary-Threshold Fully Homomorphic Encryption with Lower Complexity</li>
<li>zkGPT: An Efficient Non-interactive Zero-knowledge Proof Framework for LLM Inference</li>
<li>Distributed Private Aggregation in Graph Neural Networks</li>
<li>Phantom: Privacy-Preserving Deep Neural Network Model Obfuscation in Heterogeneous TEE and GPU System</li>
</ul>
<h4 id="鲁棒性与认证-Robustness-amp-Certification"><a href="#鲁棒性与认证-Robustness-amp-Certification" class="headerlink" title="鲁棒性与认证 (Robustness &amp; Certification)"></a>鲁棒性与认证 (Robustness &amp; Certification)</h4><ul>
<li>Robustifying ML-powered Network Classifiers with PANTS</li>
<li>AGNNCert: Defending Graph Neural Networks against Arbitrary Perturbations with Deterministic Certification</li>
<li>CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization</li>
<li>CertPHash: Towards Certified Perceptual Hashing via Robust Training</li>
</ul>
<h4 id="越狱与提示注入防御-Jailbreak-amp-Prompt-Injection-Defense"><a href="#越狱与提示注入防御-Jailbreak-amp-Prompt-Injection-Defense" class="headerlink" title="越狱与提示注入防御 (Jailbreak &amp; Prompt Injection Defense)"></a>越狱与提示注入防御 (Jailbreak &amp; Prompt Injection Defense)</h4><ul>
<li>StruQ: Defending Against Prompt Injection with Structured Queries</li>
<li>JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation</li>
<li>SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner</li>
</ul>
<h4 id="水印与知识产权保护-Watermarking-amp-IP-Protection"><a href="#水印与知识产权保护-Watermarking-amp-IP-Protection" class="headerlink" title="水印与知识产权保护 (Watermarking &amp; IP Protection)"></a>水印与知识产权保护 (Watermarking &amp; IP Protection)</h4><ul>
<li>THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models</li>
<li>AudioMarkNet: Audio Watermarking for Deepfake Speech Detection</li>
<li>Provably Robust Multi-bit Watermarking for AI-generated Text</li>
<li>LLMmap: Fingerprinting for Large Language Models</li>
<li>LightShed: Defeating Perturbation-based Image Copyright Protections</li>
</ul>
<h4 id="后门及通用防御-Backdoor-amp-General-Defense"><a href="#后门及通用防御-Backdoor-amp-General-Defense" class="headerlink" title="后门及通用防御 (Backdoor &amp; General Defense)"></a>后门及通用防御 (Backdoor &amp; General Defense)</h4><ul>
<li>Dormant: Defending against Pose-driven Human Image Animation</li>
<li>SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis</li>
<li>Pretender: Universal Active Defense against Diffusion Finetuning Attacks</li>
<li>Synthetic Artifact Auditing: Tracing LLM-Generated Synthetic Data Usage in Downstream Applications</li>
<li>DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data</li>
</ul>
<hr>
<h3 id="漏洞-分析-Vulnerabilities-Analysis"><a href="#漏洞-分析-Vulnerabilities-Analysis" class="headerlink" title="漏洞/分析 (Vulnerabilities/Analysis)"></a>漏洞/分析 (Vulnerabilities/Analysis)</h3><ul>
<li>Towards Understanding and Enhancing Security of Proof-of-Training for DNN Model Ownership Verification</li>
<li>The Ghost Navigator: Revisiting the Hidden Vulnerability of Localization in Autonomous Driving</li>
<li>Revisiting Training-Inference Trigger Intensity in Backdoor Attacks</li>
<li>Evaluating LLM-based Personal Information Extraction and Countermeasures</li>
<li>Evaluating the Effectiveness and Robustness of Visual Similarity-based Phishing Detection Models</li>
<li>SoK: On Gradient Leakage in Federated Learning</li>
<li>VoiceWukong: Benchmarking Deepfake Voice Detection</li>
<li>Analyzing the AI Nudification Application Ecosystem</li>
<li>We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs</li>
<li>When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs</li>
<li>HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns</li>
<li>Watch the Watchers! On the Security Risks of Robustness-Enhancing Diffusion Models</li>
<li>From Meme to Threat: On the Hateful Meme Understanding and Induced Hateful Content Generation in Open-Source Vision Language Models</li>
<li>Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data</li>
<li>NOKEScam: Understanding and Rectifying Non-Sense Keywords Spear Scam in Search Engines</li>
</ul>
<h2 id="NDSS-2025"><a href="#NDSS-2025" class="headerlink" title="NDSS 2025"></a>NDSS 2025</h2><h3 id="攻击-Attacks-1"><a href="#攻击-Attacks-1" class="headerlink" title="攻击 (Attacks)"></a>攻击 (Attacks)</h3><h4 id="成员推理与数据重建-Membership-Inference-amp-Data-Reconstruction"><a href="#成员推理与数据重建-Membership-Inference-amp-Data-Reconstruction" class="headerlink" title="成员推理与数据重建 (Membership Inference &amp; Data Reconstruction)"></a>成员推理与数据重建 (Membership Inference &amp; Data Reconstruction)</h4><ul>
<li>A Method to Facilitate Membership Inference Attacks in Deep Learning Models.</li>
<li>Black-box Membership Inference Attacks against Fine-tuned Diffusion Models.</li>
<li>Passive Inference Attacks on Split Learning via Adversarial Regularization.</li>
<li>RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning with Adversarial Data Manipulation.</li>
<li>Scale-MIA: A Scalable Model Inversion Attack against Secure Federated Learning via Latent Space Reconstruction.</li>
<li>URVFL: Undetectable Data Reconstruction Attack on Vertical Federated Learning.</li>
</ul>
<h4 id="对抗性与物理攻击-Adversarial-amp-Physical-Attacks-1"><a href="#对抗性与物理攻击-Adversarial-amp-Physical-Attacks-1" class="headerlink" title="对抗性与物理攻击 (Adversarial &amp; Physical Attacks)"></a>对抗性与物理攻击 (Adversarial &amp; Physical Attacks)</h4><ul>
<li>AlphaDog: No-Box Camouflage Attacks via Alpha Channel Oversight.</li>
<li>Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based Wireless Communication Systems.</li>
<li>On the Realism of LiDAR Spoofing Attacks against Autonomous Driving Vehicle at High Speed and Long Distance.</li>
<li>PhantomLiDAR: Cross-modality Signal Injection Attacks against LiDAR.</li>
<li>Revisiting Physical-World Adversarial Attack on Traffic Sign Recognition: A Commercial Systems Perspective.</li>
<li>L-HAWK: A Controllable Physical Adversarial Patch Against a Long-Distance Target.</li>
</ul>
<h4 id="后门与木马-Backdoor-amp-Trojan"><a href="#后门与木马-Backdoor-amp-Trojan" class="headerlink" title="后门与木马 (Backdoor &amp; Trojan)"></a>后门与木马 (Backdoor &amp; Trojan)</h4><ul>
<li>The Philosopher’s Stone: Trojaning Plugins of Large Language Models.</li>
<li>LADDER: Multi-Objective Backdoor Attack via Evolutionary Algorithm.</li>
</ul>
<h4 id="其他攻击-Other-Attacks"><a href="#其他攻击-Other-Attacks" class="headerlink" title="其他攻击 (Other Attacks)"></a>其他攻击 (Other Attacks)</h4><ul>
<li>I Know What You Asked: Prompt Leakage via KV-Cache Sharing in Multi-Tenant LLM Serving.</li>
<li>Automated Mass Malware Factory: The Convergence of Piggybacking and Adversarial Example in Android Malicious Software Generation.</li>
</ul>
<h3 id="防御-Defenses-1"><a href="#防御-Defenses-1" class="headerlink" title="防御 (Defenses)"></a>防御 (Defenses)</h3><h4 id="隐私保护与安全计算-Privacy-amp-Secure-Computing"><a href="#隐私保护与安全计算-Privacy-amp-Secure-Computing" class="headerlink" title="隐私保护与安全计算 (Privacy &amp; Secure Computing)"></a>隐私保护与安全计算 (Privacy &amp; Secure Computing)</h4><ul>
<li>BumbleBee: Secure Two-party Inference Framework for Large Transformers.</li>
<li>Diffence: Fencing Membership Privacy With Diffusion Models.</li>
<li>Secure Transformer Inference Made Non-interactive.</li>
<li>A New PPML Paradigm for Quantized Models.</li>
<li>Defending Against Membership Inference Attacks on Iteratively Pruned Deep Neural Networks.</li>
<li>DLBox: New Model Training Framework for Protecting Training Data.</li>
<li>MingledPie: A Cluster Mingling Approach for Mitigating Preference Profiling in CFL.</li>
<li>Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models.</li>
<li>SHAFT: Secure, Handy, Accurate and Fast Transformer Inference.</li>
<li>SIGuard: Guarding Secure Inference with Post Data Privacy.</li>
</ul>
<h4 id="后门检测与防御-Backdoor-Detection-amp-Defense"><a href="#后门检测与防御-Backdoor-Detection-amp-Defense" class="headerlink" title="后门检测与防御 (Backdoor Detection &amp; Defense)"></a>后门检测与防御 (Backdoor Detection &amp; Defense)</h4><ul>
<li>CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models.</li>
<li>BARBIE: Robust Backdoor Detection Based on Latent Separability.</li>
<li>DShield: Defending against Backdoor Attacks on Graph Neural Networks via Discrepancy Learning.</li>
<li>PBP: Post-training Backdoor Purification for Malware Classifiers.</li>
<li>SafeSplit: A Novel Defense Against Client-Side Backdoor Attacks in Split Learning.</li>
</ul>
<h4 id="模型遗忘与审查-Model-Unlearning-amp-Censorship"><a href="#模型遗忘与审查-Model-Unlearning-amp-Censorship" class="headerlink" title="模型遗忘与审查 (Model Unlearning &amp; Censorship)"></a>模型遗忘与审查 (Model Unlearning &amp; Censorship)</h4><ul>
<li>Reinforcement Unlearning.</li>
<li>THEMIS: Regulating Textual Inversion for Personalized Concept Censorship.</li>
<li>TrajDeleter: Enabling Trajectory Forgetting in Offline Reinforcement Learning Agents.</li>
</ul>
<h4 id="数据保护与内容审核-Data-Protection-amp-Content-Moderation"><a href="#数据保护与内容审核-Data-Protection-amp-Content-Moderation" class="headerlink" title="数据保护与内容审核 (Data Protection &amp; Content Moderation)"></a>数据保护与内容审核 (Data Protection &amp; Content Moderation)</h4><ul>
<li>Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution.</li>
<li>GAP-Diff: Protecting JPEG-Compressed Images from Diffusion-based Facial Customization.</li>
<li>Provably Unlearnable Data Examples.</li>
<li>SongBsAb: A Dual Prevention Approach against Singing Voice Conversion based Illegal Song Covers.</li>
<li>Try to Poison My Deep Learning Data? Nowhere to Hide Your Trajectory Spectrum!</li>
</ul>
<h4 id="通用防御与架构-General-Defense-amp-Architecture"><a href="#通用防御与架构-General-Defense-amp-Architecture" class="headerlink" title="通用防御与架构 (General Defense &amp; Architecture)"></a>通用防御与架构 (General Defense &amp; Architecture)</h4><ul>
<li>CENSOR: Defense Against Gradient Inversion via Orthogonal Subspace Bayesian Sampling.</li>
<li>ASGARD: Protecting On-Device Deep Neural Networks with Virtualization-Based Trusted Execution Environments.</li>
<li>BitShield: Defending Against Bit-Flip Attacks on DNN Executables.</li>
<li>Density Boosts Everything: A One-stop Strategy for Improving Performance, Robustness, and Sustainability of Malware Detectors.</li>
<li>IsolateGPT: An Execution Isolation Architecture for LLM-Based Agentic Systems.</li>
<li>Probe-Me-Not: Protecting Pre-trained Encoders from Malicious Probing.</li>
<li>Revisiting Concept Drift in Windows Malware Detection: Adaptation to Real Drifted Malware with Minimal Samples.</li>
</ul>
<h3 id="漏洞-分析-Vulnerabilities-Analysis-1"><a href="#漏洞-分析-Vulnerabilities-Analysis-1" class="headerlink" title="漏洞/分析 (Vulnerabilities/Analysis)"></a>漏洞/分析 (Vulnerabilities/Analysis)</h3><ul>
<li>Compiled Models, Built-In Exploits: Uncovering Pervasive Bit-Flip Attack Surfaces in DNN Executables.</li>
<li>Revisiting EM-based Estimation for Locally Differentially Private Protocols.</li>
<li>Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?</li>
<li>Do We Really Need to Design New Byzantine-robust Aggregation Rules?</li>
<li>On the Robustness of LDP Protocols for Numerical Attributes under Data Poisoning Attacks.</li>
<li>Safety Misalignment Against Large Language Models.</li>
<li>Towards Understanding Unsafe Video Generation.</li>
</ul>
<h2 id="CCS-2024"><a href="#CCS-2024" class="headerlink" title="CCS 2024"></a>CCS 2024</h2><h3 id="攻击-Attacks-2"><a href="#攻击-Attacks-2" class="headerlink" title="攻击 (Attacks)"></a>攻击 (Attacks)</h3><h4 id="隐私攻击-Privacy-Attacks"><a href="#隐私攻击-Privacy-Attacks" class="headerlink" title="隐私攻击 (Privacy Attacks)"></a>隐私攻击 (Privacy Attacks)</h4><ul>
<li>Unveiling the Vulnerability of Private Fine-Tuning in Split-Based Frameworks for Large Language Models: A Bidirectionally Enhanced Attack.</li>
<li>QueryCheetah: Fast Automated Discovery of Attribute Inference Attacks Against Query-Based Systems.</li>
<li>Membership Inference Attacks Against In-Context Learning.</li>
<li>SeqMIA: Sequential-Metric Based Membership Inference Attack.</li>
<li>PLeak: Prompt Leaking Attacks against Large Language Model Applications.</li>
<li>Is Difficulty Calibration All We Need? Towards More Practical Membership Inference Attacks.</li>
<li>A Unified Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability.</li>
</ul>
<h4 id="对抗性与物理攻击-Adversarial-amp-Physical-Attacks-2"><a href="#对抗性与物理攻击-Adversarial-amp-Physical-Attacks-2" class="headerlink" title="对抗性与物理攻击 (Adversarial &amp; Physical Attacks)"></a>对抗性与物理攻击 (Adversarial &amp; Physical Attacks)</h4><ul>
<li>Certifiable Black-Box Attacks with Randomized Adversarial Examples: Breaking Defenses with Provable Confidence.</li>
<li>Zero-Query Adversarial Attack on Black-box Automatic Speech Recognition Systems.</li>
<li>SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems.</li>
<li>The Invisible Polyjuice Potion: an Effective Physical Adversarial Attack against Face Recognition.</li>
<li>Manipulative Interference Attacks.</li>
</ul>
<h4 id="数据投毒与后门-Data-Poisoning-amp-Backdoors-1"><a href="#数据投毒与后门-Data-Poisoning-amp-Backdoors-1" class="headerlink" title="数据投毒与后门 (Data Poisoning &amp; Backdoors)"></a>数据投毒与后门 (Data Poisoning &amp; Backdoors)</h4><ul>
<li>Phantom: Untargeted Poisoning Attacks on Semi-Supervised Learning.</li>
<li>Distributed Backdoor Attacks on Federated Graph Learning and Certified Defenses.</li>
<li>Data Poisoning Attacks to Locally Differentially Private Frequent Itemset Mining Protocols.</li>
<li>BadMerging: Backdoor Attacks Against Model Merging.</li>
<li>Watch Out! Simple Horizontal Class Backdoor Can Trivially Evade Defense.</li>
</ul>
<h4 id="模型与系统利用-Model-amp-System-Exploits"><a href="#模型与系统利用-Model-amp-System-Exploits" class="headerlink" title="模型与系统利用 (Model &amp; System Exploits)"></a>模型与系统利用 (Model &amp; System Exploits)</h4><ul>
<li>Inbox Invasion: Exploiting MIME Ambiguities to Evade Email Attachment Detectors.</li>
<li>Optimization-based Prompt Injection Attack to LLM-as-a-Judge.</li>
<li>Neural Dehydration: Effective Erasure of Black-box Watermarks from DNNs with Limited Data.</li>
<li>SurrogatePrompt: Bypassing the Safety Filter of Text-to-Image Models via Substitution.</li>
<li>“Modern problems require modern solutions”: Community-Developed Techniques for Online Exam Proctoring Evasion.</li>
<li>Not One Less: Exploring Interplay between User Profiles and Items in Untargeted Attacks against Federated Recommendation.</li>
<li>HyperTheft: Thieving Model Weights from TEE-Shielded Neural Networks via Ciphertext Side Channels.</li>
<li>DeepCache: Revisiting Cache Side-Channel Attacks in Deep Neural Networks Executables.</li>
</ul>
<hr>
<h3 id="防御-Defenses-2"><a href="#防御-Defenses-2" class="headerlink" title="防御 (Defenses)"></a>防御 (Defenses)</h3><h4 id="隐私保护与安全计算-Privacy-amp-Secure-Computing-1"><a href="#隐私保护与安全计算-Privacy-amp-Secure-Computing-1" class="headerlink" title="隐私保护与安全计算 (Privacy &amp; Secure Computing)"></a>隐私保护与安全计算 (Privacy &amp; Secure Computing)</h4><ul>
<li>Camel: Communication-Efficient and Maliciously Secure Federated Learning in the Shuffle Model of Differential Privacy.</li>
<li>S2NeRF: Privacy-preserving Training Framework for NeRF.</li>
<li>$DPM: $ Clustering Sensitive Data through Separation.</li>
<li>S-BDT: Distributed Differentially Private Boosted Decision Trees.</li>
<li>Cross-silo Federated Learning with Record-level Personalized Differential Privacy.</li>
<li>Membership Inference Attacks against Vision Transformers: Mosaic MixUp Training to the Defense.</li>
<li>Formal Privacy Proof of Data Encoding: The Possibility and Impossibility of Learnable Encryption.</li>
<li>Elephants Do Not Forget: Differential Privacy with State Continuity for Privacy Budget.</li>
<li>ProBE: Proportioning Privacy Budget for Complex Exploratory Decision Support.</li>
<li>Almost Instance-optimal Clipping for Summation Problems in the Shuffle Model of Differential Privacy.</li>
<li>Securing Floating-Point Arithmetic for Noise Addition.</li>
<li>Rhombus: Fast Homomorphic Matrix-Vector Multiplication for Secure Two-Party Inference.</li>
<li>Byzantine-Robust Decentralized Federated Learning.</li>
<li>Sparrow: Space-Efficient zkSNARK for Data-Parallel Circuits and Applications to Zero-Knowledge Decision Trees.</li>
<li>AirGapAgent: Protecting Privacy-Conscious Conversational Agents.</li>
<li>CoGNN: Towards Secure and Efficient Collaborative Graph Learning.</li>
<li>Computationally Secure Aggregation and Private Information Retrieval in the Shuffle Model.</li>
<li>Zero-Knowledge Proofs of Training for Deep Neural Networks.</li>
<li>zkLLM: Zero Knowledge Proofs for Large Language Models.</li>
<li>Securely Training Decision Trees Efficiently.</li>
<li>Poster: End-to-End Privacy-Preserving Vertical Federated Learning using Private Cross-Organizational Data Collaboration.</li>
<li>Poster: Protection against Source Inference Attacks in Federated Learning using Unary Encoding and Shuffling.</li>
</ul>
<h4 id="内容审核与模型安全-Content-Moderation-amp-Model-Safety"><a href="#内容审核与模型安全-Content-Moderation-amp-Model-Safety" class="headerlink" title="内容审核与模型安全 (Content Moderation &amp; Model Safety)"></a>内容审核与模型安全 (Content Moderation &amp; Model Safety)</h4><ul>
<li>A Causal Explainable Guardrails for Large Language Models.</li>
<li>Legilimens: Practical and Unified Content Moderation for Large Language Model Services.</li>
<li>Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies.</li>
<li>PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs).</li>
<li>SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models.</li>
<li>Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code.</li>
</ul>
<h4 id="鲁棒性与检测-Robustness-amp-Detection"><a href="#鲁棒性与检测-Robustness-amp-Detection" class="headerlink" title="鲁棒性与检测 (Robustness &amp; Detection)"></a>鲁棒性与检测 (Robustness &amp; Detection)</h4><ul>
<li>Training Robust ML-based Raw-Binary Malware Detectors in Hours, not Months.</li>
<li>SpecGuard: Specification Aware Recovery for Robotic Autonomous Vehicles from Physical Attacks.</li>
<li>VisionGuard: Secure and Robust Visual Perception of Autonomous Vehicles in Practice.</li>
<li>PhyScout: Detecting Sensor Spoofing Attacks via Spatio-temporal Consistency.</li>
<li>Alchemy: Data-Free Adversarial Training.</li>
<li>I Don’t Know You, But I Can Catch You: Real-Time Defense against Diverse Adversarial Patches for Object Detectors.</li>
<li>PhySense: Defending Physically Realizable Attacks for Autonomous Systems via Consistency Reasoning.</li>
<li>Fisher Information guided Purification against Backdoor Attacks.</li>
<li>Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines.</li>
<li>ZeroFake: Zero-Shot Detection of Fake Images Generated and Edited by Text-to-Image Generation Models.</li>
<li>Poster: AuditVotes: A Framework towards Deployable Certified Robustness for GNNs.</li>
<li>Towards Proactive Protection against Unauthorized Speech Synthesis.</li>
</ul>
<h4 id="数据保护与审计-Data-Protection-amp-Auditing"><a href="#数据保护与审计-Data-Protection-amp-Auditing" class="headerlink" title="数据保护与审计 (Data Protection &amp; Auditing)"></a>数据保护与审计 (Data Protection &amp; Auditing)</h4><ul>
<li>A General Framework for Data-Use Auditing of ML Models.</li>
<li>MaskPrint: Take the Initiative in Fingerprint Protection to Mitigate the Harm of Data Breach.</li>
<li>Dye4AI: Assuring Data Boundary on Generative AI Services.</li>
<li>TabularMark: Watermarking Tabular Datasets for Machine Learning.</li>
<li>Beowulf: Mitigating Model Extraction Attacks Via Reshaping Decision Regions.</li>
<li>ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach.</li>
<li>Pulsar: Secure Steganography for Diffusion Models.</li>
<li>Demo: FT-PrivacyScore: Personalized Privacy Scoring Service for Machine Learning Participation.</li>
<li>Catch Me if You Can: Detecting Unauthorized Data Use in Training Deep Learning Models.</li>
<li>Poster: Solving the Free-rider Problem in Bittensor.</li>
<li>Poster: Enhance Hardware Domain Specific Large Language Model with Reinforcement Learning for Resilience.</li>
</ul>
<hr>
<h3 id="漏洞-分析-Vulnerabilities-Analysis-2"><a href="#漏洞-分析-Vulnerabilities-Analysis-2" class="headerlink" title="漏洞/分析 (Vulnerabilities/Analysis)"></a>漏洞/分析 (Vulnerabilities/Analysis)</h3><ul>
<li>Evaluations of Machine Learning Privacy Defenses are Misleading.</li>
<li>The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks.</li>
<li>Graphical vs. Deep Generative Models: Measuring the Impact of Differentially Private Mechanisms and Budgets on Utility.</li>
<li>“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models.</li>
<li>Demystifying RCE Vulnerabilities in LLM-Integrated Apps.</li>
<li>Using AI Assistants in Software Development: A Qualitative Study on Security Practices and Concerns.</li>
<li>Analyzing Inference Privacy Risks Through Gradients In Machine Learning.</li>
<li>PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps.</li>
<li>Uncovering Gradient Inversion Risks in Practical Language Model Training.</li>
<li>Avara: A Uniform Evaluation System for Perceptibility Analysis Against Adversarial Object Evasion Attacks.</li>
<li>Image-Perfect Imperfections: Safety, Bias, and Authenticity in the Shadow of Text-To-Image Model Evolution.</li>
<li>Blind and Low-Vision Individuals’ Detection of Audio Deepfakes.</li>
<li>Privacy Analyses in Machine Learning.</li>
<li>Novel Privacy Attacks and Defenses Against Neural Networks.</li>
</ul>
<h2 id="CCS-2025-first-cycle"><a href="#CCS-2025-first-cycle" class="headerlink" title="CCS 2025 first cycle"></a>CCS 2025 first cycle</h2><h3 id="攻击-Attacks-3"><a href="#攻击-Attacks-3" class="headerlink" title="攻击 (Attacks)"></a>攻击 (Attacks)</h3><h4 id="隐私与数据提取-Privacy-amp-Data-Extraction"><a href="#隐私与数据提取-Privacy-amp-Data-Extraction" class="headerlink" title="隐私与数据提取 (Privacy &amp; Data Extraction)"></a>隐私与数据提取 (Privacy &amp; Data Extraction)</h4><ul>
<li>Prompt Inference Attack on Distributed Large Language Model Inference Frameworks</li>
<li>Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation</li>
<li>Differentiation-Based Extraction of Proprietary Data from Fine-tuned LLMs</li>
</ul>
<h4 id="数据投毒与对抗性攻击-Data-Poisoning-amp-Adversarial-Attacks"><a href="#数据投毒与对抗性攻击-Data-Poisoning-amp-Adversarial-Attacks" class="headerlink" title="数据投毒与对抗性攻击 (Data Poisoning &amp; Adversarial Attacks)"></a>数据投毒与对抗性攻击 (Data Poisoning &amp; Adversarial Attacks)</h4><ul>
<li>Poisoning Attacks to Local Differential Privacy for Ranking Estimation</li>
<li>ControlLoc: Physical-World Hijacking Attack on Camera-based Perception in Autonomous Driving</li>
<li>On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling</li>
<li>One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP</li>
<li>Busting the Paper Ballot: Voting Meets Adversarial Machine Learning</li>
</ul>
<hr>
<h3 id="防御-Defenses-3"><a href="#防御-Defenses-3" class="headerlink" title="防御 (Defenses)"></a>防御 (Defenses)</h3><h4 id="模型遗忘与隐私保护-Unlearning-amp-Privacy"><a href="#模型遗忘与隐私保护-Unlearning-amp-Privacy" class="headerlink" title="模型遗忘与隐私保护 (Unlearning &amp; Privacy)"></a>模型遗忘与隐私保护 (Unlearning &amp; Privacy)</h4><ul>
<li>Split Unlearning</li>
<li>Rethinking Machine Unlearning in Image Generation Models</li>
<li>Anonymity Unveiled: A Practical Framework for Auditing Data Use in Deep Learning Models</li>
<li>LZKSA: Lattice-based special zero-knowledge proofs for secure aggregation’s input verification</li>
<li>Prototype Surgery: Tailoring Neural Prototypes via Soft Labels for Efficient Machine Unlearning</li>
<li>Secure Noise Sampling for Differentially Private Collaborative Learning</li>
<li>Founding Zero-Knowledge Proof of Training on Optimum Vicinity</li>
<li>Gibbon: Faster Secure Two-party Training of Gradient Boosting Decision Tree</li>
</ul>
<h4 id="后门与恶意软件防御-Backdoor-amp-Malware-Defense"><a href="#后门与恶意软件防御-Backdoor-amp-Malware-Defense" class="headerlink" title="后门与恶意软件防御 (Backdoor &amp; Malware Defense)"></a>后门与恶意软件防御 (Backdoor &amp; Malware Defense)</h4><ul>
<li>Combating Concept Drift with Explanatory Detection and Adaptation for Android Malware Classification</li>
<li>PoisonSpot: Precise Spotting of Clean-Label Backdoors via Fine-Grained Training Provenance Tracking</li>
<li>Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model</li>
<li>FilterFL: Knowledge Filtering-based Data-Free Backdoor Defense for Federated Learning</li>
</ul>
<h4 id="安全与鲁棒性系统-Secure-amp-Robust-Systems"><a href="#安全与鲁棒性系统-Secure-amp-Robust-Systems" class="headerlink" title="安全与鲁棒性系统 (Secure &amp; Robust Systems)"></a>安全与鲁棒性系统 (Secure &amp; Robust Systems)</h4><ul>
<li>TensorShield: Safeguarding On-Device Inference by Shieldin g Critical DNN Tensors with TEE</li>
<li>Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning</li>
<li>RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models</li>
<li>SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models</li>
<li>SecAlign: Defending Against Prompt Injection with Preference Optimization</li>
<li>A Practical and Secure Byzantine Robust Aggregator</li>
<li>DivTrackee versus DynTracker: Promoting Diversity in Anti-Facial Recognition against Dynamic FR Strategy</li>
</ul>
<hr>
<h3 id="漏洞-分析-Vulnerabilities-Analysis-3"><a href="#漏洞-分析-Vulnerabilities-Analysis-3" class="headerlink" title="漏洞/分析 (Vulnerabilities/Analysis)"></a>漏洞/分析 (Vulnerabilities/Analysis)</h3><ul>
<li>Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble</li>
<li>Towards Backdoor Stealthiness in Model Parameter Space</li>
<li>What’s Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift</li>
</ul>
<h2 id="CCS-2025-second-cycle"><a href="#CCS-2025-second-cycle" class="headerlink" title="CCS 2025 second cycle"></a>CCS 2025 second cycle</h2><h3 id="Adversarial-Attacks-amp-Robustness-对抗性攻击与模型鲁棒性"><a href="#Adversarial-Attacks-amp-Robustness-对抗性攻击与模型鲁棒性" class="headerlink" title="Adversarial Attacks &amp; Robustness (对抗性攻击与模型鲁棒性)"></a>Adversarial Attacks &amp; Robustness (对抗性攻击与模型鲁棒性)</h3><p>这类攻击通过对输入数据（如图像、文本、传感器信号）进行微小、人难以察觉的扰动，来欺骗模型做出错误的判断。</p>
<ul>
<li><p><strong>Adversarial Observations in Weather Forecasting</strong>: 探讨在气象预报中引入对抗性观测数据的影响 (攻击)。</p>
</li>
<li><p><strong>Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving</strong>: 针对自动驾驶地图构建的物理世界攻击 (攻击)。</p>
</li>
<li><p><strong>Evaluating the robustness of a production malware detection system to transferable adversarial attacks</strong>: 评估恶意软件检测系统对可迁移对抗性攻击的鲁棒性 (评估/攻击)。</p>
</li>
<li><p><strong>Threat from Windshield: Vehicle Windows as Involuntary Attack Sources on Automotive Voice Assistants</strong>: 利用挡风玻璃作为媒介，对车载语音助手进行物理攻击 (攻击)。</p>
</li>
<li><p><strong>Adversarially Robust Assembly Language Model for Packed Executables Detection</strong>: 针对加壳可执行文件的对抗性鲁棒语言模型 (防御)。</p>
</li>
<li><p><strong>Exact Robustness Certification of k-Nearest Neighbors</strong>: 对k近邻算法提供可证明的鲁棒性保证 (防御)。</p>
</li>
<li><p><strong>Provable Repair of Deep Neural Network Defects by Preimage Synthesis and Property Refinement</strong>: 对神经网络中由对抗性攻击等引起的缺陷进行可证明的修复 (防御)。</p>
</li>
<li><p><strong>Towards Real-Time Defense Against Object-Based LiDAR Attacks in Autonomous Driving</strong>: 针对自动驾驶中激光雷达的实时攻击防御 (防御)。</p>
</li>
</ul>
<hr>
<h3 id="Privacy-Attacks-隐私攻击"><a href="#Privacy-Attacks-隐私攻击" class="headerlink" title="Privacy Attacks (隐私攻击)"></a>Privacy Attacks (隐私攻击)</h3><p>这类攻击旨在从模型或系统中窃取敏感信息，例如训练数据、用户查询或个人身份信息。常见类型包括成员推断、模型提取和侧信道攻击。</p>
<ul>
<li><p><strong>Peekaboo, I See Your Queries: Passive Attacks Against DSSE Via Intermittent Observations</strong>: 通过间歇性观察对动态对称可搜索加密方案进行被动式攻击，窃取查询信息 (攻击)。</p>
</li>
<li><p><strong>DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</strong>: 针对检索增强生成（RAG）模型的成员推断攻击，判断特定数据是否被用于训练 (攻击)。</p>
</li>
<li><p><strong>Can Personal Health Information Be Secured in LLM? Privacy Attack and Defense in the Medical Domain</strong>: 探讨大语言模型在医疗领域的隐私攻击与防御 (攻击/防御)。</p>
</li>
<li><p><strong>Timing Attacks on Differential Privacy are Practical</strong>: 证明了对差分隐私机制的计时攻击在实践中是可行的 (攻击)。</p>
</li>
<li><p><strong>Byte by Byte: Unmasking Browser Fingerprinting at the Function Level using V8 Bytecode Transformers</strong>: 通过分析V8字节码来揭示和增强浏览器指纹识别技术 (攻击)。</p>
</li>
<li><p><strong>MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs</strong>: 利用侧信道攻击窃取混合专家模型（MoE）中的用户隐私 (攻击)。</p>
</li>
<li><p><strong>Safeguarding Graph Neural Networks against Topology Inference Attacks</strong>: 防御旨在推断图结构（如社交网络关系）的拓扑推断攻击 (防御)。</p>
</li>
<li><p><strong>You Can’t Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors</strong>: 通过系统向量缓解大语言模型中的提示词泄露问题 (防御)。</p>
</li>
<li><p><strong>Mosformer: Maliciously Secure Three-Party Inference Framework for Large Transformers</strong>: 一个保护隐私的安全多方计算框架，用于Transformer模型的推理 (防御)。</p>
</li>
<li><p><strong>THOR: Secure Transformer Inference with Homomorphic Encryption</strong>: 使用同态加密技术实现安全的Transformer模型推理，保护数据机密性 (防御)。</p>
</li>
<li><p><strong>PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss Random Variable Optimization</strong>: 一种优化差分隐私深度学习效用的新方法 (防御)。</p>
</li>
<li><p><strong>IOValve: Leakage-Free I/O Sandbox for Large-Scale Untrusted Data Processing</strong>: 为大规模不可信数据处理设计的无泄漏I/O沙箱 (防御)。</p>
</li>
</ul>
<hr>
<h3 id="Data-Poisoning-amp-Backdoor-Attacks-数据投毒与后门攻击"><a href="#Data-Poisoning-amp-Backdoor-Attacks-数据投毒与后门攻击" class="headerlink" title="Data Poisoning &amp; Backdoor Attacks (数据投毒与后门攻击)"></a>Data Poisoning &amp; Backdoor Attacks (数据投毒与后门攻击)</h3><p>这类攻击通过向训练数据中注入少量精心制作的“毒样本”，在模型中植入“后门”。模型在正常输入下表现正常，但在遇到包含特定触发器（trigger）的输入时，会产生攻击者预设的恶意行为。</p>
<ul>
<li><p><strong>VillainNet: Targeted Poisoning Attacks Against SuperNets Along the Accuracy-Latency Pareto Frontier</strong>: 针对超网（SuperNets）的精确投毒攻击 (攻击)。</p>
</li>
<li><p><strong>The Phantom Menace in PET-Hardened Deep Learning Models: Invisible Configuration-Induced Attacks</strong>: 揭示了在参数高效微调（PET）模型中由配置引发的隐形攻击，类似于后门 (攻击)。</p>
</li>
<li><p><strong>Cascading Adversarial Bias from Injection to Distillation in Language Models</strong>: 探讨对抗性偏见如何从注入阶段传播到模型蒸馏阶段，是一种偏见投毒 (攻击)。</p>
</li>
<li><p><strong>On Hyperparameters and Backdoor-Resistance in Horizontal Federated Learning</strong>: 研究水平联邦学习中超参数对后门攻击抵抗性的影响 (评估/防御)。</p>
</li>
<li><p><strong>Deep Learning from Imperfectly Labeled Malware Data</strong>: 研究在不完美标注的恶意软件数据上进行学习，这与投毒攻击场景相关 (评估/防御)。</p>
</li>
<li><p><strong>Armadillo: Robust Single-Server Secure Aggregation for Federated Learning with Input Validation</strong>: 在联邦学习中抵抗投毒攻击的安全聚合协议 (防御)。</p>
</li>
<li><p><strong>Sentry: Authenticating Machine Learning Artifacts on the Fly</strong>: 用于实时验证机器学习模型和数据真实性的框架，可抵御投毒和篡改 (防御)。</p>
</li>
</ul>
<hr>
<h3 id="Prompt-Injection-amp-LLM-Manipulation-提示注入与大模型操纵"><a href="#Prompt-Injection-amp-LLM-Manipulation-提示注入与大模型操纵" class="headerlink" title="Prompt Injection &amp; LLM Manipulation (提示注入与大模型操纵)"></a>Prompt Injection &amp; LLM Manipulation (提示注入与大模型操纵)</h3><p>这类攻击主要针对基于大语言模型（LLM）的应用，特别是检索增强生成（RAG）系统。攻击者通过构造恶意提示词（Prompt）或污染外部知识库，来操纵模型的输出，使其泄露信息、产生有害内容或执行非预期任务。</p>
<ul>
<li><p><strong>FlippedRAG: Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models</strong>: 对RAG系统进行黑盒攻击，以操纵其生成的观点和内容 (攻击)。</p>
</li>
<li><p><strong>GASLITEing the Retrieval: Exploring Vulnerabilities in Dense Embedding-based Search</strong>: 探索基于密集嵌入的检索系统的漏洞，这种系统是RAG的核心 (攻击)。</p>
</li>
<li><p><strong>ImportSnare: Directed ”Code Manual” Hijacking in Retrieval-Augmented Code Generation</strong>: 针对RAG代码生成系统的“代码手册”劫持攻击 (攻击)。</p>
</li>
<li><p><strong>Here Comes The AI Worm: Preventing the Propagation of Adversarial Self-Replicating Prompts Within GenAI Ecosystems</strong>: 探讨可自我复制的对抗性提示（AI蠕虫）在生成式AI生态系统中的传播与防御 (攻击/防御)。</p>
</li>
<li><p><strong>Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection</strong>: 通过知识注入的方式来保护RAG代码生成系统的安全 (防御)。</p>
</li>
</ul>
<hr>
<h3 id="System-Software-amp-Hardware-Vulnerabilities-系统、软件与硬件漏洞"><a href="#System-Software-amp-Hardware-Vulnerabilities-系统、软件与硬件漏洞" class="headerlink" title="System, Software &amp; Hardware Vulnerabilities (系统、软件与硬件漏洞)"></a>System, Software &amp; Hardware Vulnerabilities (系统、软件与硬件漏洞)</h3><p>这类研究关注的是机器学习系统所依赖的底层软件、硬件或网络协议中的安全漏洞，而不仅是模型本身。</p>
<ul>
<li><p><strong>PickleBall: Secure Deserialization of Pickle-based Machine Learning Models</strong>: 关注Python Pickle格式在加载ML模型时的反序列化漏洞及安全措施 (攻击/防御)。</p>
</li>
<li><p><strong>Denial of Sequencing Attacks in Ethereum Layer 2 Rollups</strong>: 针对以太坊二层扩容方案（Rollups）的拒绝服务攻击 (攻击)。</p>
</li>
<li><p><strong>Automatic Discovery of User-exploitable Architectural Security Vulnerabilities in Closed-Source RISC-V CPUs</strong>: 自动发现闭源RISC-V处理器中的体系结构安全漏洞 (攻击/工具)。</p>
</li>
<li><p><strong>Styled to Steal: The Overlooked Attack Surface in Email Clients</strong>: 揭示了电子邮件客户端中被忽视的攻击面 (攻击)。</p>
</li>
<li><p><strong>Chekhov’s Gun: Uncovering Hidden Risks in macOS Application-Sandboxed PID-Domain Services</strong>: 发现macOS沙箱服务中的隐藏安全风险 (攻击)。</p>
</li>
<li><p><strong>Deep Dive into In-app Browsers: Uncovering Hidden Pitfalls in Certificate Validation</strong>: 揭露应用内浏览器在证书验证方面的安全隐患 (攻击)。</p>
</li>
<li><p><strong>Hardening Deep Neural Network Binaries against Reverse Engineering Attacks</strong>: 强化深度学习模型二进制文件以抵抗逆向工程攻击 (防御)。</p>
</li>
<li><p><strong>CITesting: Systematic Testing of Context Integrity Violations in Cellular Core Networks</strong>: 对蜂窝网络核心网中上下文完整性破坏漏洞进行系统性测试 (测试/评估)。</p>
</li>
</ul>
<hr>
<h3 id="Model-Data-Integrity-amp-Provenance-Attacks-模型-数据完整性与溯源攻击"><a href="#Model-Data-Integrity-amp-Provenance-Attacks-模型-数据完整性与溯源攻击" class="headerlink" title="Model/Data Integrity &amp; Provenance Attacks (模型/数据完整性与溯源攻击)"></a>Model/Data Integrity &amp; Provenance Attacks (模型/数据完整性与溯源攻击)</h3><p>这类攻击的目标是破坏用于验证模型或数据来源的机制，例如数字水印。</p>
<ul>
<li><p><strong>Removal Attack and Defense on AI Generated Content Latent-based Watermarking</strong>: 针对AIGC内容中基于潜在空间的水印的移除攻击与防御 (攻击/防御)。</p>
</li>
<li><p><strong>PreferCare: Preference Dataset Copyright Protection in LLM Alignment by Watermark Injection and Verification</strong>: 通过水印保护用于LLM对齐的偏好数据集的版权 (防御)。</p>
</li>
</ul>
<hr>
<h3 id="Security-Auditing-Benchmarking-amp-Measurement-安全审计、基准与测量"><a href="#Security-Auditing-Benchmarking-amp-Measurement-安全审计、基准与测量" class="headerlink" title="Security Auditing, Benchmarking &amp; Measurement (安全审计、基准与测量)"></a>Security Auditing, Benchmarking &amp; Measurement (安全审计、基准与测量)</h3><p>这些论文不一定提出新的攻击或防御方法，而是专注于开发工具、基准（Benchmark）或进行大规模测量，以评估和理解现有系统的安全状况。</p>
<ul>
<li><p><strong>What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale</strong>: 对大规模共享的扩散模型进行概念审计，以发现潜在风险 (审计)。</p>
</li>
<li><p><strong>The Odyssey of robots.txt Governance: Measuring Convention Implications of Web Bots in Large Language Model Services</strong>: 测量<code>robots.txt</code>协议对大模型网络爬虫的影响，评估其治理现状 (测量)。</p>
</li>
<li><p><strong>UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images</strong>: 为图像安全分类器提供一个包含真实和AI生成图像的基准测试集 (基准)。</p>
</li>
<li><p><strong>YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models</strong>: 面向青少年的大模型安全基准和保护模型 (基准/防御)。</p>
</li>
<li><p><strong>Automatically Detecting Online Deceptive Patterns</strong>: 自动检测网络中的欺骗性模式（如暗模式） (工具/检测)。</p>
</li>
<li><p><strong>OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs</strong>: 利用审计日志和LLM来重构高级持续性威胁（APT）的攻击链 (工具/检测)。</p>
</li>
</ul>
<hr>
<h3 id="Comprehensive-Defense-Frameworks-综合性防御框架"><a href="#Comprehensive-Defense-Frameworks-综合性防御框架" class="headerlink" title="Comprehensive Defense Frameworks (综合性防御框架)"></a>Comprehensive Defense Frameworks (综合性防御框架)</h3><p>这类工作提供端到端或系统性的安全框架，旨在防御多种类型的攻击。</p>
<ul>
<li><strong>AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents</strong>: 为操作计算机的AI智能体（Agent）设计的端到端实时安全防御框架 (防御)。</li>
</ul>
<h2 id="S-amp-P-2025"><a href="#S-amp-P-2025" class="headerlink" title="S&amp;P 2025"></a>S&amp;P 2025</h2><h3 id="攻击-Attacks-4"><a href="#攻击-Attacks-4" class="headerlink" title="攻击 (Attacks)"></a>攻击 (Attacks)</h3><h4 id="提示工程与越狱-Prompt-Engineering-amp-Jailbreaking"><a href="#提示工程与越狱-Prompt-Engineering-amp-Jailbreaking" class="headerlink" title="提示工程与越狱 (Prompt Engineering &amp; Jailbreaking)"></a>提示工程与越狱 (Prompt Engineering &amp; Jailbreaking)</h4><ul>
<li>Modifier Unlocked: Jailbreaking Text-to-Image Models Through Prompts.</li>
<li>Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-to-Image Generation Models.</li>
<li>On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts.</li>
<li>Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt Injection Attacks via the Fine-Tuning Interface.</li>
<li>Prompt Inversion Attack Against Collaborative Inference of Large Language Models.</li>
</ul>
<h4 id="数据投毒与后门-Data-Poisoning-amp-Backdoors-2"><a href="#数据投毒与后门-Data-Poisoning-amp-Backdoors-2" class="headerlink" title="数据投毒与后门 (Data Poisoning &amp; Backdoors)"></a>数据投毒与后门 (Data Poisoning &amp; Backdoors)</h4><ul>
<li>Preference Poisoning Attacks on Reward Model Learning.</li>
<li>Architectural Neural Backdoors from First Principles.</li>
<li>Practical Poisoning Attacks with Limited Byzantine Clients in Clustered Federated Learning.</li>
</ul>
<h4 id="模型与数据窃取-Model-amp-Data-Extraction"><a href="#模型与数据窃取-Model-amp-Data-Extraction" class="headerlink" title="模型与数据窃取 (Model &amp; Data Extraction)"></a>模型与数据窃取 (Model &amp; Data Extraction)</h4><ul>
<li>Codebreaker: Dynamic Extraction Attacks on Code Language Models.</li>
<li>Rigging the Foundation: Manipulating Pre-training for Advanced Membership Inference Attacks.</li>
<li>UnMarker: A Universal Attack on Defensive Image Watermarking.</li>
<li>CipherSteal: Stealing Input Data from TEE-Shielded Neural Networks with Ciphertext Side Channels.</li>
</ul>
<h4 id="其他攻击-Other-Attacks-1"><a href="#其他攻击-Other-Attacks-1" class="headerlink" title="其他攻击 (Other Attacks)"></a>其他攻击 (Other Attacks)</h4><ul>
<li>My Model is Malware to You: Transforming AI Models into Malware by Abusing TensorFlow APIs.</li>
<li>Make a Feint to the East While Attacking in the West: Blinding LLM-Based Code Auditors with Flashboom Attacks.</li>
<li>The Inadequacy of Similarity-Based Privacy Metrics: Privacy Attacks Against “Truly Anonymous” Synthetic Datasets.</li>
<li>EvilHarmony: Stealthy Adversarial Attacks Against Black-Box Speech Recognition Systems.</li>
<li>Investigating Physical Latency Attacks Against Camera-Based Perception.</li>
</ul>
<hr>
<h3 id="防御-Defenses-4"><a href="#防御-Defenses-4" class="headerlink" title="防御 (Defenses)"></a>防御 (Defenses)</h3><h4 id="后门与攻击检测-Backdoor-amp-Attack-Detection"><a href="#后门与攻击检测-Backdoor-amp-Attack-Detection" class="headerlink" title="后门与攻击检测 (Backdoor &amp; Attack Detection)"></a>后门与攻击检测 (Backdoor &amp; Attack Detection)</h4><ul>
<li>Secure Transfer Learning: Training Clean Model Against Backdoor in Pre-Trained Encoder and Downstream Dataset.</li>
<li>Query Provenance Analysis: Efficient and Robust Defense Against Query-Based Black-Box Attacks.</li>
<li>BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target.</li>
<li>PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning.</li>
<li>DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks.</li>
<li>Lombard-VLD: Voice Liveness Detection Based on Human Auditory Feedback.</li>
</ul>
<h4 id="隐私保护与安全计算-Privacy-amp-Secure-Computing-2"><a href="#隐私保护与安全计算-Privacy-amp-Secure-Computing-2" class="headerlink" title="隐私保护与安全计算 (Privacy &amp; Secure Computing)"></a>隐私保护与安全计算 (Privacy &amp; Secure Computing)</h4><ul>
<li>GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models.</li>
<li>SHARK: Actively Secure Inference Using Function Secret Sharing.</li>
<li>Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity.</li>
<li>FairZK: A Scalable System to Prove Machine Learning Fairness in Zero-Knowledge.</li>
<li>PAC-Private Algorithms.</li>
<li>An Attack-Agnostic Defense Framework Against Manipulation Attacks Under Local Differential Privacy.</li>
<li>From Easy to Hard: Building a Shortcut for Differentially Private Image Synthesis.</li>
</ul>
<h4 id="鲁棒性与对抗防御-Robustness-amp-Adversarial-Defense"><a href="#鲁棒性与对抗防御-Robustness-amp-Adversarial-Defense" class="headerlink" title="鲁棒性与对抗防御 (Robustness &amp; Adversarial Defense)"></a>鲁棒性与对抗防御 (Robustness &amp; Adversarial Defense)</h4><ul>
<li>TSQP: Safeguarding Real-Time Inference for Quantization Neural Networks on Edge Devices.</li>
<li>Fight Fire with Fire: Combating Adversarial Patch Attacks using Pattern-randomized Defensive Patches.</li>
<li>Adversarial Robust ViT-Based Automatic Modulation Recognition in Practical Deep Learning-Based Wireless Systems.</li>
<li>EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with Audio Adversarial Perturbations.</li>
<li>Spoofing Eavesdroppers with Audio Misinformation.</li>
</ul>
<h4 id="通用防御与审计-General-Defense-amp-Auditing"><a href="#通用防御与审计-General-Defense-amp-Auditing" class="headerlink" title="通用防御与审计 (General Defense &amp; Auditing)"></a>通用防御与审计 (General Defense &amp; Auditing)</h4><ul>
<li>Edge Unlearning is Not “on Edge”! an Adaptive Exact Unlearning System on Resource-Constrained Devices.</li>
<li>Towards Reliable Verification of Unauthorized Data Usage in Personalized Text-to-Image Diffusion Models.</li>
<li>Watermarking Language Models for Many Adaptive Users.</li>
<li>Guardain: Protecting Emerging Generative AI Workloads on Heterogeneous NPU.</li>
</ul>
<hr>
<h3 id="漏洞-分析-Vulnerabilities-Analysis-4"><a href="#漏洞-分析-Vulnerabilities-Analysis-4" class="headerlink" title="漏洞/分析 (Vulnerabilities/Analysis)"></a>漏洞/分析 (Vulnerabilities/Analysis)</h3><ul>
<li>Understanding Users’ Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms.</li>
<li>On the (In)Security of LLM App Stores.</li>
<li>SoK: Decoding the Enigma of Encrypted Network Traffic Classifiers.</li>
<li>On the Conflict Between Robustness and Learning in Collaborative Machine Learning.</li>
<li>Not All Edges are Equally Robust: Evaluating the Robustness of Ranking-Based Federated Learning.</li>
<li>SoK: Watermarking for AI-Generated Content.</li>
<li>From One Stolen Utterance: Assessing the Risks of Voice Cloning in the AIGC Era.</li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">wangxh</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://blog.expecto.top/2025/08/28/2025-diao-yan/">https://blog.expecto.top/2025/08/28/2025-diao-yan/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">wangxh</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/">
                                    <span class="chip bg-color">论文调研</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/2025/08/28/2025-diao-yan/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/10.jpg" class="responsive-img" alt="2025调研">
                        
                        <span class="card-title">2025调研</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            wangxh
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/">
                        <span class="chip bg-color">论文调研</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/04/03/a-survey-of-safety-on-large-vision-language-models-attacks-defenses-and-evaluations-yue-du-bi-ji/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/9.jpg" class="responsive-img" alt="A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations 阅读笔记">
                        
                        <span class="card-title">A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations 阅读笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            wangxh
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">机器学习</span>
                    </a>
                    
                    <a href="/tags/%E7%BB%BC%E8%BF%B0/">
                        <span class="chip bg-color">综述</span>
                    </a>
                    
                    <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                        <span class="chip bg-color">大语言模型</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2021-2025</span>
            
            <a href="/about" target="_blank">wangxh</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">59.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis"> 
    <a href="https://github.com/Knok0ut" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>


<!--












 -->
</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
